import json
import logging  # Will be replaced by etl_logger if used, but this is a generator script
import os
import pathlib
import re
import subprocess
import tempfile
from typing import Any

# Configure basic logging for the generator script itself
logger = logging.getLogger(__name__)
if not logger.handlers: # Avoid adding multiple handlers if reloaded
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# Default paths (relative to project root)
DEFAULT_PSA_OPENAPI_SCHEMA_PATH = "config/schemas/psa/PSA_OpenAPI_schema.json"
DEFAULT_PSA_OUTPUT_DIR = "fabric_api/models/psa_models/"


def _load_openapi_schema(schema_path: pathlib.Path) -> dict[str, Any]:
    """Load an OpenAPI schema from a file path."""
    with open(schema_path, encoding='utf-8') as f:
        return json.load(f)

def _find_references_in_schema_object(schema_obj: dict[str, Any], refs: set[str]) -> set[str]:
    """Find all $ref references in a schema object."""
    if isinstance(schema_obj, dict):
        for k, v in schema_obj.items():
            if k == "$ref" and isinstance(v, str) and v.startswith("#/components/schemas/"):
                ref_name = v.split("/")[-1]
                refs.add(ref_name)
            elif isinstance(v, (dict, list)):
                _find_references_in_schema_object(v, refs) # Recursive call
    elif isinstance(schema_obj, list):
        for item in schema_obj:
            if isinstance(item, (dict, list)):
                _find_references_in_schema_object(item, refs) # Recursive call
    return refs


def _extract_models_with_dependencies(
    full_schema: dict[str, Any], model_names_to_extract: list[str]
) -> dict[str, Any]:
    """
    Extract specified models and their dependencies from a full OpenAPI schema.
    """
    subset_schema_components = {}

    models_to_process = set(model_names_to_extract)
    processed_models = set()

    while models_to_process:
        model_name = models_to_process.pop()
        if model_name in processed_models:
            continue

        model_schema_obj = full_schema.get("components", {}).get("schemas", {}).get(model_name)
        if not model_schema_obj:
            logger.warning(f"Model '{model_name}' not found in the provided OpenAPI schema. Skipping.")
            processed_models.add(model_name)
            continue

        subset_schema_components[model_name] = model_schema_obj

        # Find dependencies (references) within this model's schema
        # Initialize new set for each model to avoid issues with shared mutable default args
        dependent_refs = _find_references_in_schema_object(model_schema_obj, set())

        for ref in dependent_refs:
            if ref not in processed_models and ref not in subset_schema_components:
                models_to_process.add(ref)

        processed_models.add(model_name)

    final_subset_schema = {
        "openapi": full_schema.get("openapi", "3.0.0"),
        "info": full_schema.get("info", {"title": "Subset Schema for PSA Models", "version": "1.0.0"}),
        "paths": {}, # Paths are not needed for model generation
        "components": {"schemas": subset_schema_components},
    }
    logger.info(f"Generated subset schema with {len(subset_schema_components)} models for PSA.")
    return final_subset_schema


def _create_psa_init_file(output_dir: pathlib.Path, all_generated_model_names: list[str]):
    """Create the __init__.py file for the generated PSA models."""
    init_path = output_dir / "__init__.py"

    # Ensure class names are unique and sorted for consistent __init__.py
    unique_sorted_model_names = sorted(list(set(all_generated_model_names)))

    with open(init_path, "w", encoding="utf-8") as f:
        f.write('"""PSA API models generated from OpenAPI schema by datamodel-code-generator.\n')
        f.write("This file is autogenerated. Do not edit manually.\n")
        f.write("Generated by fabric_api/generators/generate_psa_models.py\n")
        f.write('"""\n')
        f.write("# flake8: noqa\n")
        f.write("# fmt: off\n\n")

        f.write("from .models import (\n")
        for model_name in unique_sorted_model_names:
            f.write(f"    {model_name},\n")
        f.write(")\n\n")

        f.write("__all__ = [\n")
        for model_name in unique_sorted_model_names:
            f.write(f'    "{model_name}",\n')
        f.write("]\n")
        f.write("# fmt: on\n")

    logger.info(f"Successfully created/updated PSA models __init__.py at {init_path}")


def _post_process_psa_models_file(models_file_path: pathlib.Path, reference_model_names: list[str]):
    """
    Post-processes the generated models.py file for PSA models.
    - Adds a header.
    - Fixes CustomFieldValue.value type hint.
    - Ensures reference models are defined only once.
    - Adds PostedInvoice = Invoice alias.
    """
    logger.info(f"Post-processing PSA models file: {models_file_path}")

    with open(models_file_path, encoding='utf-8') as f:
        content = f.read()

    # Standard header for the combined models file
    header = (
        '"""\nPSA API models generated from OpenAPI schema by datamodel-code-generator.\n\n'
        "Compatible with Pydantic v2 and SparkDantic for Spark schema generation.\n"
        "This file contains all models to avoid circular imports and manage dependencies.\n"
        "This file is autogenerated. Do not edit manually.\n"
        "Generated by fabric_api/generators/generate_psa_models.py\n"
        '"""\n'
        "# flake8: noqa\n"
        "# fmt: off\n\n"
        "from __future__ import annotations\n\n"
        "from datetime import datetime\n"
        "from typing import Any, Dict, List, Optional, Literal\n" # Ensure Literal is imported
        "from uuid import UUID\n\n" # Ensure UUID is imported
        "from pydantic import Field\n"
        "from sparkdantic import SparkModel\n\n"
    )

    lines = content.split('\n')
    processed_lines = []

    # Remove existing imports and __future__
    in_imports_section = True
    for line in lines:
        if in_imports_section and (line.startswith("from ") or line.startswith("import ") or line.strip() == ""):
            continue
        in_imports_section = False
        processed_lines.append(line)

    content = "\n".join(processed_lines)

    # Fix CustomFieldValue.value type hint (if class exists)
    # value: Optional[Dict[str, Any]] = Field(default=None, alias="value") -> value: Optional[Any] ...
    custom_field_value_pattern = r"(class CustomFieldValue\(.+?\):.*?value:\s*Optional\[)Dict\[str, Any\](\].*?# API inconsistency)?"
    replace_with = r"\1Any\2" # Keep the comment if it was there

    if "class CustomFieldValue(" in content: # Check if class exists
        modified_content, num_subs = re.subn(custom_field_value_pattern, replace_with, content, flags=re.DOTALL)
        if num_subs > 0:
            content = modified_content
            logger.info("Applied fix for CustomFieldValue.value type hint.")
        else:
            # If the above didn't match, try a simpler pattern for the type hint only
            simple_pattern = r"(value:\s*Optional\[)Dict\[str, Any\](\])"
            modified_content_simple, num_subs_simple = re.subn(simple_pattern, r"\1Any\2", content)
            if num_subs_simple > 0:
                content = modified_content_simple
                logger.info("Applied simplified fix for CustomFieldValue.value type hint.")
            else:
                logger.warning("Could not apply CustomFieldValue.value fix; pattern not found.")


    # Add PostedInvoice = Invoice alias if Invoice model is present
    if "class Invoice(" in content:
        content += "\n\n# PostedInvoice is an alias of Invoice, used for consistency in some contexts\n"
        content += "PostedInvoice = Invoice\n"
        logger.info("Added PostedInvoice = Invoice alias.")

    # Prepend the standard header
    final_content = header + content

    with open(models_file_path, 'w', encoding='utf-8') as f:
        f.write(final_content)

    logger.info(f"PSA models file post-processing complete for {models_file_path}")
    return final_content


def generate_psa_models(
    openapi_schema_path_str: str = DEFAULT_PSA_OPENAPI_SCHEMA_PATH,
    output_dir_str: str = DEFAULT_PSA_OUTPUT_DIR,
    entities_to_include: list[str] | None = None, # e.g. ["Agreement", "TimeEntry"]
    reference_models_to_include: list[str] | None = None # e.g. ["ActivityReference"]
):
    """
    Generates Pydantic models for specified PSA entities from an OpenAPI schema.
    """
    openapi_schema_path = pathlib.Path(openapi_schema_path_str).resolve()
    output_dir = pathlib.Path(output_dir_str).resolve()

    logger.info("Starting PSA Pydantic model generation...")
    logger.info(f"OpenAPI schema path: {openapi_schema_path}")
    logger.info(f"Output directory: {output_dir}")

    if not openapi_schema_path.exists():
        logger.error(f"OpenAPI schema file not found at {openapi_schema_path}. Exiting PSA model generation.")
        return False

    # Define default entities if none are provided
    if entities_to_include is None:
        entities_to_include = [
            "Agreement", "TimeEntry", "ExpenseEntry",
            "Invoice", # This will also implicitly include PostedInvoice due to alias
            "ProductItem",
            # Add other core entities here as needed by default
        ]
        logger.info(f"No specific entities provided, using default set: {entities_to_include}")

    if reference_models_to_include is None:
        reference_models_to_include = [ # These are common reference types
            "ActivityReference", "AgreementReference", "AgreementTypeReference", "BatchReference",
            # Add other common reference types if necessary
        ]
        logger.info(f"Using default reference models: {reference_models_to_include}")

    full_schema = _load_openapi_schema(openapi_schema_path)

    all_models_for_subset = list(set(entities_to_include + reference_models_to_include))

    logger.info(f"Creating subset schema including {len(all_models_for_subset)} initial models and their dependencies...")
    subset_schema = _extract_models_with_dependencies(full_schema, all_models_for_subset)

    output_dir.mkdir(parents=True, exist_ok=True)
    models_file_path = output_dir / "models.py"

    # Write subset schema to a temporary file for datamodel-codegen
    with tempfile.NamedTemporaryFile(suffix=".json", mode="w", delete=False, dir=output_dir) as tmp_schema_file:
        json.dump(subset_schema, tmp_schema_file, indent=2)
        tmp_schema_file_path = tmp_schema_file.name # Get path for codegen

    logger.info(f"Temporary subset schema written to: {tmp_schema_file_path}")

    # datamodel-codegen command
    # Using --aliases for specific field name changes (e.g. systemId -> SystemId)
    # This requires aliases to be defined in the input schema or via a separate alias file.
    # The original script did post-processing for this. We will rely on that.
    # The base_class is set to SparkModel.
    cmd = [
        "datamodel-codegen",
        "--input", tmp_schema_file_path,
        "--input-file-type", "openapi", # Important: use openapi for full component resolution
        "--output", str(models_file_path),
        "--base-class", "sparkdantic.SparkModel",
        "--target-python-version", "3.11",
        "--use-standard-collections",
        "--use-schema-description",
        "--use_field_description", # Note: original script had this, check CLI option name
        "--reuse-model",
        "--field-constraints",
        "--strict-nullable",
        "--use-union-operator",
        "--disable-timestamp", # Avoid adding generation timestamp in the models file itself
        "--custom-template-dir", "fabric_api/generators/templates/psa", # Path to custom templates
        # "--snake-case-field", # Usually false for PSA to keep original casing
    ]

    try:
        logger.info(f"Running datamodel-codegen with command: {' '.join(cmd)}")
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        logger.info(f"Successfully generated initial PSA models to {models_file_path}")

        # Post-process the generated models.py file
        _post_process_psa_models_file(models_file_path, reference_models_to_include)

        # Create __init__.py file
        # Extract all class names from the generated models.py for __init__.py
        # This should include entities, their dependencies, and reference models.
        all_generated_model_names = sorted(list(subset_schema["components"]["schemas"].keys()))
        # Ensure PostedInvoice is added if Invoice was generated, due to alias
        if "Invoice" in all_generated_model_names and "PostedInvoice" not in all_generated_model_names:
            all_generated_model_names.append("PostedInvoice")

        _create_psa_init_file(output_dir, all_generated_model_names)

        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Error generating PSA models using datamodel-codegen: {e}")
        logger.error(f"Command output (stdout): {e.stdout}")
        logger.error(f"Command output (stderr): {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"An unexpected error occurred during PSA model generation: {e}", exc_info=True)
        return False
    finally:
        # Clean up temporary schema file
        if os.path.exists(tmp_schema_file_path):
            os.remove(tmp_schema_file_path)
            logger.info(f"Cleaned up temporary schema file: {tmp_schema_file_path}")


if __name__ == "__main__":
    # This allows the script to be run directly for testing/debugging
    # Setup basic logging for direct script run
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # For direct execution, consider where schemas are relative to this script
    # Assuming project root is parent of fabric_api
    project_root = pathlib.Path(__file__).parent.parent.parent
    default_schema = project_root / DEFAULT_PSA_OPENAPI_SCHEMA_PATH
    default_output = project_root / DEFAULT_PSA_OUTPUT_DIR

    logger.info("Running generate_psa_models.py directly.")
    logger.info(f"Using schema: {default_schema}")
    logger.info(f"Outputting to: {default_output}")

    # Create dummy template dir if it doesn't exist for direct run
    # In real scenario, this path should be correct.
    dummy_template_dir = project_root / "fabric_api/generators/templates/psa"
    dummy_template_dir.mkdir(parents=True, exist_ok=True)


    success = generate_psa_models(
        openapi_schema_path_str=str(default_schema),
        output_dir_str=str(default_output)
        # entities_to_include and reference_models_to_include will use defaults
    )
    if success:
        logger.info("PSA Model generation completed successfully (direct run).")
    else:
        logger.error("PSA Model generation failed (direct run).")
