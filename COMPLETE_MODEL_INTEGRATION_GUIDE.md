# Complete Model Integration Guide - PSA and BC Unified Lakehouse

This guide demonstrates how the PSA and BC pipelines fully integrate with autogenerated Pydantic models from their respective schemas.

## Architecture Overview

```
Model Generation → SparkDantic Models → Bronze/Silver/Gold Pipeline → Unified Analytics
      ↑                    ↓                      ↓                         ↓
   OpenAPI              Validation           Schema Aware            Conformed Data
   CDM Manifests       Type Safety          Processing             Cross-System
```

## 1. Model Generation Process

### PSA Models (from OpenAPI)

```bash
# Generate PSA models from ConnectWise OpenAPI
python regenerate_models.py \
    --schema PSA_OpenAPI_schema.json \
    --output-dir fabric_api/connectwise_models \
    --entities Agreement TimeEntry ExpenseEntry ProductItem Invoice
```

Generated structure:
```python
# fabric_api/connectwise_models/models.py
from sparkdantic import SparkModel

class Agreement(SparkModel):
    """ConnectWise Agreement entity."""
    id: int | None = None
    name: str | None = None
    agreementNumber: str | None = None
    typeName: str | None = None
    companyId: int | None = None
    # ... all fields from OpenAPI spec
    
    def model_spark_schema(self) -> StructType:
        """Auto-generated Spark schema from model."""
        ...

class TimeEntry(SparkModel):
    """ConnectWise Time Entry entity."""
    id: int | None = None
    memberId: int | None = None
    invoiceId: int | None = None
    agreementId: int | None = None
    workTypeId: int | None = None
    # ... all fields from OpenAPI spec
```

### BC Models (from CDM)

```bash
# Generate BC models from CDM manifests
python fabric_api/generate_bc_models_camelcase.py \
    --input-dir cdm_manifests \
    --output fabric_api/bc_models/models.py
```

Generated structure:
```python
# fabric_api/bc_models/models.py
from sparkdantic import SparkModel

class Customer(SparkModel):
    """Customer entity from Business Central CDM."""
    No: str | None = Field(default=None, alias="No-18")
    Name: str | None = Field(default=None, alias="Name-18")
    CustomerPostingGroup: str | None = None
    Company: str | None = Field(default=None, alias="$Company")
    # ... all fields from CDM manifest
    
    def model_spark_schema(self) -> StructType:
        """Auto-generated Spark schema from model."""
        ...

class GLEntry(SparkModel):
    """General Ledger Entry from Business Central."""
    EntryNo: int | None = Field(default=None, alias="EntryNo-17")
    PostingDate: datetime | None = None
    Amount: float | None = None
    DimensionSetID: int | None = None
    # ... all fields from CDM manifest
```

## 2. Pipeline Integration

### PSA Pipeline with Models

```python
# fabric_api/pipeline_psa_enhanced_with_models.py
from .connectwise_models import (
    Agreement,
    TimeEntry,
    ExpenseEntry,
    ProductItem,
    PostedInvoice
)

class PSAPipeline:
    def __init__(self, spark: SparkSession):
        self.spark = spark
        
        # Direct model mapping for validation
        self.model_mapping = {
            "Agreement": Agreement,
            "TimeEntry": TimeEntry,
            "ExpenseEntry": ExpenseEntry,
            "ProductItem": ProductItem,
            "PostedInvoice": PostedInvoice,
        }
    
    def process_entity_to_bronze(self, entity_name: str, ...):
        # Get the model class
        model_class = self.model_mapping.get(entity_name)
        
        # Extract and validate data
        valid_data = extract_entity(...)  # Returns list of model instances
        
        # Convert to DataFrame with proper schema
        raw_df = dataframe_from_models(valid_data, entity_name)
        # This uses model.model_spark_schema() internally
        
        # Write to bronze with schema
        bronze_table = f"bronze.psa.{entity_name}"
        raw_df.write.mode(mode).saveAsTable(bronze_table)
    
    def process_bronze_to_silver(self, entity_name: str, ...):
        # Read bronze data
        bronze_df = self.spark.table(f"bronze.psa.{entity_name}")
        
        # Get model for validation
        model_class = self.model_mapping.get(entity_name)
        
        if model_class:
            # Get expected schema
            model_schema = model_class.model_spark_schema()
            
            # Validate columns exist
            common_columns = set(bronze_df.columns) & set(model_schema.fieldNames())
            validated_df = bronze_df.select(*common_columns)
            
            # Cast types to match model
            for field in model_schema.fields:
                if field.name in validated_df.columns:
                    validated_df = validated_df.withColumn(
                        field.name,
                        col(field.name).cast(field.dataType)
                    )
```

### BC Pipeline with Models

```python
# fabric_api/bc_medallion_pipeline_enhanced_with_models.py
from .bc_models import (
    Customer,
    GLEntry,
    Job,
    Resource,
    # ... all BC models
)

class BCMedallionPipelineEnhanced:
    def __init__(self, spark: SparkSession):
        self.spark = spark
        
        # CDM model mapping
        self.model_mapping = {
            "Customer": Customer,
            "GLEntry": GLEntry,
            "Job": Job,
            "Resource": Resource,
            # ... all BC models
        }
    
    def bronze_to_silver(self, table_name: str, ...):
        # Strip numeric suffix (Customer18 → Customer)
        clean_table_name = re.sub(r'\d+$', '', table_name)
        
        # Get CDM model
        model_class = self.model_mapping.get(clean_table_name)
        
        if model_class:
            # Validate against CDM schema
            model_schema = model_class.model_spark_schema()
            
            # Map CDM field names to actual columns
            # Example: "No-18" → "No", "$Company" → "Company"
            for field in model_schema.fields:
                # Field has alias for CDM name
                cdm_name = field.metadata.get("alias", field.name)
                if cdm_name in bronze_df.columns:
                    bronze_df = bronze_df.withColumnRenamed(cdm_name, field.name)
```

## 3. Unified Orchestrator

```python
# fabric_api/unified_lakehouse_orchestrator.py
from .pipeline_psa_enhanced_with_models import PSAPipeline
from .bc_medallion_pipeline_enhanced_with_models import BCMedallionPipelineEnhanced

class UnifiedLakehouseOrchestrator:
    def __init__(self, spark: SparkSession):
        # Both pipelines use their model systems
        self.psa_pipeline = PSAPipeline(spark)
        self.bc_pipeline = BCMedallionPipelineEnhanced(spark)
    
    def run_full_pipeline(self, ...):
        # PSA uses OpenAPI models
        psa_results = self.psa_pipeline.run_full_pipeline(
            entity_names=["Agreement", "TimeEntry"],
            process_gold=True
        )
        
        # BC uses CDM models
        bc_results = self.bc_pipeline.run_pipeline(
            tables=["Customer18", "GLEntry17"],
            incremental=False
        )
        
        # Create conformed dimensions using both
        conformed_results = self.create_conformed_dimensions()
```

## 4. Schema Evolution Workflow

### When APIs Change

1. **PSA Schema Update**:
   ```bash
   # Get new OpenAPI spec
   curl -o PSA_OpenAPI_schema_v2.json https://api.connectwise.com/v2/openapi
   
   # Regenerate models
   python regenerate_models.py --schema PSA_OpenAPI_schema_v2.json
   
   # Models automatically picked up by pipeline
   ```

2. **BC Schema Update**:
   ```bash
   # Get new CDM manifests
   # Place in cdm_manifests/
   
   # Regenerate models
   python fabric_api/generate_bc_models_camelcase.py
   
   # Pipeline uses new models immediately
   ```

## 5. Benefits of Full Integration

### Type Safety
```python
# Model validation ensures type safety
time_entry = TimeEntry(
    id=123,
    memberId=456,
    invoiceId="789"  # Type error! Expected int
)
```

### Schema Generation
```python
# Automatic Spark schema from models
time_schema = TimeEntry.model_spark_schema()
df = spark.read.schema(time_schema).json("data/time_entries.json")
```

### Field Documentation
```python
# Models include field descriptions
class Agreement(SparkModel):
    agreementNumber: str | None = Field(
        default=None,
        description="Unique agreement identifier"
    )
```

### Validation
```python
# Pydantic validation on data ingestion
try:
    agreement = Agreement(**raw_data)
except ValidationError as e:
    print(f"Invalid data: {e}")
```

## 6. Complete Example

```python
from fabric_api.unified_lakehouse_orchestrator import UnifiedLakehouseOrchestrator

# Initialize orchestrator
orchestrator = UnifiedLakehouseOrchestrator(spark)

# Run complete pipeline with model validation
results = orchestrator.run_full_pipeline(
    run_psa=True,
    run_bc=True,
    create_integrated=True,
    incremental=False
)

# Models are used throughout:
# 1. Data extraction validates against models
# 2. Bronze → Silver uses model schemas
# 3. Silver → Gold respects model field types
# 4. Conformed dimensions map model fields

# Query results using model field names
df = spark.sql("""
    SELECT 
        customer_name,   -- From PSA model
        No,             -- From BC model (Customer.No)
        agreementNumber, -- From PSA model
        PostingDate     -- From BC model (GLEntry.PostingDate)
    FROM gold.integrated.vw_unified_financial
    WHERE fiscal_year = 2024
""")
```

## 7. Troubleshooting Model Integration

### Check Model Schema
```python
# View model schema
model = TimeEntry
schema = model.model_spark_schema()
print("Fields:", schema.fieldNames())
print("Types:", [(f.name, f.dataType) for f in schema.fields])
```

### Validate Data Against Model
```python
# Test single record
test_data = {"id": 123, "memberId": 456}
try:
    time_entry = TimeEntry(**test_data)
    print("Valid!")
except ValidationError as e:
    print(f"Invalid: {e}")
```

### Compare Schemas
```python
# Compare model vs actual data
model_fields = set(TimeEntry.model_spark_schema().fieldNames())
data_fields = set(df.columns)
print("Missing in data:", model_fields - data_fields)
print("Extra in data:", data_fields - model_fields)
```

## 8. Best Practices

1. **Always Regenerate Models** when APIs change
2. **Use Model Validation** in bronze layer
3. **Cast Types** in silver layer based on models
4. **Document Model Changes** in version control
5. **Test Schema Evolution** before production

## Summary

The complete integration ensures:
- ✅ Type safety from API to analytics
- ✅ Automatic schema management
- ✅ Consistent field naming
- ✅ Easy schema evolution
- ✅ Cross-system compatibility

Both PSA and BC pipelines leverage their autogenerated models throughout the medallion architecture, providing a robust foundation for the unified lakehouse.