{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Unified ETL Pipeline - ConnectWise PSA\n",
    "This notebook tests the unified ETL framework with all ConnectWise entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the Unified ETL Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the wheel files from lakehouse\n",
    "import sys\n",
    "sys.path.append('/lakehouse/default/Files/dist/')\n",
    "\n",
    "# Or install with pip\n",
    "# %pip install /lakehouse/default/Files/dist/unified-etl-core-1.0.0-py3-none-any.whl\n",
    "# %pip install /lakehouse/default/Files/dist/unified-etl-connectwise-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Model Generation for All Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all generated models\n",
    "from unified_etl_connectwise.models import (\n",
    "    Agreement,\n",
    "    TimeEntry,\n",
    "    ExpenseEntry,\n",
    "    ProductItem,\n",
    "    PostedInvoice,\n",
    "    Invoice as UnpostedInvoice,  # UnpostedInvoice uses Invoice model\n",
    ")\n",
    "from unified_etl_connectwise.utils.api_utils import get_fields_for_api_call\n",
    "\n",
    "# Test that all models work\n",
    "model_mapping = {\n",
    "    \"Agreement\": Agreement,\n",
    "    \"TimeEntry\": TimeEntry,\n",
    "    \"ExpenseEntry\": ExpenseEntry,\n",
    "    \"ProductItem\": ProductItem,\n",
    "    \"PostedInvoice\": PostedInvoice,\n",
    "    \"UnpostedInvoice\": UnpostedInvoice,\n",
    "}\n",
    "\n",
    "print(\"Testing all ConnectWise models:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for entity_name, model_class in model_mapping.items():\n",
    "    # Test field generation\n",
    "    fields = get_fields_for_api_call(model_class, max_depth=2)\n",
    "    field_count = len(fields.split(','))\n",
    "    \n",
    "    # Test Spark schema generation\n",
    "    spark_schema = model_class.spark_schema()\n",
    "    \n",
    "    print(f\"\\n{entity_name}:\")\n",
    "    print(f\"  - API fields: {field_count}\")\n",
    "    print(f\"  - Spark schema fields: {len(spark_schema.fields)}\")\n",
    "    print(f\"  - Sample fields: {', '.join(fields.split(',')[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure ConnectWise Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "# Get credentials from Key Vault\n",
    "kv_name = \"your-keyvault-name\"  # Replace with your Key Vault name\n",
    "\n",
    "try:\n",
    "    # Try to get from Key Vault\n",
    "    cw_company = mssparkutils.credentials.getSecret(kv_name, \"CW-COMPANY\")\n",
    "    cw_public_key = mssparkutils.credentials.getSecret(kv_name, \"CW-PUBLIC-KEY\")\n",
    "    cw_private_key = mssparkutils.credentials.getSecret(kv_name, \"CW-PRIVATE-KEY\")\n",
    "    cw_base_url = mssparkutils.credentials.getSecret(kv_name, \"CW-BASE-URL\")\n",
    "except:\n",
    "    # Fallback to environment variables for testing\n",
    "    cw_company = os.environ.get(\"CW_COMPANY\", \"\")\n",
    "    cw_public_key = os.environ.get(\"CW_PUBLIC_KEY\", \"\")\n",
    "    cw_private_key = os.environ.get(\"CW_PRIVATE_KEY\", \"\")\n",
    "    cw_base_url = os.environ.get(\"CW_BASE_URL\", \"https://api-na.myconnectwise.net/v4_6_release/apis/3.0\")\n",
    "\n",
    "# Configure extractor\n",
    "config = {\n",
    "    \"base_url\": cw_base_url,\n",
    "    \"auth\": {\n",
    "        \"type\": \"api_key\",\n",
    "        \"credentials\": {\n",
    "            \"company\": cw_company,\n",
    "            \"public_key\": cw_public_key,\n",
    "            \"private_key\": cw_private_key,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured for company: {cw_company}\")\n",
    "print(f\"Base URL: {cw_base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Bronze Layer Extraction for All Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_connectwise.extract import ConnectWiseExtractor\n",
    "from datetime import datetime\n",
    "\n",
    "# Create extractor\n",
    "extractor = ConnectWiseExtractor(config)\n",
    "\n",
    "# Define endpoints for each entity\n",
    "entity_endpoints = {\n",
    "    \"Agreement\": \"/finance/agreements\",\n",
    "    \"TimeEntry\": \"/time/entries\",\n",
    "    \"ExpenseEntry\": \"/expense/entries\",\n",
    "    \"ProductItem\": \"/procurement/products\",\n",
    "    \"PostedInvoice\": \"/finance/invoices/posted\",\n",
    "    \"UnpostedInvoice\": \"/finance/invoices\",\n",
    "}\n",
    "\n",
    "# Extract a small sample from each entity\n",
    "bronze_base_path = \"/lakehouse/default/Tables/bronze\"\n",
    "extraction_results = {}\n",
    "\n",
    "for entity_name, endpoint in entity_endpoints.items():\n",
    "    print(f\"\\nExtracting {entity_name} from {endpoint}...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract with small page size for testing\n",
    "        df = extractor.extract(\n",
    "            endpoint=endpoint,\n",
    "            page_size=10,  # Small sample\n",
    "        )\n",
    "        \n",
    "        record_count = df.count()\n",
    "        extraction_results[entity_name] = {\n",
    "            \"success\": True,\n",
    "            \"count\": record_count,\n",
    "            \"df\": df\n",
    "        }\n",
    "        \n",
    "        # Save to bronze\n",
    "        bronze_path = f\"{bronze_base_path}/bronze_cw_{entity_name.lower()}\"\n",
    "        df.write.mode(\"overwrite\").format(\"delta\").save(bronze_path)\n",
    "        \n",
    "        print(f\"‚úÖ Extracted {record_count} records\")\n",
    "        print(f\"   Saved to: {bronze_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        extraction_results[entity_name] = {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        print(f\"‚ùå Failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data from successful extractions\n",
    "for entity_name, result in extraction_results.items():\n",
    "    if result[\"success\"] and result[\"count\"] > 0:\n",
    "        print(f\"\\n{entity_name} Sample (first 3 records):\")\n",
    "        print(\"=\" * 80)\n",
    "        result[\"df\"].show(3, truncate=False)\n",
    "        print(\"\\nSchema:\")\n",
    "        result[\"df\"].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Silver Layer Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_core.extract.base import validate_batch\n",
    "\n",
    "# Test validation for each entity type\n",
    "validation_results = {}\n",
    "\n",
    "for entity_name, result in extraction_results.items():\n",
    "    if result[\"success\"] and result[\"count\"] > 0:\n",
    "        print(f\"\\nValidating {entity_name}...\")\n",
    "        \n",
    "        # Get sample data as list of dicts\n",
    "        sample_data = result[\"df\"].limit(5).toPandas().to_dict('records')\n",
    "        \n",
    "        # Get model class\n",
    "        model_class = model_mapping[entity_name]\n",
    "        \n",
    "        # Validate\n",
    "        valid_models, errors = validate_batch(sample_data, model_class)\n",
    "        \n",
    "        validation_results[entity_name] = {\n",
    "            \"total\": len(sample_data),\n",
    "            \"valid\": len(valid_models),\n",
    "            \"errors\": len(errors)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Valid: {len(valid_models)}/{len(sample_data)}\")\n",
    "        if errors:\n",
    "            print(f\"‚ö†Ô∏è  Errors: {len(errors)}\")\n",
    "            print(f\"   First error: {errors[0]['errors'][0] if errors else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Unified ETL Pipeline Test Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nModel Generation:\")\n",
    "for entity in model_mapping.keys():\n",
    "    print(f\"  - {entity}: ‚úÖ\")\n",
    "\n",
    "print(\"\\nBronze Layer Extraction:\")\n",
    "for entity, result in extraction_results.items():\n",
    "    if result[\"success\"]:\n",
    "        print(f\"  - {entity}: ‚úÖ ({result['count']} records)\")\n",
    "    else:\n",
    "        print(f\"  - {entity}: ‚ùå ({result['error']})\")\n",
    "\n",
    "print(\"\\nSilver Layer Validation:\")\n",
    "for entity, result in validation_results.items():\n",
    "    print(f\"  - {entity}: {result['valid']}/{result['total']} valid\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Implement full Silver transformations (flattening, standardization)\")\n",
    "print(\"  2. Add Gold layer with business logic\")\n",
    "print(\"  3. Configure incremental processing\")\n",
    "print(\"  4. Add Business Central entities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}