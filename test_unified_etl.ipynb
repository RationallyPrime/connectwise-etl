{"cells":[{"cell_type":"markdown","source":["# Test Unified ETL Pipeline - ConnectWise PSA\n","This notebook tests the unified ETL framework with all ConnectWise entities"],"metadata":{},"id":"da4025b7-7f4a-4d2a-a20d-6fb8638065ab"},{"cell_type":"markdown","source":["## 1. Install the Unified ETL Packages"],"metadata":{},"id":"851c2ca9-7bfc-4f48-8cdb-3ca0d8f02fb7"},{"cell_type":"code","source":["\n","# Or install with pip\n","%pip install /lakehouse/default/Files/unified_etl_core-1.0.0-py3-none-any.whl\n","%pip install /lakehouse/default/Files/unified_etl_connectwise-1.0.0-py3-none-any.whl\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[3,4,5,6,7,8,9],"state":"finished","livy_statement_state":"available","session_id":"c4fd79d6-91cd-45aa-820c-b388960c303b","normalized_state":"finished","queued_time":"2025-05-26T16:46:20.4060452Z","session_start_time":"2025-05-26T16:46:20.4077656Z","execution_start_time":"2025-05-26T16:46:31.0341666Z","execution_finish_time":"2025-05-26T16:47:23.059446Z","parent_msg_id":"56975758-826d-4475-b375-9bab3c1b63f0"},"text/plain":"StatementMeta(, c4fd79d6-91cd-45aa-820c-b388960c303b, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing /lakehouse/default/Files/unified_etl_core-1.0.0-py3-none-any.whl\nCollecting delta-spark>=2.2.0 (from unified-etl-core==1.0.0)\n  Downloading delta_spark-3.3.1-py3-none-any.whl.metadata (1.9 kB)\nCollecting pydantic>=2.11.4 (from unified-etl-core==1.0.0)\n  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyspark>=3.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-core==1.0.0) (3.5.1.5.4.20240407)\nRequirement already satisfied: pyyaml>=6.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-core==1.0.0) (6.0.1)\nRequirement already satisfied: requests>=2.28.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-core==1.0.0) (2.31.0)\nCollecting sparkdantic (from unified-etl-core==1.0.0)\n  Downloading sparkdantic-2.4.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: tenacity>=8.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-core==1.0.0) (8.2.3)\nRequirement already satisfied: importlib-metadata>=1.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from delta-spark>=2.2.0->unified-etl-core==1.0.0) (7.0.1)\nCollecting pyspark>=3.3.0 (from unified-etl-core==1.0.0)\n  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25hCollecting annotated-types>=0.6.0 (from pydantic>=2.11.4->unified-etl-core==1.0.0)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.33.2 (from pydantic>=2.11.4->unified-etl-core==1.0.0)\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-extensions>=4.12.2 (from pydantic>=2.11.4->unified-etl-core==1.0.0)\n  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic>=2.11.4->unified-etl-core==1.0.0)\n  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: py4j==0.10.9.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pyspark>=3.3.0->unified-etl-core==1.0.0) (0.10.9.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-core==1.0.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-core==1.0.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-core==1.0.0) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-core==1.0.0) (2024.2.2)\nCollecting jmespath<2.0.0,>=1.0.1 (from sparkdantic->unified-etl-core==1.0.0)\n  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\nCollecting packaging<26.0,>=24.2 (from sparkdantic->unified-etl-core==1.0.0)\n  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting pyparsing<4.0.0,>=3.1.1 (from sparkdantic->unified-etl-core==1.0.0)\n  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: zipp>=0.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark>=2.2.0->unified-etl-core==1.0.0) (3.17.0)\nDownloading delta_spark-3.3.1-py3-none-any.whl (21 kB)\nDownloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.2/444.2 kB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sparkdantic-2.4.0-py3-none-any.whl (10 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\nDownloading packaging-25.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.5-py2.py3-none-any.whl size=317747860 sha256=381ee1c7a8c8c503c17dc9f8a91f2bf2494a838962982adf2cbe7a313afd5cca\n  Stored in directory: /home/trusted-service-user/.cache/pip/wheels/0c/7f/b4/0e68c6d8d89d2e582e5498ad88616c16d7c19028680e9d3840\nSuccessfully built pyspark\nInstalling collected packages: typing-extensions, pyspark, pyparsing, packaging, jmespath, annotated-types, typing-inspection, pydantic-core, delta-spark, pydantic, sparkdantic, unified-etl-core\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Not uninstalling typing-extensions at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: pyspark\n    Found existing installation: pyspark 3.5.1.5.4.20240407\n    Not uninstalling pyspark at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865\n    Can't uninstall 'pyspark'. No files were found to uninstall.\n  Attempting uninstall: pyparsing\n    Found existing installation: pyparsing 3.0.9\n    Not uninstalling pyparsing at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865\n    Can't uninstall 'pyparsing'. No files were found to uninstall.\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.1\n    Not uninstalling packaging at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865\n    Can't uninstall 'packaging'. No files were found to uninstall.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnni 3.0 requires filelock<3.12, but you have filelock 3.13.1 which is incompatible.\nmlflow-skinny 2.12.2 requires packaging<25, but you have packaging 25.0 which is incompatible.\ndatasets 2.19.1 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2024.6.1 which is incompatible.\nfsspec-wrapper 0.1.15 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed annotated-types-0.7.0 delta-spark-3.3.1 jmespath-1.0.1 packaging-25.0 pydantic-2.11.5 pydantic-core-2.33.2 pyparsing-3.2.3 pyspark-3.5.5 sparkdantic-2.4.0 typing-extensions-4.13.2 typing-inspection-0.4.1 unified-etl-core-1.0.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nProcessing /lakehouse/default/Files/unified_etl_connectwise-1.0.0-py3-none-any.whl\nRequirement already satisfied: pandas>=1.5.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-connectwise==1.0.0) (2.1.4)\nRequirement already satisfied: requests>=2.28.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-connectwise==1.0.0) (2.31.0)\nRequirement already satisfied: tenacity>=8.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-connectwise==1.0.0) (8.2.3)\nRequirement already satisfied: unified-etl-core in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from unified-etl-connectwise==1.0.0) (1.0.0)\nRequirement already satisfied: numpy<2,>=1.23.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.0->unified-etl-connectwise==1.0.0) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.0->unified-etl-connectwise==1.0.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.0->unified-etl-connectwise==1.0.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.0->unified-etl-connectwise==1.0.0) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-connectwise==1.0.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-connectwise==1.0.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-connectwise==1.0.0) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.28.0->unified-etl-connectwise==1.0.0) (2024.2.2)\nRequirement already satisfied: delta-spark>=2.2.0 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from unified-etl-core->unified-etl-connectwise==1.0.0) (3.3.1)\nRequirement already satisfied: pydantic>=2.11.4 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from unified-etl-core->unified-etl-connectwise==1.0.0) (2.11.5)\nRequirement already satisfied: pyspark>=3.3.0 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from unified-etl-core->unified-etl-connectwise==1.0.0) (3.5.5)\nRequirement already satisfied: pyyaml>=6.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from unified-etl-core->unified-etl-connectwise==1.0.0) (6.0.1)\nRequirement already satisfied: sparkdantic in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from unified-etl-core->unified-etl-connectwise==1.0.0) (2.4.0)\nRequirement already satisfied: importlib-metadata>=1.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from delta-spark>=2.2.0->unified-etl-core->unified-etl-connectwise==1.0.0) (7.0.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from pydantic>=2.11.4->unified-etl-core->unified-etl-connectwise==1.0.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from pydantic>=2.11.4->unified-etl-core->unified-etl-connectwise==1.0.0) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from pydantic>=2.11.4->unified-etl-core->unified-etl-connectwise==1.0.0) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from pydantic>=2.11.4->unified-etl-core->unified-etl-connectwise==1.0.0) (0.4.1)\nRequirement already satisfied: py4j==0.10.9.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pyspark>=3.3.0->unified-etl-core->unified-etl-connectwise==1.0.0) (0.10.9.7)\nRequirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->unified-etl-connectwise==1.0.0) (1.16.0)\nRequirement already satisfied: jmespath<2.0.0,>=1.0.1 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from sparkdantic->unified-etl-core->unified-etl-connectwise==1.0.0) (1.0.1)\nRequirement already satisfied: packaging<26.0,>=24.2 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from sparkdantic->unified-etl-core->unified-etl-connectwise==1.0.0) (25.0)\nRequirement already satisfied: pyparsing<4.0.0,>=3.1.1 in /nfs4/pyenv-4d508ba9-a8ec-47b9-b883-ffc30985c865/lib/python3.11/site-packages (from sparkdantic->unified-etl-core->unified-etl-connectwise==1.0.0) (3.2.3)\nRequirement already satisfied: zipp>=0.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark>=2.2.0->unified-etl-core->unified-etl-connectwise==1.0.0) (3.17.0)\nInstalling collected packages: unified-etl-connectwise\nSuccessfully installed unified-etl-connectwise-1.0.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1755c581-1d60-4f78-9db6-9565fff7ebbe"},{"cell_type":"markdown","source":["## 2. Test Model Generation for All Entities"],"metadata":{},"id":"9e27c716-d5a1-4348-9f8d-729783d3ac80"},{"cell_type":"code","source":["# Import all generated models\n","from unified_etl_connectwise.models import (\n","    Agreement,\n","    TimeEntry,\n","    ExpenseEntry,\n","    ProductItem,\n","    PostedInvoice,\n","    Invoice as UnpostedInvoice,  # UnpostedInvoice uses Invoice model\n",")\n","from unified_etl_connectwise.utils.api_utils import get_fields_for_api_call\n","\n","# Test that all models work\n","model_mapping = {\n","    \"Agreement\": Agreement,\n","    \"TimeEntry\": TimeEntry,\n","    \"ExpenseEntry\": ExpenseEntry,\n","    \"ProductItem\": ProductItem,\n","    \"PostedInvoice\": PostedInvoice,\n","    \"UnpostedInvoice\": UnpostedInvoice,\n","}\n","\n","print(\"Testing all ConnectWise models:\")\n","print(\"=\" * 50)\n","\n","for entity_name, model_class in model_mapping.items():\n","    # Test field generation\n","    fields = get_fields_for_api_call(model_class, max_depth=2)\n","    field_count = len(fields.split(','))\n","    \n","    # Test Spark schema generation\n","    spark_schema = model_class.model_spark_schema()\n","    \n","    print(f\"\\n{entity_name}:\")\n","    print(f\"  - API fields: {field_count}\")\n","    print(f\"  - Spark schema fields: {len(spark_schema.fields)}\")\n","    print(f\"  - Sample fields: {', '.join(fields.split(',')[:5])}...\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"c4fd79d6-91cd-45aa-820c-b388960c303b","normalized_state":"finished","queued_time":"2025-05-26T16:46:20.4081727Z","session_start_time":null,"execution_start_time":"2025-05-26T16:47:26.7193669Z","execution_finish_time":"2025-05-26T16:47:27.0474793Z","parent_msg_id":"b5fd304e-3403-4ad5-98ef-2e8cb7ad4d36"},"text/plain":"StatementMeta(, c4fd79d6-91cd-45aa-820c-b388960c303b, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Testing all ConnectWise models:\n==================================================\n\nAgreement:\n  - API fields: 88\n  - Spark schema fields: 82\n  - Sample fields: agreementStatus, allowOverruns, applicationCycle, applicationLimit, applicationUnits...\n\nTimeEntry:\n  - API fields: 61\n  - Spark schema fields: 62\n  - Sample fields: activity, actualHours, addToDetailDescriptionFlag, addToInternalAnalysisFlag, addToResolutionFlag...\n\nExpenseEntry:\n  - API fields: 32\n  - Spark schema fields: 31\n  - Sample fields: agreement, agreementAmount, amount, billAmount, billableOption...\n\nProductItem:\n  - API fields: 80\n  - Spark schema fields: 78\n  - Sample fields: addComponentsFlag, agreement, agreementAmount, asioSubscriptionsID, billableOption...\n\nPostedInvoice:\n  - API fields: 80\n  - Spark schema fields: 78\n  - Sample fields: accountNumber, addToBatchEmailList, adjustedBy, adjustmentReason, agreement...\n\nUnpostedInvoice:\n  - API fields: 80\n  - Spark schema fields: 78\n  - Sample fields: accountNumber, addToBatchEmailList, adjustedBy, adjustmentReason, agreement...\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35cc51d7-bd36-4cb4-b9f5-1935cc71440c"},{"cell_type":"markdown","source":["## 3. Configure ConnectWise Connection"],"metadata":{},"id":"27396df0-8942-41b7-88bd-7a6587dddbf9"},{"cell_type":"code","source":["# Set credentials directly\n","import os\n","\n","os.environ[\"CW_AUTH_USERNAME\"] = \"thekking+yemGyHDPdJ1hpuqx\"\n","os.environ[\"CW_AUTH_PASSWORD\"] = \"yMqpe26Jcu55FbQk\"\n","os.environ[\"CW_CLIENTID\"] = \"c7ea92d2-eaf5-4bfb-a09c-58d7f9dd7b81\"\n","os.environ[\"CW_BASE_URL\"] = \"https://verk.thekking.is/v4_6_release/apis/3.0\"\n","\n","# Configure extractor (even though it doesn't use these values, it expects a config)\n","config = {\n","    \"base_url\": os.environ[\"CW_BASE_URL\"],\n","    \"auth\": {\n","        \"type\": \"api_key\",\n","        \"credentials\": {\n","            \"company\": \"thekking\",\n","            \"public_key\": \"yemGyHDPdJ1hpuqx\",\n","            \"private_key\": \"yMqpe26Jcu55FbQk\",\n","            \"client_id\": \"c7ea92d2-eaf5-4bfb-a09c-58d7f9dd7b81\",\n","        }\n","    }\n","}\n","\n","print(f\"Configured for company: thekking\")\n","print(f\"Base URL: {os.environ['CW_BASE_URL']}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"c4fd79d6-91cd-45aa-820c-b388960c303b","normalized_state":"finished","queued_time":"2025-05-26T16:46:20.4100926Z","session_start_time":null,"execution_start_time":"2025-05-26T16:47:27.0498139Z","execution_finish_time":"2025-05-26T16:47:27.3090264Z","parent_msg_id":"4b928f1c-60f1-41d7-b208-4a2060f55239"},"text/plain":"StatementMeta(, c4fd79d6-91cd-45aa-820c-b388960c303b, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Configured for company: thekking\nBase URL: https://verk.thekking.is/v4_6_release/apis/3.0\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74f8c9e4-0d86-48d5-9ff0-8afb0fc1a432"},{"cell_type":"markdown","source":["## 4. Test Bronze Layer Extraction for All Entities"],"metadata":{},"id":"aeb47bf0-b00d-4e66-ba93-19e24f50b981"},{"cell_type":"code","source":["from unified_etl_connectwise.extract import ConnectWiseExtractor\n","from datetime import datetime\n","\n","# Create extractor\n","extractor = ConnectWiseExtractor(config)\n","\n","# Define endpoints for each entity\n","entity_endpoints = {\n","    \"Agreement\": \"/finance/agreements\",\n","    \"TimeEntry\": \"/time/entries\",\n","    \"ExpenseEntry\": \"/expense/entries\",\n","    \"ProductItem\": \"/procurement/products\",\n","    \"PostedInvoice\": \"/finance/invoices/posted\",\n","    \"UnpostedInvoice\": \"/finance/invoices\",\n","}\n","\n","# Extract a small sample from each entity\n","bronze_base_path = \"/lakehouse/default/Tables/bronze\"\n","extraction_results = {}\n","\n","for entity_name, endpoint in entity_endpoints.items():\n","    print(f\"\\nExtracting {entity_name} from {endpoint}...\")\n","    \n","    try:\n","        # Extract with small page size for testing\n","        df = extractor.extract(\n","            endpoint=endpoint,\n","            page_size=1000,  # Small sample\n","        )\n","        \n","        record_count = df.count()\n","        extraction_results[entity_name] = {\n","            \"success\": True,\n","            \"count\": record_count,\n","            \"df\": df\n","        }\n","        \n","        # Save to bronze\n","        bronze_path = f\"{bronze_base_path}/bronze_cw_{entity_name.lower()}\"\n","        df.write.mode(\"overwrite\").format(\"delta\").save(bronze_path)\n","        \n","        print(f\"✅ Extracted {record_count} records\")\n","        print(f\"   Saved to: {bronze_path}\")\n","        \n","    except Exception as e:\n","        extraction_results[entity_name] = {\n","            \"success\": False,\n","            \"error\": str(e)\n","        }\n","        print(f\"❌ Failed: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"submitted","livy_statement_state":"running","session_id":"c4fd79d6-91cd-45aa-820c-b388960c303b","normalized_state":"running","queued_time":"2025-05-26T16:46:20.4119893Z","session_start_time":null,"execution_start_time":"2025-05-26T16:47:27.3111802Z","execution_finish_time":null,"parent_msg_id":"7601b764-dd8b-48cb-92ac-45e73ef0e33f"},"text/plain":"StatementMeta(, c4fd79d6-91cd-45aa-820c-b388960c303b, 13, Submitted, Running, Running)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nExtracting Agreement from /finance/agreements...\n❌ Failed: An error occurred while calling o6477.save.\n: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/a3a23dd7-9f52-4b88-b056-46da3617c0b2/lakehouse/default/Tables/bronze/bronze_cw_agreement/_delta_log?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:779)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1067)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\n\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.listFrom(HadoopFileSystemLogStore.scala:87)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore(Checkpoints.scala:443)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore$(Checkpoints.scala:431)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpointBefore(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:398)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:386)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:375)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:369)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:368)\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:575)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:861)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:856)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:655)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:655)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:655)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:855)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:874)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:873)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:883)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:755)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:189)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:203)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:203)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:191)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:175)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:251)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:905)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:413)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:323)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:242)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\nExtracting TimeEntry from /time/entries...\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"adb7acb0-9901-4a7e-a080-eec54596f0b6"},{"cell_type":"markdown","source":["## 5. Display Sample Data"],"metadata":{},"id":"6c560f10-dd5a-42f9-bb4b-75ca4e8412e7"},{"cell_type":"code","source":["# Show sample data from successful extractions\n","for entity_name, result in extraction_results.items():\n","    if result[\"success\"] and result[\"count\"] > 0:\n","        print(f\"\\n{entity_name} Sample (first 3 records):\")\n","        print(\"=\" * 80)\n","        result[\"df\"].show(3, truncate=False)\n","        print(\"\\nSchema:\")\n","        result[\"df\"].printSchema()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-05-26T16:46:20.4138184Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"704009e6-2deb-498b-aa65-4cf1fb223dc5"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"48418a0d-2568-4d52-8b8d-b5d1db6fd87f"},{"cell_type":"markdown","source":["## 6. Test Silver Layer Validation"],"metadata":{},"id":"6c12ee54-2eba-4fbf-a3bf-e966b60b766d"},{"cell_type":"code","source":["from unified_etl_core.extract.base import validate_batch\n","\n","# Test validation for each entity type\n","validation_results = {}\n","\n","for entity_name, result in extraction_results.items():\n","    if result[\"success\"] and result[\"count\"] > 0:\n","        print(f\"\\nValidating {entity_name}...\")\n","        \n","        # Get sample data as list of dicts\n","        sample_data = result[\"df\"].limit(5).toPandas().to_dict('records')\n","        \n","        # Get model class\n","        model_class = model_mapping[entity_name]\n","        \n","        # Validate\n","        valid_models, errors = validate_batch(sample_data, model_class)\n","        \n","        validation_results[entity_name] = {\n","            \"total\": len(sample_data),\n","            \"valid\": len(valid_models),\n","            \"errors\": len(errors)\n","        }\n","        \n","        print(f\"✅ Valid: {len(valid_models)}/{len(sample_data)}\")\n","        if errors:\n","            print(f\"⚠️  Errors: {len(errors)}\")\n","            print(f\"   First error: {errors[0]['errors'][0] if errors else 'None'}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-05-26T16:46:20.4156238Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"0a756ef4-f8bb-40b3-a1e9-f1ed2fe8f359"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"14da2c74-e43e-4191-a2b3-45b92382f019"},{"cell_type":"markdown","source":["## 7. Summary Report"],"metadata":{},"id":"63a69977-1bef-423e-a82a-0c65e9db6e93"},{"cell_type":"code","source":["print(\"🎉 Unified ETL Pipeline Test Summary\")\n","print(\"=\" * 50)\n","print(\"\\nModel Generation:\")\n","for entity in model_mapping.keys():\n","    print(f\"  - {entity}: ✅\")\n","\n","print(\"\\nBronze Layer Extraction:\")\n","for entity, result in extraction_results.items():\n","    if result[\"success\"]:\n","        print(f\"  - {entity}: ✅ ({result['count']} records)\")\n","    else:\n","        print(f\"  - {entity}: ❌ ({result['error']})\")\n","\n","print(\"\\nSilver Layer Validation:\")\n","for entity, result in validation_results.items():\n","    print(f\"  - {entity}: {result['valid']}/{result['total']} valid\")\n","\n","print(\"\\nNext Steps:\")\n","print(\"  1. Implement full Silver transformations (flattening, standardization)\")\n","print(\"  2. Add Gold layer with business logic\")\n","print(\"  3. Configure incremental processing\")\n","print(\"  4. Add Business Central entities\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-05-26T16:46:20.4174849Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"a0c7bb87-67ae-46d5-af11-17eedb751136"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba4ad1e0-ab24-423f-beff-522ec7f4d697"}],"metadata":{"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"22176b2b-5fe3-46bb-8b0a-193e1356c3cf"}],"default_lakehouse":"22176b2b-5fe3-46bb-8b0a-193e1356c3cf","default_lakehouse_name":"Lakehouse","default_lakehouse_workspace_id":"a3a23dd7-9f52-4b88-b056-46da3617c0b2"}}},"nbformat":4,"nbformat_minor":5}