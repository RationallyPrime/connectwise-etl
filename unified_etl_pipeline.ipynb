{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified ETL Pipeline - ConnectWise PSA\n",
    "\n",
    "This notebook provides a complete ETL pipeline for ConnectWise data using the unified framework.\n",
    "It supports both full refresh and incremental processing modes.\n",
    "\n",
    "## Features:\n",
    "- Bronze layer: API extraction with validation\n",
    "- Silver layer: Schema transformation and flattening\n",
    "- Gold layer: Business logic and fact tables\n",
    "- Incremental processing: Only process changed records\n",
    "- Dimension generation: Create dimension tables from enums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install the unified ETL packages from wheel files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the wheel files\n",
    "%pip install /lakehouse/default/Files/unified_etl_core-1.0.0-py3-none-any.whl\n",
    "%pip install /lakehouse/default/Files/unified_etl_connectwise-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get Spark session\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Configuration parameters\n",
    "INCREMENTAL_MODE = True  # Set to False for full refresh\n",
    "DAYS_TO_REFRESH = 30    # How many days back to refresh for incremental\n",
    "PROCESS_LAYERS = [\"bronze\", \"silver\", \"gold\"]  # Which layers to process\n",
    "\n",
    "# Entity configuration\n",
    "ENTITIES = [\n",
    "    \"Agreement\",\n",
    "    \"TimeEntry\", \n",
    "    \"ExpenseEntry\",\n",
    "    \"ProductItem\",\n",
    "    \"PostedInvoice\",\n",
    "    \"UnpostedInvoice\"\n",
    "]\n",
    "\n",
    "# Endpoint mapping\n",
    "ENTITY_ENDPOINTS = {\n",
    "    \"Agreement\": \"/finance/agreements\",\n",
    "    \"TimeEntry\": \"/time/entries\",\n",
    "    \"ExpenseEntry\": \"/expense/entries\",\n",
    "    \"ProductItem\": \"/procurement/products\",\n",
    "    \"PostedInvoice\": \"/finance/invoices/posted\",\n",
    "    \"UnpostedInvoice\": \"/finance/invoices\",\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Mode: {'Incremental' if INCREMENTAL_MODE else 'Full Refresh'}\")\n",
    "print(f\"  Days to refresh: {DAYS_TO_REFRESH}\")\n",
    "print(f\"  Layers: {PROCESS_LAYERS}\")\n",
    "print(f\"  Entities: {len(ENTITIES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Existing Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_tables():\n",
    "    \"\"\"Check what tables exist in each layer.\"\"\"\n",
    "    table_info = {}\n",
    "    \n",
    "    for schema in [\"bronze\", \"silver\", \"gold\"]:\n",
    "        try:\n",
    "            tables = spark.sql(f\"SHOW TABLES IN {schema}\").collect()\n",
    "            schema_tables = {}\n",
    "            \n",
    "            for row in tables:\n",
    "                table_name = row.tableName\n",
    "                full_name = f\"{schema}.{table_name}\"\n",
    "                try:\n",
    "                    count = spark.sql(f\"SELECT COUNT(*) FROM {full_name}\").collect()[0][0]\n",
    "                    schema_tables[table_name] = count\n",
    "                except:\n",
    "                    schema_tables[table_name] = None\n",
    "                    \n",
    "            table_info[schema] = schema_tables\n",
    "            print(f\"\\n{schema.upper()} schema: {len(tables)} tables\")\n",
    "            for table, count in sorted(schema_tables.items()):\n",
    "                if count is not None:\n",
    "                    print(f\"  {table}: {count:,} rows\")\n",
    "                else:\n",
    "                    print(f\"  {table}: <error counting>\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n{schema.upper()} schema: Not found or error - {str(e)}\")\n",
    "            table_info[schema] = {}\n",
    "    \n",
    "    return table_info\n",
    "\n",
    "print(\"=== Existing Table Status ===\")\n",
    "existing_tables = check_existing_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bronze Layer - Extract from ConnectWise API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_connectwise import ConnectWiseClient\n",
    "from unified_etl_connectwise.api_utils import build_condition_string\n",
    "\n",
    "def extract_bronze_data(incremental=True, days_back=30):\n",
    "    \"\"\"Extract data from ConnectWise API to Bronze layer.\"\"\"\n",
    "    client = ConnectWiseClient()\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate date threshold for incremental\n",
    "    if incremental:\n",
    "        since_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n",
    "        print(f\"\\nIncremental mode: Fetching data since {since_date}\")\n",
    "    else:\n",
    "        print(f\"\\nFull refresh mode: Fetching all data\")\n",
    "    \n",
    "    for entity_name, endpoint in ENTITY_ENDPOINTS.items():\n",
    "        print(f\"\\nExtracting {entity_name} from {endpoint}...\")\n",
    "        \n",
    "        try:\n",
    "            # Build conditions based on entity type and mode\n",
    "            conditions = None\n",
    "            order_by = \"id\"\n",
    "            \n",
    "            if incremental:\n",
    "                if entity_name in [\"TimeEntry\", \"ExpenseEntry\"]:\n",
    "                    conditions = build_condition_string(date_entered_gte=since_date)\n",
    "                    order_by = \"dateEntered desc\"\n",
    "                else:\n",
    "                    conditions = f\"(lastUpdated>=[{since_date}])\"\n",
    "                    order_by = \"lastUpdated desc\"\n",
    "            \n",
    "            # Extract data\n",
    "            df = client.extract(\n",
    "                endpoint=endpoint,\n",
    "                conditions=conditions,\n",
    "                order_by=order_by,\n",
    "                page_size=1000\n",
    "            )\n",
    "            \n",
    "            # Add ETL metadata\n",
    "            df = df.withColumn(\"_etl_timestamp\", F.current_timestamp())\n",
    "            df = df.withColumn(\"_etl_source\", F.lit(\"connectwise\"))\n",
    "            df = df.withColumn(\"_etl_batch_id\", F.lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "            \n",
    "            record_count = df.count()\n",
    "            results[entity_name] = {\n",
    "                \"df\": df,\n",
    "                \"count\": record_count,\n",
    "                \"success\": True\n",
    "            }\n",
    "            print(f\"  ‚úÖ Extracted {record_count:,} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[entity_name] = {\n",
    "                \"df\": None,\n",
    "                \"count\": 0,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute Bronze extraction\n",
    "if \"bronze\" in PROCESS_LAYERS:\n",
    "    print(\"\\n=== BRONZE LAYER PROCESSING ===\")\n",
    "    bronze_results = extract_bronze_data(incremental=INCREMENTAL_MODE, days_back=DAYS_TO_REFRESH)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nBronze Extraction Summary:\")\n",
    "    total_records = sum(r[\"count\"] for r in bronze_results.values() if r[\"success\"])\n",
    "    print(f\"  Total records extracted: {total_records:,}\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping Bronze layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write Bronze Data with MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bronze_data(results, incremental=True):\n",
    "    \"\"\"Write extracted data to Bronze tables using MERGE for incremental.\"\"\"\n",
    "    write_summary = {}\n",
    "    \n",
    "    for entity_name, result in results.items():\n",
    "        if not result[\"success\"] or result[\"count\"] == 0:\n",
    "            continue\n",
    "            \n",
    "        bronze_table = f\"bronze.bronze_cw_{entity_name.lower()}\"\n",
    "        df = result[\"df\"]\n",
    "        \n",
    "        try:\n",
    "            # Check if table exists\n",
    "            table_exists = spark.catalog.tableExists(bronze_table)\n",
    "            \n",
    "            if incremental and table_exists:\n",
    "                # Deduplicate incoming data\n",
    "                df_deduped = df.dropDuplicates([\"id\"])\n",
    "                if df_deduped.count() < df.count():\n",
    "                    print(f\"\\n{entity_name}: Removed {df.count() - df_deduped.count()} duplicates\")\n",
    "                \n",
    "                # Use MERGE for incremental updates\n",
    "                temp_view = f\"temp_{entity_name.lower()}_updates\"\n",
    "                df_deduped.createOrReplaceTempView(temp_view)\n",
    "                \n",
    "                # Get columns for merge\n",
    "                merge_key = \"id\"\n",
    "                update_cols = [col for col in df.columns if col != merge_key]\n",
    "                update_expr = \", \".join([f\"target.{col} = source.{col}\" for col in update_cols])\n",
    "                insert_cols = \", \".join(df.columns)\n",
    "                insert_values = \", \".join([f\"source.{col}\" for col in df.columns])\n",
    "                \n",
    "                merge_sql = f\"\"\"\n",
    "                MERGE INTO {bronze_table} AS target\n",
    "                USING {temp_view} AS source\n",
    "                ON target.{merge_key} = source.{merge_key}\n",
    "                WHEN MATCHED THEN \n",
    "                    UPDATE SET {update_expr}\n",
    "                WHEN NOT MATCHED THEN\n",
    "                    INSERT ({insert_cols}) VALUES ({insert_values})\n",
    "                \"\"\"\n",
    "                \n",
    "                spark.sql(merge_sql)\n",
    "                spark.catalog.dropTempView(temp_view)\n",
    "                \n",
    "                # Get final count\n",
    "                final_count = spark.sql(f\"SELECT COUNT(*) FROM {bronze_table}\").collect()[0][0]\n",
    "                write_summary[entity_name] = {\n",
    "                    \"mode\": \"merge\",\n",
    "                    \"records_processed\": df_deduped.count(),\n",
    "                    \"final_count\": final_count\n",
    "                }\n",
    "                print(f\"  ‚úÖ Merged {df_deduped.count()} records into {bronze_table} (total: {final_count:,})\")\n",
    "                \n",
    "            else:\n",
    "                # Full overwrite or new table\n",
    "                df_deduped = df.dropDuplicates([\"id\"])\n",
    "                df_deduped.write.mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "                \n",
    "                write_summary[entity_name] = {\n",
    "                    \"mode\": \"overwrite\",\n",
    "                    \"records_processed\": df_deduped.count(),\n",
    "                    \"final_count\": df_deduped.count()\n",
    "                }\n",
    "                print(f\"  ‚úÖ Wrote {df_deduped.count()} records to {bronze_table}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            write_summary[entity_name] = {\"error\": str(e)}\n",
    "            print(f\"  ‚ùå Failed to write {entity_name}: {str(e)}\")\n",
    "    \n",
    "    return write_summary\n",
    "\n",
    "# Write Bronze data\n",
    "if \"bronze\" in PROCESS_LAYERS and 'bronze_results' in locals():\n",
    "    print(\"\\n=== Writing Bronze Data ===\")\n",
    "    bronze_write_summary = write_bronze_data(bronze_results, incremental=INCREMENTAL_MODE)\n",
    "    \n",
    "    # Store entities that were updated for Silver processing\n",
    "    bronze_updated_entities = [name for name, summary in bronze_write_summary.items() \n",
    "                              if \"error\" not in summary]\n",
    "    print(f\"\\nBronze updated entities: {bronze_updated_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Silver Layer - Transform and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_connectwise.config import SILVER_CONFIG\n",
    "from unified_etl_core.silver import apply_silver_transformations\n",
    "import unified_etl_connectwise.models.models as cw_models\n",
    "\n",
    "def process_silver_layer(entities_to_process=None, incremental=True):\n",
    "    \"\"\"Process Bronze to Silver transformations.\"\"\"\n",
    "    \n",
    "    # Model mapping\n",
    "    model_mapping = {\n",
    "        \"Agreement\": cw_models.Agreement,\n",
    "        \"TimeEntry\": cw_models.TimeEntry,\n",
    "        \"ExpenseEntry\": cw_models.ExpenseEntry,\n",
    "        \"UnpostedInvoice\": cw_models.Invoice,\n",
    "        \"PostedInvoice\": cw_models.Invoice,\n",
    "        \"ProductItem\": cw_models.ProductItem,\n",
    "    }\n",
    "    \n",
    "    # If no specific entities provided, process all configured ones\n",
    "    if entities_to_process is None:\n",
    "        entities_to_process = list(SILVER_CONFIG[\"entities\"].keys())\n",
    "    \n",
    "    silver_summary = {}\n",
    "    silver_changes = {}  # Track what changed for Gold processing\n",
    "    \n",
    "    for entity_name in entities_to_process:\n",
    "        if entity_name not in SILVER_CONFIG[\"entities\"]:\n",
    "            print(f\"\\n‚ö†Ô∏è  {entity_name} not configured for Silver processing\")\n",
    "            continue\n",
    "            \n",
    "        entity_config = SILVER_CONFIG[\"entities\"][entity_name]\n",
    "        bronze_table = entity_config[\"bronze_table\"]\n",
    "        silver_table = entity_config[\"silver_table\"]\n",
    "        \n",
    "        # Check if we're using schema prefix\n",
    "        if not bronze_table.startswith(\"bronze.\"):\n",
    "            bronze_table = f\"bronze.{bronze_table}\"\n",
    "        if not silver_table.startswith(\"silver.\"):\n",
    "            silver_table = f\"silver.{silver_table}\"\n",
    "        \n",
    "        print(f\"\\nProcessing {entity_name}: {bronze_table} ‚Üí {silver_table}\")\n",
    "        \n",
    "        try:\n",
    "            # Read Bronze data\n",
    "            if incremental and spark.catalog.tableExists(silver_table):\n",
    "                # Get last refresh timestamp from Silver table\n",
    "                last_refresh = spark.sql(f\"\"\"\n",
    "                    SELECT MAX(_etl_timestamp) as last_refresh \n",
    "                    FROM {silver_table}\n",
    "                \"\"\").collect()[0][0]\n",
    "                \n",
    "                if last_refresh:\n",
    "                    # Only process new/changed records\n",
    "                    bronze_df = spark.sql(f\"\"\"\n",
    "                        SELECT * FROM {bronze_table}\n",
    "                        WHERE _etl_timestamp > '{last_refresh}'\n",
    "                    \"\"\")\n",
    "                    print(f\"  Incremental: Processing records since {last_refresh}\")\n",
    "                else:\n",
    "                    bronze_df = spark.table(bronze_table)\n",
    "                    print(f\"  No timestamp found, processing all records\")\n",
    "            else:\n",
    "                # Full refresh\n",
    "                bronze_df = spark.table(bronze_table)\n",
    "                print(f\"  Full refresh: Processing all records\")\n",
    "            \n",
    "            record_count = bronze_df.count()\n",
    "            if record_count == 0:\n",
    "                print(f\"  No new records to process\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Found {record_count:,} records to process\")\n",
    "            \n",
    "            # Track changed IDs for Gold processing\n",
    "            silver_changes[entity_name] = bronze_df.select(\"id\").distinct()\n",
    "            \n",
    "            # Get model class\n",
    "            model_class = model_mapping.get(entity_name)\n",
    "            if not model_class:\n",
    "                print(f\"  ‚ö†Ô∏è  No model class found for {entity_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Apply Silver transformations\n",
    "            silver_df = apply_silver_transformations(\n",
    "                df=bronze_df,\n",
    "                entity_config=entity_config,\n",
    "                model_class=model_class,\n",
    "            )\n",
    "            \n",
    "            # Write to Silver\n",
    "            if incremental and spark.catalog.tableExists(silver_table):\n",
    "                # Use MERGE for incremental\n",
    "                temp_view = f\"temp_silver_{entity_name.lower()}\"\n",
    "                silver_df.createOrReplaceTempView(temp_view)\n",
    "                \n",
    "                business_keys = entity_config.get(\"business_keys\", [\"id\"])\n",
    "                merge_conditions = \" AND \".join([f\"target.{key} = source.{key}\" for key in business_keys])\n",
    "                \n",
    "                merge_sql = f\"\"\"\n",
    "                MERGE INTO {silver_table} AS target\n",
    "                USING {temp_view} AS source\n",
    "                ON {merge_conditions}\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "                \"\"\"\n",
    "                \n",
    "                spark.sql(merge_sql)\n",
    "                spark.catalog.dropTempView(temp_view)\n",
    "                \n",
    "                final_count = spark.sql(f\"SELECT COUNT(*) FROM {silver_table}\").collect()[0][0]\n",
    "                print(f\"  ‚úÖ Merged {record_count} records into {silver_table} (total: {final_count:,})\")\n",
    "            else:\n",
    "                # Full overwrite\n",
    "                silver_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(silver_table)\n",
    "                print(f\"  ‚úÖ Wrote {silver_df.count()} records to {silver_table}\")\n",
    "            \n",
    "            silver_summary[entity_name] = {\n",
    "                \"success\": True,\n",
    "                \"records_processed\": record_count,\n",
    "                \"columns\": len(silver_df.columns)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            silver_summary[entity_name] = {\"success\": False, \"error\": str(e)}\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    return silver_summary, silver_changes\n",
    "\n",
    "# Execute Silver processing\n",
    "if \"silver\" in PROCESS_LAYERS:\n",
    "    print(\"\\n=== SILVER LAYER PROCESSING ===\")\n",
    "    \n",
    "    # Determine which entities to process\n",
    "    if 'bronze_updated_entities' in locals() and INCREMENTAL_MODE:\n",
    "        # Only process entities that had Bronze updates\n",
    "        entities_for_silver = bronze_updated_entities\n",
    "    else:\n",
    "        # Process all entities\n",
    "        entities_for_silver = None\n",
    "    \n",
    "    silver_summary, silver_changes = process_silver_layer(\n",
    "        entities_to_process=entities_for_silver,\n",
    "        incremental=INCREMENTAL_MODE\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nSilver Processing Summary:\")\n",
    "    for entity, summary in silver_summary.items():\n",
    "        if summary.get(\"success\"):\n",
    "            print(f\"  {entity}: ‚úÖ {summary['records_processed']} records, {summary['columns']} columns\")\n",
    "        else:\n",
    "            print(f\"  {entity}: ‚ùå {summary.get('error', 'Unknown error')}\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping Silver layer\")\n",
    "    silver_changes = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gold Layer - Create Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_connectwise.transforms import (\n",
    "    create_agreement_period_fact,\n",
    "    create_expense_entry_fact,\n",
    "    create_invoice_line_fact,\n",
    "    create_time_entry_fact,\n",
    ")\n",
    "\n",
    "def process_gold_layer(silver_changes=None, incremental=True):\n",
    "    \"\"\"Create Gold fact tables with business logic.\"\"\"\n",
    "    gold_summary = {}\n",
    "    \n",
    "    # Time Entry Facts\n",
    "    if not silver_changes or \"TimeEntry\" in silver_changes or \"Agreement\" in silver_changes:\n",
    "        print(\"\\nüìä Creating Time Entry Facts...\")\n",
    "        try:\n",
    "            # Read required tables\n",
    "            time_entry_df = spark.table(\"silver.silver_cw_timeentry\")\n",
    "            agreement_df = spark.table(\"silver.silver_cw_agreement\") if spark.catalog.tableExists(\"silver.silver_cw_agreement\") else None\n",
    "            member_df = spark.table(\"silver.silver_cw_member\") if spark.catalog.tableExists(\"silver.silver_cw_member\") else None\n",
    "            \n",
    "            # Filter to changed records if incremental\n",
    "            if incremental and silver_changes and \"TimeEntry\" in silver_changes:\n",
    "                changed_ids = silver_changes[\"TimeEntry\"]\n",
    "                changed_ids.createOrReplaceTempView(\"temp_changed_time_ids\")\n",
    "                time_entry_df = spark.sql(\"\"\"\n",
    "                    SELECT t.* FROM silver.silver_cw_timeentry t\n",
    "                    JOIN temp_changed_time_ids c ON t.id = c.id\n",
    "                \"\"\")\n",
    "                print(f\"  Processing {time_entry_df.count()} changed time entries\")\n",
    "            \n",
    "            # Create fact table\n",
    "            fact_df = create_time_entry_fact(\n",
    "                spark=spark,\n",
    "                time_entry_df=time_entry_df,\n",
    "                agreement_df=agreement_df,\n",
    "                member_df=member_df,\n",
    "            )\n",
    "            \n",
    "            # Write fact table\n",
    "            if incremental and spark.catalog.tableExists(\"gold.gold_fact_time_entry\"):\n",
    "                # MERGE for incremental\n",
    "                fact_df.createOrReplaceTempView(\"temp_fact_time_entry\")\n",
    "                spark.sql(\"\"\"\n",
    "                    MERGE INTO gold.gold_fact_time_entry AS target\n",
    "                    USING temp_fact_time_entry AS source\n",
    "                    ON target.timeEntryId = source.timeEntryId\n",
    "                    WHEN MATCHED THEN UPDATE SET *\n",
    "                    WHEN NOT MATCHED THEN INSERT *\n",
    "                \"\"\")\n",
    "                final_count = spark.sql(\"SELECT COUNT(*) FROM gold.gold_fact_time_entry\").collect()[0][0]\n",
    "                print(f\"  ‚úÖ Time Entry Facts: Merged {fact_df.count()} records (total: {final_count:,})\")\n",
    "            else:\n",
    "                fact_df.write.mode(\"overwrite\").saveAsTable(\"gold.gold_fact_time_entry\")\n",
    "                print(f\"  ‚úÖ Time Entry Facts: Created with {fact_df.count():,} records\")\n",
    "            \n",
    "            gold_summary[\"time_entry_fact\"] = {\"success\": True, \"count\": fact_df.count()}\n",
    "            \n",
    "        except Exception as e:\n",
    "            gold_summary[\"time_entry_fact\"] = {\"success\": False, \"error\": str(e)}\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    # Invoice Line Facts (includes uninvoiced billable work)\n",
    "    if not silver_changes or any(e in silver_changes for e in [\"TimeEntry\", \"UnpostedInvoice\", \"PostedInvoice\", \"ProductItem\"]):\n",
    "        print(\"\\nüí∞ Creating Invoice Line Facts...\")\n",
    "        try:\n",
    "            # Read required tables\n",
    "            invoice_df = spark.table(\"silver.silver_cw_invoice\")\n",
    "            time_entry_df = spark.table(\"silver.silver_cw_timeentry\") if spark.catalog.tableExists(\"silver.silver_cw_timeentry\") else None\n",
    "            product_df = spark.table(\"silver.silver_cw_productitem\") if spark.catalog.tableExists(\"silver.silver_cw_productitem\") else None\n",
    "            agreement_df = spark.table(\"silver.silver_cw_agreement\") if spark.catalog.tableExists(\"silver.silver_cw_agreement\") else None\n",
    "            \n",
    "            # Create fact table (now includes uninvoiced entries)\n",
    "            fact_df = create_invoice_line_fact(\n",
    "                spark=spark,\n",
    "                invoice_df=invoice_df,\n",
    "                time_entry_df=time_entry_df,\n",
    "                product_df=product_df,\n",
    "                agreement_df=agreement_df,\n",
    "            )\n",
    "            \n",
    "            # Write fact table\n",
    "            fact_df.write.mode(\"overwrite\").saveAsTable(\"gold.gold_fact_invoice_line\")\n",
    "            print(f\"  ‚úÖ Invoice Line Facts: Created with {fact_df.count():,} records (includes uninvoiced)\")\n",
    "            \n",
    "            gold_summary[\"invoice_line_fact\"] = {\"success\": True, \"count\": fact_df.count()}\n",
    "            \n",
    "        except Exception as e:\n",
    "            gold_summary[\"invoice_line_fact\"] = {\"success\": False, \"error\": str(e)}\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    # Agreement Period Facts\n",
    "    if not silver_changes or \"Agreement\" in silver_changes:\n",
    "        print(\"\\nüìÖ Creating Agreement Period Facts...\")\n",
    "        try:\n",
    "            agreement_df = spark.table(\"silver.silver_cw_agreement\")\n",
    "            \n",
    "            # Create monthly period facts\n",
    "            fact_df = create_agreement_period_fact(\n",
    "                spark=spark,\n",
    "                agreement_df=agreement_df,\n",
    "                config={\"start_date\": \"2020-01-01\", \"frequency\": \"month\"}\n",
    "            )\n",
    "            \n",
    "            # Write fact table\n",
    "            fact_df.write.mode(\"overwrite\").saveAsTable(\"gold.gold_fact_agreement_period\")\n",
    "            print(f\"  ‚úÖ Agreement Period Facts: Created with {fact_df.count():,} records\")\n",
    "            \n",
    "            gold_summary[\"agreement_period_fact\"] = {\"success\": True, \"count\": fact_df.count()}\n",
    "            \n",
    "        except Exception as e:\n",
    "            gold_summary[\"agreement_period_fact\"] = {\"success\": False, \"error\": str(e)}\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    # Expense Entry Facts\n",
    "    if not silver_changes or \"ExpenseEntry\" in silver_changes:\n",
    "        print(\"\\nüí≥ Creating Expense Entry Facts...\")\n",
    "        try:\n",
    "            expense_df = spark.table(\"silver.silver_cw_expenseentry\")\n",
    "            agreement_df = spark.table(\"silver.silver_cw_agreement\") if spark.catalog.tableExists(\"silver.silver_cw_agreement\") else None\n",
    "            \n",
    "            # Create fact table\n",
    "            fact_df = create_expense_entry_fact(\n",
    "                spark=spark,\n",
    "                expense_df=expense_df,\n",
    "                agreement_df=agreement_df,\n",
    "            )\n",
    "            \n",
    "            # Write fact table\n",
    "            fact_df.write.mode(\"overwrite\").saveAsTable(\"gold.gold_fact_expense_entry\")\n",
    "            print(f\"  ‚úÖ Expense Entry Facts: Created with {fact_df.count():,} records\")\n",
    "            \n",
    "            gold_summary[\"expense_entry_fact\"] = {\"success\": True, \"count\": fact_df.count()}\n",
    "            \n",
    "        except Exception as e:\n",
    "            gold_summary[\"expense_entry_fact\"] = {\"success\": False, \"error\": str(e)}\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    return gold_summary\n",
    "\n",
    "# Execute Gold processing\n",
    "if \"gold\" in PROCESS_LAYERS:\n",
    "    print(\"\\n=== GOLD LAYER PROCESSING ===\")\n",
    "    gold_summary = process_gold_layer(\n",
    "        silver_changes=silver_changes if INCREMENTAL_MODE else None,\n",
    "        incremental=INCREMENTAL_MODE\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nGold Processing Summary:\")\n",
    "    for fact_name, summary in gold_summary.items():\n",
    "        if summary.get(\"success\"):\n",
    "            print(f\"  {fact_name}: ‚úÖ {summary['count']:,} records\")\n",
    "        else:\n",
    "            print(f\"  {fact_name}: ‚ùå {summary.get('error', 'Unknown error')}\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping Gold layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_core.date_utils import generate_date_dimension\n",
    "from unified_etl_core.dimensions import create_dimension_from_column\n",
    "\n",
    "def generate_dimensions():\n",
    "    \"\"\"Generate dimension tables from Silver data.\"\"\"\n",
    "    \n",
    "    # Generate date dimension if it doesn't exist\n",
    "    if not spark.catalog.tableExists(\"gold.gold_dim_date\"):\n",
    "        print(\"\\nüìÖ Creating Date Dimension...\")\n",
    "        try:\n",
    "            date_dim = generate_date_dimension(\n",
    "                spark=spark,\n",
    "                start_date=\"2020-01-01\",\n",
    "                end_date=\"2030-12-31\",\n",
    "                fiscal_year_start_month=7,\n",
    "            )\n",
    "            date_dim.write.mode(\"overwrite\").saveAsTable(\"gold.gold_dim_date\")\n",
    "            print(f\"  ‚úÖ Created dim_date with {date_dim.count():,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    # Define dimensions to generate\n",
    "    dimension_configs = [\n",
    "        # From TimeEntry\n",
    "        {\"source_table\": \"silver_cw_timeentry\", \"column\": \"billableOption\", \"dimension_name\": \"dim_billable_option\"},\n",
    "        {\"source_table\": \"silver_cw_timeentry\", \"column\": \"status\", \"dimension_name\": \"dim_time_status\"},\n",
    "        {\"source_table\": \"silver_cw_timeentry\", \"column\": \"chargeToType\", \"dimension_name\": \"dim_charge_type\"},\n",
    "        # From Agreement\n",
    "        {\"source_table\": \"silver_cw_agreement\", \"column\": \"agreementStatus\", \"dimension_name\": \"dim_agreement_status\"},\n",
    "        {\"source_table\": \"silver_cw_agreement\", \"column\": \"billCycleIdentifier\", \"dimension_name\": \"dim_bill_cycle\"},\n",
    "        # From Invoice\n",
    "        {\"source_table\": \"silver_cw_invoice\", \"column\": \"statusName\", \"dimension_name\": \"dim_invoice_status\"},\n",
    "        # From ExpenseEntry\n",
    "        {\"source_table\": \"silver_cw_expenseentry\", \"column\": \"typeName\", \"dimension_name\": \"dim_expense_type\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüî∑ Generating Dimension Tables...\")\n",
    "    dimension_summary = {}\n",
    "    \n",
    "    for config in dimension_configs:\n",
    "        table_name = f\"gold.{config['dimension_name']}\"\n",
    "        print(f\"\\nCreating {config['dimension_name']}...\")\n",
    "        \n",
    "        try:\n",
    "            dim_df = create_dimension_from_column(\n",
    "                spark=spark,\n",
    "                source_table=config[\"source_table\"],\n",
    "                column_name=config[\"column\"],\n",
    "                dimension_name=config[\"dimension_name\"],\n",
    "                include_counts=True,\n",
    "            )\n",
    "            \n",
    "            dim_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "            count = dim_df.count()\n",
    "            dimension_summary[config[\"dimension_name\"]] = count\n",
    "            print(f\"  ‚úÖ Created with {count} unique values\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            dimension_summary[config[\"dimension_name\"]] = \"error\"\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "    \n",
    "    return dimension_summary\n",
    "\n",
    "# Generate dimensions\n",
    "if \"gold\" in PROCESS_LAYERS:\n",
    "    print(\"\\n=== DIMENSION GENERATION ===\")\n",
    "    dimension_summary = generate_dimensions()\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping dimension generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary and Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä ETL PIPELINE COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check final table counts\n",
    "final_tables = check_existing_tables()\n",
    "\n",
    "# Calculate totals\n",
    "for schema, tables in final_tables.items():\n",
    "    if tables:\n",
    "        total_rows = sum(count for count in tables.values() if count is not None)\n",
    "        print(f\"\\n{schema.upper()}: {len(tables)} tables, {total_rows:,} total rows\")\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\nüîç Data Quality Checks:\")\n",
    "\n",
    "# Check for uninvoiced revenue\n",
    "if spark.catalog.tableExists(\"gold.gold_fact_invoice_line\"):\n",
    "    uninvoiced_revenue = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as uninvoiced_lines,\n",
    "            SUM(lineAmount) as uninvoiced_amount\n",
    "        FROM gold.gold_fact_invoice_line\n",
    "        WHERE invoiceId IS NULL\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"\\nüí∞ Uninvoiced Revenue:\")\n",
    "    print(f\"  Lines: {uninvoiced_revenue['uninvoiced_lines']:,}\")\n",
    "    print(f\"  Amount: ${uninvoiced_revenue['uninvoiced_amount']:,.2f}\" if uninvoiced_revenue['uninvoiced_amount'] else \"  Amount: $0.00\")\n",
    "\n",
    "# Check MRR trends\n",
    "if spark.catalog.tableExists(\"gold.gold_fact_agreement_period\"):\n",
    "    mrr_trend = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DATE_FORMAT(period_start, 'yyyy-MM') as month,\n",
    "            SUM(monthly_revenue) as mrr,\n",
    "            COUNT(DISTINCT id) as active_agreements\n",
    "        FROM gold.gold_fact_agreement_period\n",
    "        WHERE is_active_period = true\n",
    "        GROUP BY DATE_FORMAT(period_start, 'yyyy-MM')\n",
    "        ORDER BY month DESC\n",
    "        LIMIT 3\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    print(\"\\nüìà Recent MRR Trends:\")\n",
    "    for row in mrr_trend:\n",
    "        print(f\"  {row['month']}: ${row['mrr']:,.2f} ({row['active_agreements']} agreements)\")\n",
    "\n",
    "# Save refresh timestamp\n",
    "refresh_timestamp = datetime.now().isoformat()\n",
    "spark.conf.set(\"spark.unified_etl.last_refresh\", refresh_timestamp)\n",
    "print(f\"\\n‚è∞ Refresh completed at: {refresh_timestamp}\")\n",
    "print(f\"Mode: {'Incremental' if INCREMENTAL_MODE else 'Full Refresh'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline execution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}