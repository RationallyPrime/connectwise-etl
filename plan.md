Perfect. I’ll now revise and enhance Gemini’s PSA pipeline improvement plan so it aligns with your Fabric-first ETL framework, prioritizing modularity, Spark runtime compatibility, Direct Lake readiness, and clear delegation for Claude Code implementation.

I’ll focus the improved plan on making the dynamic SparkModel inheritance robust and testable, enabling schema-driven field selection, and shaping the extract layer to feed into a clean bronze load—while laying scaffolding for downstream Silver/Gold workflows.

I’ll let you know as soon as the revised version is ready for your review.


# PSA Pipeline Modernization Plan for Fabric OneLake ETL

## Phase 1: SparkModel Schema Compatibility (Pydantic + SparkDantic)

**Goal:** Ensure all Pydantic models (e.g. `Agreement`, `Invoice`) that inherit from `sparkdantic.SparkModel` function correctly in Microsoft Fabric Spark Runtime 1.3. This guarantees that schema inference and DataFrame conversion work smoothly.

* **Verify Pydantic Model Inheritance:** Confirm that all auto-generated ConnectWise models (in `fabric_api/connectwise_models/`) subclass `SparkModel` as intended. Our codegen already uses `--base-class sparkdantic.SparkModel` when generating models, so each model (e.g. `Agreement`) inherits Spark capabilities. We should regenerate models with the latest `sparkdantic` version if needed and ensure no Pydantic v2 issues (e.g. update any deprecated field usage).
* **Test Spark Schema Conversion:** Use SparkDantic’s utilities (e.g. `YourModel.model_spark_schema()` or equivalent) to generate PySpark `StructType` schemas and create DataFrames from sample objects. In particular, validate that nested fields and types are handled. If `SparkModel.model_spark_schema()` works, we can use it directly when writing data. If any incompatibility arises in Spark 1.3, adjust model configs (e.g. ensure `arbitrary_types_allowed=True` is set as in SparkModel’s config) or update `sparkdantic` to a compatible release. Add a small test to create a Spark DataFrame from a list of Pydantic objects for each core model in a Fabric notebook environment.
* **Fallback Plan for Nested Fields:** If certain nested Pydantic fields cause schema issues at runtime, implement a safe fallback. For example, as done in our bronze loader, complex nested objects can be flattened to ID or name strings if direct schema conversion fails. This ensures compatibility while we fine-tune SparkDantic usage. Document this fallback in code comments so it’s clear why it’s there.
* **Files Affected:** `fabric_api/connectwise_models/` (all model files, re-generated if necessary), `fabric_api/generate_models_from_json.py` (ensure it targets SparkModel base correctly), and `fabric_api/bronze_loader.py` (where we call `model_spark_schema` and might apply fallbacks).

## Phase 2: Schema-Driven Field Selection Utility

**Goal:** Minimize boilerplate in API calls by dynamically deriving ConnectWise API field lists from our Pydantic models. This leverages the schema to request only needed fields.

* **Enhance `get_fields_for_api_call`:** Our utility in `fabric_api/api_utils.py` already extracts top-level field names from a Pydantic model class and converts them to the API’s expected camelCase format. We should validate that this covers all required fields for each entity. If ConnectWise supports limited nesting (e.g. `"company/name"` style), consider extending the utility’s `max_depth` logic to include one level of nested fields for key relationships. Otherwise, continue using only top-level fields and skip internal `_info` or unsupported fields (the function already skips `_info` attributes).
* **Apply Utility Across Extractors:** Ensure every extract function uses this utility instead of hardcoding field lists. For example, `fetch_agreements_raw()` already uses `fields = get_fields_for_api_call(schemas.Agreement)` to build its query. Do the same for invoices, time entries, expenses, and products: e.g., in `fabric_api/extract/invoices.py`, have `fields_str = get_fields_for_api_call(schemas.PostedInvoice)` and `schemas.UnpostedInvoice`. This way, the ConnectWiseClient `.paginate` calls only retrieve the schema-defined fields for efficiency and consistency.
* **Reduce Boilerplate in Connectors:** By driving field selection off the schema, adding new fields to a model automatically reflects in API calls. This reduces custom code – no need to manually sync field lists with model attributes. It also makes connectors reusable: the same `get_fields_for_api_call` logic can serve any new entity model we add.
* **Files Affected:** `fabric_api/api_utils.py` (possibly extending its logic for nested fields or additional filtering), each file in `fabric_api/extract/` that calls the ConnectWise API (apply the utility for Agreements, PostedInvoices, UnpostedInvoices, TimeEntries, ExpenseEntries, ProductItems). Update unit tests or add tests to verify that the returned field string covers expected keys for each model.

## Phase 3: Modular Extraction, Validation, and Loading Workflow

**Goal:** Cleanly separate the ETL stages – **extract** (API fetch), **validate/transform** (Pydantic schema validation), and **load** (write to Delta) – into modular, testable units. This improves clarity and allows each stage to be independently tested or reused.

* **Dedicated Extract Functions:** Continue the pattern of small functions that fetch raw JSON data from the API for each entity. For example, `fetch_agreements_raw(client, ...) -> List[dict]` and similarly `fetch_posted_invoices_raw`, `fetch_unposted_invoices_raw`, etc. These should do nothing but call the API (via `client.get` or `client.paginate`) with appropriate parameters (including the fields string from Phase 2) and return the list of records. No validation or Spark logic here. This makes them easy to mock for testing and reuse in different pipelines (e.g., both daily full load and incremental updates can use the same fetchers).
* **Pydantic Validation Layer:** Introduce a uniform way to validate raw records against Pydantic models. We have examples: in the agreements flow, after fetching, we call `ManageAgreement.model_validate(raw)` to get a typed object, or use a safe wrapper that catches `ValidationError`. We can generalize this. For instance, create a helper `validate_records(raw_list, model_class) -> Tuple[List[BaseModel], List[dict]]` that returns a list of valid model instances and a list of error info dicts. This encapsulates the loop of model validation and error capture (as seen in `bronze_loader.process_entity`) and can be unit-tested with contrived bad data. Use Pydantic’s `.model_validate` as it’s already in use and capture `e.errors()` for detailed error reporting.
* **Isolate Loading to Delta:** Implement a generic writer utility that takes a list of Pydantic model instances (or their dicts) and writes to a Delta table. In our plan, `write_pydantic_to_delta(models: List[BaseModel], spark, target_path, mode)` can handle: converting models to a Spark DataFrame (using SparkDantic as in Phase 1) and performing the Delta `.write` call. We might not need a separate function if we integrate this in an orchestrator, but ensuring the logic is identical for all entities is key. The bronze loader’s `process_entity` essentially does this: it converts validated objects to a list of dicts and either uses `schema_class.model_spark_schema()` + `spark.createDataFrame` or falls back to inferring schema, then writes out. We should factor that out or at least use the same code path for all entities to avoid divergence.
* **Orchestrator Structure:** With the above pieces, our pipeline runner (e.g. in `fabric_api/bronze_loader.py` or integrated into `pipeline.py`) can orchestrate the flow: call `fetch_<entity>_raw`, then `validate_records`, then `write_pydantic_to_delta`. The `bronze_loader.py` already defines an `ENTITY_CONFIG` mapping and a `process_entity()` for this purpose. We should refine this as needed (for example, ensure it’s easy to add partition columns or switch between overwrite/append). This modular approach means each stage can be tested (we have separate tests for validation and for the DataFrame writing logic).
* **File Adjustments:** Most of the heavy lifting is in `fabric_api/bronze_loader.py`: finalize the `process_entity` function to implement the above clearly, and possibly add a `write_pydantic_to_delta` helper inside or in a utils module for clarity. The `fabric_api/extract/*.py` modules remain focused on extraction only (any validation in older code, like calls to `safe_validate` within extract, can be deprecated in favor of the centralized approach). Update `fabric_api/pipeline.py` to use these new modular functions – e.g., `run_daily` could iterate over entity list and call the new process functions instead of manually intertwining extraction and transformation. This separation makes the code easier to maintain and less ambiguous for an automated agent like Claude Code to implement or modify.

## Phase 4: OneLake-Focused Data Output and Structure

**Goal:** Align the pipeline’s output and file structure with the Microsoft Fabric OneLake conventions. All data lands in Delta Lake tables in the Lakehouse, immediately queryable by Power BI via Direct Lake, with minimal intermediate steps.

* **Direct Delta Writes to OneLake:** Ensure that every load stage writes to the Lakehouse’s `Tables` folder for the target dataset. We should parameterize the base path to OneLake (using an env var like `CW_LAKEHOUSE_ROOT` or a function argument) but default it to the Fabric default (e.g. `"/lakehouse/default/Tables/connectwise_bronze"` for Bronze stage tables). In our write logic, use `df.write.format("delta").mode(mode).save(path)` to land the Delta files. We already do this in the bronze loader. We should double-check that using the Lakehouse file path indeed registers the table (in Fabric, any subfolder under `Tables/` becomes a table named after the folder). For example, writing to `/lakehouse/default/Tables/connectwise_bronze/agreements` will populate the `connectwise_bronze.agreements` table that Direct Lake can read.
* **Table Naming and Partitioning:** Adopt clear naming for tables. For Bronze (raw data), we might prefix with `connectwise_bronze` (or use a separate Lakehouse). Our current implementation writes each entity to a subfolder (`.../connectwise_bronze/<entity>`). This is fine; just ensure consistency (all lowercase folder names to avoid Spark confusion, as we do by `entity_name.lower()` in code). For large datasets, consider adding partition columns (e.g. by date) – the plan has a placeholder for `partition_cols` per entity. We can defer complex partitioning until the volume grows, but the structure is there to add it easily. The write mode for initial loads can be `"overwrite"`, but for daily append we’ll use `"append"` to accumulate history (with incremental filtering introduced later).
* **Fabric Spark Compatibility:** Use Fabric best practices for Spark. For instance, when creating Spark sessions in notebooks or jobs, use the provided session or builder. (In Fabric notebooks, a Spark session is usually available; if not, we create one as shown in examples.) Avoid any incompatible Spark features – stick to Delta format which Fabric fully supports. The `.option("mergeSchema","true")` can be used on writes to allow schema evolution if our models change, ensuring we don’t break Direct Lake reads when new columns appear.
* **Minimal Custom Post-Processing:** Since we write directly to OneLake Delta tables, we do not need any separate upload step or data movement. The `fabric_api.upload` module remains a no-op (can be removed or kept as stub for interface completeness). The pipeline should log the OneLake paths of outputs for traceability, but otherwise the data is immediately in place. This approach (often called *OneLake-first*) means the data lake is the source of truth – no in-memory aggregation needed to transfer data elsewhere.
* **Files Affected:** `fabric_api/bronze_loader.py` (ensure the `bronze_path` defaults to a OneLake path and that writing uses Delta format with appropriate mode), `fabric_api/pipeline.py` (if integrating the bronze loader into daily runs, pass the Lakehouse root properly), `fabric_api/fabric_helpers.py` if present (as mentioned in README, may contain OneLake path utilities – ensure any such helper aligns with this approach). Also update documentation (README) to emphasize that the pipeline writes to Lakehouse tables and how to connect Power BI to those tables.

## Phase 5: Implementation Phases and Next Steps

**Goal:** Lay out the work in clear increments that can be tackled by an AI coding assistant (Claude Code) or developers without ambiguity. Each phase above corresponds to a coherent unit of work:

* **Phase 1 – SparkModel Validation:** *Scope:* Update model generation and test schema conversion. *Files:* `connectwise_models/*`, `generate_models_from_json.py`. *Rationale:* Needed to leverage Spark for DataFrame creation and ensure Pydantic v2 models work in Spark 1.3. After this phase, we should be able to call `model_spark_schema()` and `.model_validate()` without errors in the Fabric environment.
* **Phase 2 – Field Selection Refactor:** *Scope:* Improve `get_fields_for_api_call` and apply it across all data extractors. *Files:* `api_utils.py`, `extract/*.py`. *Rationale:* Reduces hard-coded field lists and keeps API calls efficient. Success means all fetch functions are shorter and driven by schemas (verify by logging the `fields` param they use).
* **Phase 3 – Modular ETL Functions:** *Scope:* Implement the structured flow (raw fetch → validate → DataFrame → Delta) in code, ideally via a reusable pattern. *Files:* `bronze_loader.py`, possibly `pipeline.py` (refactoring extraction calls). *Rationale:* Separation of concerns for easier testing and maintenance. After this, we can unit test each stage (e.g., feed dummy JSON to validation function and get a Pydantic object).
* **Phase 4 – OneLake Integration:** *Scope:* Configure output paths and writing modes for Fabric. *Files:* `bronze_loader.py` (writing logic), `pipeline.py` (or whichever orchestrator calls the loader). *Rationale:* Ensures data lands in Fabric’s Lakehouse in Delta format, ready for consumption. We’ll verify in a Fabric workspace that tables appear and data is queryable via Spark SQL or Power BI.
* **Phase 5 – Incremental & Future Enhancements:** *(Beyond the immediate scope)* Once the above is in place, plan to handle incremental loads and Silver transformations. For example, maintain a watermark of last processed timestamp and use ConnectWise conditions (e.g., `lastUpdated > ...`) to fetch only new/updated records. Also plan a Silver layer where Bronze JSON structures (or foreign keys) are flattened into relational tables (possibly using the existing `Manage*` models for schema). These would be separate phases (Phase 6: Incremental Load, Phase 7: Silver layer), ensuring the pipeline continues to meet the goals of modularity and minimal custom logic.

Each phase is designed to be implemented and tested independently, aligning with priorities of **modularity, testability, and schema-driven design**. By tackling the improvements in these stages, the ConnectWise PSA ETL pipeline will be optimized for Microsoft Fabric, with clear code structure and reliable data flows from API to OneLake Delta tables.
