"""
Simplified Business Central data extractor for Microsoft Fabric.

This module extracts data from Business Central API and loads directly to Bronze layer
in Microsoft Fabric, using autogenerated models instead of YAML configuration.
"""

import time
from datetime import datetime, timedelta
from typing import Any

import pandas as pd
import pyarrow as pa
import pyspark.sql.functions as F
import requests
from pyspark.sql import SparkSession

from fabric_api.bc_models import models
from fabric_api.core.pipeline_utils import log_dataframe_info, log_error, log_info


class BCExtractor:
    """Extract data from Business Central API and load to Bronze layer."""
    
    def __init__(
        self,
        spark: SparkSession,
        company_id: str,
        environment: str = "PRODUCTION",
        api_version: str = "v2.0"
    ):
        """Initialize the extractor with API configuration."""
        self.spark = spark
        self.company_id = company_id
        self.environment = environment
        self.api_version = api_version
        self.base_url = f"https://api.businesscentral.dynamics.com/{api_version}/{environment}/api/v2.0"
        
        # Map API endpoints to Pydantic models and table names
        self.entity_mapping = {
            "generalLedgerEntries": {
                "model": models.GLEntry,
                "table_name": "GLEntry",
                "table_number": "17"
            },
            "customers": {
                "model": models.Customer,
                "table_name": "Customer", 
                "table_number": "18"
            },
            "customerLedgerEntries": {
                "model": models.CustLedgerEntry,
                "table_name": "CustLedgerEntry",
                "table_number": "21"
            },
            "vendors": {
                "model": models.Vendor,
                "table_name": "Vendor",
                "table_number": "23"
            },
            "vendorLedgerEntries": {
                "model": models.VendorLedgerEntry,
                "table_name": "VendorLedgerEntry",
                "table_number": "25"
            },
            "items": {
                "model": models.Item,
                "table_name": "Item",
                "table_number": "27"
            },
            "dimensions": {
                "model": models.Dimension,
                "table_name": "Dimension",
                "table_number": "348"
            },
            "dimensionValues": {
                "model": models.DimensionValue,
                "table_name": "DimensionValue",
                "table_number": "349"
            },
            "dimensionSetEntries": {
                "model": models.DimensionSetEntry,
                "table_name": "DimensionSetEntry",
                "table_number": "480"
            },
            "currencies": {
                "model": models.Currency,
                "table_name": "Currency",
                "table_number": "4"
            },
            "companyInformation": {
                "model": models.CompanyInformation,
                "table_name": "CompanyInformation",
                "table_number": "79"
            },
            "jobs": {
                "model": models.Job,
                "table_name": "Job",
                "table_number": "167"
            },
            "jobLedgerEntries": {
                "model": models.JobLedgerEntry,
                "table_name": "JobLedgerEntry",
                "table_number": "169"
            },
            "resources": {
                "model": models.Resource,
                "table_name": "Resource",
                "table_number": "156"
            },
            "salesInvoices": {
                "model": models.SalesInvoiceHeader,
                "table_name": "SalesInvoiceHeader",
                "table_number": "112"
            },
            "salesInvoiceLines": {
                "model": models.SalesInvoiceLine,
                "table_name": "SalesInvoiceLine",
                "table_number": "113"
            },
            "generalLedgerAccounts": {
                "model": models.GLAccount,
                "table_name": "GLAccount",
                "table_number": "15"
            }
        }

    def get_auth_token(self) -> str:
        """Get authentication token from Fabric Key Vault or environment."""
        # In a real implementation, this would retrieve from Fabric Key Vault
        # For now, returning a placeholder
        return "Bearer YOUR_ACCESS_TOKEN"

    def extract_entity(
        self,
        endpoint: str,
        date_filter: str | None = None,
        max_pages: int = 100
    ) -> list[dict[str, Any]]:
        """Extract data from a Business Central API endpoint."""
        log_info(f"Extracting data from endpoint: {endpoint}")
        
        headers = {
            "Authorization": self.get_auth_token(),
            "Accept": "application/json"
        }
        
        params = {
            "$company": self.company_id,
            "$top": 1000  # Max records per page
        }
        
        if date_filter:
            params["$filter"] = date_filter
            
        all_records = []
        page_count = 0
        next_link = f"{self.base_url}/{endpoint}"
        
        while next_link and page_count < max_pages:
            try:
                response = requests.get(next_link, headers=headers, params=params)
                response.raise_for_status()
                
                data = response.json()
                records = data.get("value", [])
                all_records.extend(records)
                
                log_info(f"Retrieved {len(records)} records from page {page_count + 1}")
                
                # Get next page link
                next_link = data.get("@odata.nextLink")
                params = {}  # Clear params for subsequent requests
                page_count += 1
                
                # Rate limiting
                time.sleep(0.5)
                
            except requests.exceptions.RequestException as e:
                log_error(f"API request failed: {str(e)}")
                break
                
        log_info(f"Total records extracted: {len(all_records)}")
        return all_records

    def validate_and_transform(
        self,
        records: list[dict[str, Any]],
        model_class: type[models.SparkModel]
    ) -> pd.DataFrame:
        """Validate records against Pydantic model and convert to DataFrame."""
        validated_records = []
        
        for record in records:
            try:
                # Validate against Pydantic model
                validated = model_class(**record)
                validated_records.append(validated.model_dump())
            except Exception as e:
                log_error(f"Validation error: {str(e)}")
                # Continue with other records
                
        # Convert to pandas DataFrame
        df = pd.DataFrame(validated_records)
        
        # Add metadata columns
        df["$Company"] = self.company_id
        df["ExtractedAt"] = datetime.now()
        df["Environment"] = self.environment
        
        return df

    def load_to_bronze(
        self,
        df: pd.DataFrame,
        table_name: str,
        bronze_path: str,
        mode: str = "overwrite"
    ):
        """Load DataFrame to Bronze layer in Delta format."""
        log_info(f"Loading {len(df)} records to Bronze layer: {table_name}")
        
        # Convert pandas DataFrame to Spark DataFrame
        spark_df = self.spark.createDataFrame(df)
        
        # Add Bronze layer metadata
        spark_df = spark_df.withColumn("BronzeProcessedAt", F.current_timestamp())
        
        # Write to Bronze layer
        bronze_table_path = f"{bronze_path}.{table_name}"
        spark_df.write.mode(mode).option("mergeSchema", "true").saveAsTable(bronze_table_path)
        
        log_info(f"Successfully loaded data to {bronze_table_path}")

    def extract_all_entities(
        self,
        bronze_path: str,
        start_date: str | None = None,
        end_date: str | None = None,
        specific_entities: list[str] | None = None
    ):
        """Extract all configured entities from Business Central."""
        log_info("Starting Business Central data extraction")
        
        # Default to last 30 days if no dates provided
        if not start_date:
            start_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")
            
        # Determine which entities to extract
        entities_to_extract = specific_entities or list(self.entity_mapping.keys())
        
        for endpoint in entities_to_extract:
            if endpoint not in self.entity_mapping:
                log_error(f"Unknown endpoint: {endpoint}")
                continue
                
            entity_config = self.entity_mapping[endpoint]
            model_class = entity_config["model"]
            table_name = entity_config["table_name"]
            
            # Build date filter for transactional entities
            date_filter = None
            if endpoint in ["generalLedgerEntries", "customerLedgerEntries", 
                           "vendorLedgerEntries", "jobLedgerEntries"]:
                date_filter = f"postingDate ge {start_date} and postingDate le {end_date}"
            elif endpoint in ["salesInvoices"]:
                date_filter = f"documentDate ge {start_date} and documentDate le {end_date}"
                
            try:
                # Extract data from API
                records = self.extract_entity(endpoint, date_filter)
                
                if records:
                    # Validate and transform
                    df = self.validate_and_transform(records, model_class)
                    
                    # Load to Bronze
                    self.load_to_bronze(df, table_name, bronze_path)
                else:
                    log_info(f"No records found for {endpoint}")
                    
            except Exception as e:
                log_error(f"Failed to process {endpoint}: {str(e)}")
                continue
                
        log_info("Business Central data extraction completed")


# Example usage in a Fabric notebook:
"""
from pyspark.sql import SparkSession
from fabric_api.bc_extractor import BCExtractor

# Spark session is already available in Fabric notebooks
spark = spark

# Initialize extractor
extractor = BCExtractor(
    spark=spark,
    company_id="YOUR_COMPANY_ID",
    environment="PRODUCTION"
)

# Extract all entities
extractor.extract_all_entities(
    bronze_path="bronze",
    start_date="2025-01-01",
    end_date="2025-01-19"
)
"""