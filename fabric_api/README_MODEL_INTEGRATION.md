# Model Integration in Unified Lakehouse

This document explains how the PSA and BC pipelines integrate with autogenerated Pydantic models from their respective schemas.

## PSA Models (ConnectWise OpenAPI)

### Model Generation
PSA models are generated from the OpenAPI schema using `regenerate_models.py`:

```bash
python regenerate_models.py --schema PSA_OpenAPI_schema.json --output-dir fabric_api/connectwise_models
```

### Generated Models Structure
```python
# fabric_api/connectwise_models/models.py
from sparkdantic import SparkModel

class Agreement(SparkModel):
    id: int | None = None
    name: str | None = None
    agreementNumber: str | None = None
    # ... all fields from OpenAPI

class TimeEntry(SparkModel):
    id: int | None = None
    memberId: int | None = None
    invoiceId: int | None = None
    # ... all fields from OpenAPI
```

### PSA Pipeline Integration
The PSA pipeline uses these models in the Bronze layer:

```python
# fabric_api/pipeline_psa_enhanced.py
from .connectwise_models import (
    Agreement,
    TimeEntry,
    ExpenseEntry,
    ProductItem,
    PostedInvoice
)

class PSAPipeline:
    def process_entity_to_bronze(self, entity_name: str, ...):
        # Use model mapping for validation
        model_class = {
            "Agreement": Agreement,
            "TimeEntry": TimeEntry,
            "ExpenseEntry": ExpenseEntry,
            "ProductItem": ProductItem,
            "PostedInvoice": PostedInvoice
        }.get(entity_name)
        
        # Generate Spark schema from Pydantic model
        if model_class:
            spark_schema = model_class.model_spark_schema()
            
        # Create DataFrame with proper schema
        raw_df = dataframe_from_models(valid_data, spark_schema)
```

## BC Models (Business Central CDM)

### Model Generation
BC models are generated from CDM manifests using `generate_bc_models_camelcase.py`:

```bash
python fabric_api/generate_bc_models_camelcase.py --input-dir cdm_manifests --output fabric_api/bc_models/models.py
```

### Generated Models Structure
```python
# fabric_api/bc_models/models.py
from sparkdantic import SparkModel

class Customer(SparkModel):
    """Customer entity from Business Central"""
    No: str | None = Field(default=None, alias="No-18")
    Name: str | None = Field(default=None, alias="Name-18")
    CustomerPostingGroup: str | None = None
    # ... all fields from CDM

class GLEntry(SparkModel):
    """General Ledger Entry entity"""
    EntryNo: int | None = Field(default=None, alias="EntryNo-17")
    PostingDate: datetime | None = None
    Amount: float | None = None
    # ... all fields from CDM
```

### BC Pipeline Integration
The BC pipeline uses these models in the Bronze to Silver transformation:

```python
# fabric_api/bc_medallion_pipeline_enhanced.py
from .bc_models import models

class BCMedallionPipelineEnhanced:
    def __init__(self, spark: SparkSession):
        # Map BC table names to Pydantic models
        self.model_mapping = {
            "Customer": models.Customer,
            "GLEntry": models.GLEntry,
            "Job": models.Job,
            "Resource": models.Resource,
            # ... other BC models
        }
    
    def bronze_to_silver(self, table_name: str, ...):
        # Get model for validation
        model_class = self.model_mapping.get(clean_table_name)
        if model_class:
            # Get Spark schema from model
            spark_schema = model_class.model_spark_schema()
            
            # Validate and cast columns
            for field in spark_schema.fields:
                if field.name in bronze_df.columns:
                    bronze_df = bronze_df.withColumn(
                        field.name,
                        F.col(field.name).cast(field.dataType)
                    )
```

## Unified Pipeline Integration

The unified orchestrator uses both model systems:

```python
# fabric_api/unified_lakehouse_orchestrator.py
from .pipeline_psa_enhanced import PSAPipeline
from .bc_medallion_pipeline_enhanced import BCMedallionPipelineEnhanced

class UnifiedLakehouseOrchestrator:
    def __init__(self, spark: SparkSession):
        # Both pipelines use their respective models
        self.psa_pipeline = PSAPipeline(spark)
        self.bc_pipeline = BCMedallionPipelineEnhanced(spark)
    
    def run_full_pipeline(self, ...):
        # PSA uses ConnectWise models
        psa_results = self.run_psa_bronze_to_silver()
        
        # BC uses CDM models
        bc_results = self.run_bc_bronze_to_silver()
```

## Schema Evolution Workflow

### 1. PSA Schema Updates
When ConnectWise API changes:
```bash
# Download new OpenAPI schema
curl -o PSA_OpenAPI_schema.json https://api.connectwise.com/openapi

# Regenerate models
python regenerate_models.py

# Models automatically used by pipeline
```

### 2. BC Schema Updates
When Business Central schema changes:
```bash
# Get new CDM manifests
# Place in cdm_manifests/

# Regenerate models
python fabric_api/generate_bc_models_camelcase.py

# Models automatically used by pipeline
```

## Key Benefits

1. **Type Safety**: All data is validated against Pydantic models
2. **Schema Generation**: Spark schemas are automatically generated from models
3. **Documentation**: Models are self-documenting with field descriptions
4. **Evolution**: Schema changes are handled by regenerating models
5. **Consistency**: Both PSA and BC use the same SparkDantic base

## Example Usage

### Reading PSA Data with Schema
```python
# Automatic schema from model
time_entries = TimeEntry.model_spark_schema()
df = spark.read.schema(time_entries).json("path/to/time_entries.json")
```

### Reading BC Data with Schema
```python
# Automatic schema from model  
customers = Customer.model_spark_schema()
df = spark.read.schema(customers).parquet("path/to/customers.parquet")
```

### Creating DataFrames
```python
# From validated models
from fabric_api.storage.fabric_delta import dataframe_from_models

# PSA data
psa_models = [Agreement(...), TimeEntry(...)]
psa_df = dataframe_from_models(psa_models, "Agreement")

# BC data
bc_models = [Customer(...), GLEntry(...)]
bc_df = dataframe_from_models(bc_models, "Customer")
```

## Troubleshooting

### Model Generation Issues

1. **PSA Models**: Check OpenAPI schema validity
   ```bash
   python regenerate_models.py --entities Agreement --schema PSA_OpenAPI_schema.json
   ```

2. **BC Models**: Verify CDM manifest format
   ```bash
   python fabric_api/generate_bc_models_camelcase.py --input-dir cdm_manifests
   ```

### Schema Mismatch

1. Compare generated model with actual data:
   ```python
   model_schema = TimeEntry.model_spark_schema()
   actual_schema = df.schema
   print(f"Model fields: {model_schema.fieldNames()}")
   print(f"Data fields: {actual_schema.fieldNames()}")
   ```

2. Enable schema merge for flexibility:
   ```python
   df.write.option("mergeSchema", "true").saveAsTable("silver.psa.TimeEntry")
   ```

### Type Casting Issues

1. Check model field types:
   ```python
   for field in TimeEntry.model_spark_schema().fields:
       print(f"{field.name}: {field.dataType}")
   ```

2. Cast fields explicitly:
   ```python
   df = df.withColumn("invoiceId", F.col("invoiceId").cast(IntegerType()))
   ```