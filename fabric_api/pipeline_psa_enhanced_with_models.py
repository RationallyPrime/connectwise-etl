"""
Enhanced PSA Pipeline with Model Integration.

This version shows explicit integration with autogenerated Pydantic models
from the ConnectWise OpenAPI schema.
"""

import logging
from datetime import datetime, timedelta
from typing import Any, cast

from pydantic import BaseModel
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit

from .client import ConnectWiseClient
from .connectwise_models import (
    Agreement,
    ExpenseEntry,
    PostedInvoice,
    ProductItem,
    TimeEntry,
)
from .core.spark_utils import get_spark_session
from .extract.generic import extract_entity
from .storage.fabric_delta import dataframe_from_models
from .transform.dataframe_utils import flatten_all_nested_structures

logger = logging.getLogger(__name__)


class PSAPipeline:
    """Enhanced PSA pipeline with schema-aware medallion architecture and model integration."""

    def __init__(self, spark: SparkSession | None = None):
        """Initialize the PSA pipeline."""
        self.spark = spark or get_spark_session()
        self.schemas = {"bronze": "bronze.psa", "silver": "silver.psa", "gold": "gold.psa"}

        # Map entity names to Pydantic models
        self.model_mapping = {
            "Agreement": Agreement,
            "TimeEntry": TimeEntry,
            "ExpenseEntry": ExpenseEntry,
            "ProductItem": ProductItem,
            "PostedInvoice": PostedInvoice,
        }

        # Ensure schemas exist
        self._create_schemas()

    def _create_schemas(self):
        """Create necessary schemas if they don't exist."""
        for schema_name in self.schemas.values():
            try:
                self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
                logger.info(f"Ensured schema exists: {schema_name}")
            except Exception as e:
                logger.error(f"Error creating schema {schema_name}: {e}")

    def process_entity_to_bronze(
        self,
        entity_name: str,
        client: ConnectWiseClient | None = None,
        conditions: str | None = None,
        page_size: int = 100,
        max_pages: int | None = None,
        mode: str = "append",
    ) -> dict[str, Any]:
        """Process a single entity to bronze layer using autogenerated models."""
        client = client or ConnectWiseClient()

        # Get the model class for this entity
        model_class = self.model_mapping.get(entity_name)
        if not model_class:
            logger.warning(f"No model class found for entity {entity_name}")

        logger.info(f"Extracting {entity_name} data for bronze layer...")
        data_result = extract_entity(
            client=client,
            entity_name=entity_name,
            page_size=page_size,
            max_pages=max_pages,
            conditions=conditions,
            return_validated=True,
        )

        valid_data, validation_errors = cast(
            tuple[list[BaseModel], list[dict[str, Any]]], data_result
        )

        total_records = len(valid_data) + len(validation_errors)
        if total_records == 0:
            logger.warning(f"No {entity_name} data extracted")
            return {"entity": entity_name, "extracted": 0, "bronze_rows": 0}

        # Create DataFrame from validated models using proper schema
        if valid_data:
            # The validated data are instances of our autogenerated models
            raw_df = dataframe_from_models(valid_data, entity_name)
        else:
            # For errors, create DataFrame from raw data
            error_df = self.spark.createDataFrame(validation_errors)  # type: ignore
            raw_df = error_df

        # Add ETL metadata
        raw_df = raw_df.withColumn("etlTimestamp", lit(datetime.utcnow().isoformat()))
        raw_df = raw_df.withColumn("etlEntity", lit(entity_name))

        # Write to bronze.psa schema
        bronze_table = f"{self.schemas['bronze']}.{entity_name}"
        raw_df.write.mode(mode).saveAsTable(bronze_table)

        row_count = raw_df.count()
        logger.info(f"Wrote {row_count} rows to {bronze_table}")

        return {
            "entity": entity_name,
            "extracted": total_records,
            "bronze_rows": row_count,
            "bronze_table": bronze_table,
            "model_class": model_class.__name__ if model_class else None,
        }

    def process_bronze_to_silver(self, entity_name: str, mode: str = "overwrite") -> dict[str, Any]:
        """Process data from bronze to silver layer with model validation."""
        bronze_table = f"{self.schemas['bronze']}.{entity_name}"

        try:
            bronze_df = self.spark.table(bronze_table)
        except Exception as e:
            logger.error(f"Error reading bronze table {bronze_table}: {e}")
            return {"entity": entity_name, "status": "error", "error": str(e)}

        initial_count = bronze_df.count()
        logger.info(f"Found {initial_count} rows in {bronze_table}")

        # Get the model class for validation
        model_class = self.model_mapping.get(entity_name)

        # Apply transformations
        # 1. Flatten nested structures
        flattened_df = flatten_all_nested_structures(bronze_df)

        # 2. Validate against model schema if available
        if model_class:
            try:
                # Get Spark schema from model
                model_schema = model_class.model_spark_schema()

                # Select only columns that exist in both DataFrame and model
                common_columns = list(set(flattened_df.columns) & set(model_schema.fieldNames()))
                validated_df = flattened_df.select(*common_columns)

                # Cast columns to match model schema types
                for field in model_schema.fields:
                    if field.name in validated_df.columns:
                        validated_df = validated_df.withColumn(
                            field.name, col(field.name).cast(field.dataType)
                        )

                logger.info(f"Validated {entity_name} against {model_class.__name__} schema")
                silver_df = validated_df
            except Exception as e:
                logger.warning(f"Could not validate against model schema: {e}")
                silver_df = flattened_df
        else:
            silver_df = flattened_df

        # 3. Apply entity-specific transformations
        silver_df = self._apply_entity_transformations(silver_df, entity_name)

        # 4. Write to silver.psa schema
        silver_table = f"{self.schemas['silver']}.{entity_name}"
        silver_df.write.mode(mode).option("mergeSchema", "true").saveAsTable(silver_table)

        final_count = silver_df.count()
        logger.info(f"Wrote {final_count} rows to {silver_table}")

        return {
            "entity": entity_name,
            "status": "success",
            "bronze_rows": initial_count,
            "silver_rows": final_count,
            "silver_table": silver_table,
            "model_validated": model_class is not None,
        }

    def _apply_entity_transformations(self, df: DataFrame, entity_name: str) -> DataFrame:
        """Apply entity-specific transformations based on model requirements."""
        # Agreement specific transformations
        if entity_name == "Agreement":
            # Extract agreement number from custom fields if available
            if "customFields" in df.columns:
                try:
                    df = df.withColumn(
                        "agreementNumber", col("customFields").getItem("agreementNumber")
                    )
                except:
                    pass

        # TimeEntry specific transformations
        elif entity_name == "TimeEntry":
            # Ensure invoice ID column naming consistency
            if "invoice_id" in df.columns and "invoiceId" not in df.columns:
                df = df.withColumn("invoiceId", col("invoice_id"))

        # PostedInvoice specific transformations
        elif entity_name == "PostedInvoice":
            # Add any invoice-specific logic here
            pass

        return df

    def process_silver_to_gold(self) -> dict[str, Any]:
        """Process silver layer to create gold layer data marts."""
        from .gold.conformed_dimensions import run_conformed_dimensions
        from .gold.psa_financial_model import run_psa_financial_gold

        results = {}

        try:
            # Create conformed dimensions
            logger.info("Creating conformed dimensions...")
            dim_results = run_conformed_dimensions(
                silver_path=self.schemas["silver"],
                gold_path=self.schemas["gold"],
                spark=self.spark,
                include_bc_data=False,  # PSA standalone mode
            )
            results["dimensions"] = dim_results

            # Create PSA financial model
            logger.info("Creating PSA financial model...")
            financial_results = run_psa_financial_gold(
                silver_path=self.schemas["silver"], gold_path=self.schemas["gold"], spark=self.spark
            )
            results["facts"] = financial_results

            results["status"] = "success"

        except Exception as e:
            logger.error(f"Error processing silver to gold: {e}")
            results["status"] = "error"
            results["error"] = str(e)

        return results

    def run_full_pipeline(
        self,
        entity_names: list[str] | None = None,
        client: ConnectWiseClient | None = None,
        conditions: str | None = None,
        page_size: int = 100,
        max_pages: int | None = None,
        bronze_mode: str = "append",
        silver_mode: str = "overwrite",
        process_gold: bool = True,
    ) -> dict[str, dict[str, Any]]:
        """Run the full PSA medallion pipeline with model validation."""
        client = client or ConnectWiseClient()

        if entity_names is None:
            # Use all entities we have models for
            entity_names = list(self.model_mapping.keys())

        results = {}

        # Phase 1: Extract to bronze with model validation
        logger.info("Phase 1: Extracting to bronze layer")
        for entity_name in entity_names:
            logger.info(f"Processing {entity_name} to bronze...")
            bronze_result = self.process_entity_to_bronze(
                entity_name=entity_name,
                client=client,
                conditions=conditions,
                page_size=page_size,
                max_pages=max_pages,
                mode=bronze_mode,
            )
            results[entity_name] = {"bronze": bronze_result}

        # Phase 2: Transform bronze to silver with model validation
        logger.info("Phase 2: Transforming bronze to silver")
        for entity_name in entity_names:
            if results[entity_name]["bronze"]["bronze_rows"] > 0:
                logger.info(f"Processing {entity_name} from bronze to silver...")
                silver_result = self.process_bronze_to_silver(
                    entity_name=entity_name, mode=silver_mode
                )
                results[entity_name]["silver"] = silver_result
            else:
                results[entity_name]["silver"] = {
                    "status": "skipped",
                    "reason": "No data in bronze",
                }

        # Phase 3: Process gold layer
        if process_gold:
            logger.info("Phase 3: Processing gold layer")
            gold_results = self.process_silver_to_gold()
            results["gold"] = gold_results

        return results

    def run_daily_pipeline(
        self,
        entity_names: list[str] | None = None,
        days_back: int = 1,
        client: ConnectWiseClient | None = None,
        page_size: int = 100,
        max_pages: int | None = None,
        process_gold: bool = True,
    ) -> dict[str, dict[str, Any]]:
        """Run daily incremental pipeline with model validation."""
        yesterday = datetime.now() - timedelta(days=days_back)
        yesterday_str = yesterday.strftime("%Y-%m-%d")
        today = datetime.now()
        today_str = today.strftime("%Y-%m-%d")

        logger.info(f"Running daily pipeline for {yesterday_str}")

        # Build date-based condition
        condition = f"lastUpdated>=[{yesterday_str}] AND lastUpdated<[{today_str}]"

        return self.run_full_pipeline(
            entity_names=entity_names,
            client=client,
            conditions=condition,
            page_size=page_size,
            max_pages=max_pages,
            bronze_mode="append",
            silver_mode="overwrite",
            process_gold=process_gold,
        )


# Example usage showing model integration:
"""
from fabric_api.pipeline_psa_enhanced_with_models import PSAPipeline

# Initialize pipeline with model mapping
pipeline = PSAPipeline(spark)

# The models are automatically used for:
# 1. Schema validation in bronze layer
# 2. Column type casting in silver layer
# 3. Data quality checks

results = pipeline.run_full_pipeline(
    entity_names=["Agreement", "TimeEntry"],
    process_gold=True
)

# Check which models were used
for entity, result in results.items():
    if isinstance(result, dict) and "bronze" in result:
        print(f"{entity}: Used model {result['bronze'].get('model_class')}")
"""
