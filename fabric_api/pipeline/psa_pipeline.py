"""
Enhanced PSA Pipeline with Model Integration.

This version shows explicit integration with autogenerated Pydantic models
from the ConnectWise OpenAPI schema.
"""
Enhanced PSA Pipeline with Model Integration and Core ETL Utilities.
"""

from datetime import datetime, timedelta
from typing import Any, cast, Optional # Added Optional

from pydantic import BaseModel # Keep for type hinting if valid_data is BaseModel list
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit

# Core ETL Utilities
from core_etl.logging_utils import etl_logger
from core_etl.config_utils import get_psa_entity_config # New
from core_etl.delta_writer import write_delta_table # New
# Spark session is expected to be passed by the caller (e.g. from core_etl.spark_utils.get_spark_session)

# Fabric API specific imports (adjust paths based on final structure)
from fabric_api.extract.api_client import ConnectWiseClient # Updated path
from fabric_api.models.psa_models import ( # Updated path
    Agreement,
    ExpenseEntry,
    PostedInvoice,
    ProductItem,
    TimeEntry,
)
# Assuming these helpers are refactored or their paths adjusted if psa_pipeline.py moves
from fabric_api.extract.generic import extract_entity # Assuming new path
from fabric_api.storage.fabric_delta import dataframe_from_models # Assuming new path
from fabric_api.transform.dataframe_utils import flatten_all_nested_structures # Assuming new path


class PSAPipeline:
    """Enhanced PSA pipeline with schema-aware medallion architecture and model integration."""

    def __init__(self, spark: SparkSession): # SparkSession is now mandatory
        """Initialize the PSA pipeline."""
        self.spark = spark
        # Schemas can be made configurable via get_psa_entity_config or get_global_setting
        self.schemas = {
            "bronze": get_psa_entity_config("_global").get("bronze_schema_name", "bronze.psa"),
            "silver": get_psa_entity_config("_global").get("silver_schema_name", "silver.psa"),
            "gold": get_psa_entity_config("_global").get("gold_schema_name", "gold.psa"),
        }


        # Map entity names to Pydantic models from the new location
        self.model_mapping = {
            "Agreement": Agreement,
            "TimeEntry": TimeEntry,
            "ExpenseEntry": ExpenseEntry,
            "ProductItem": ProductItem,
            "PostedInvoice": PostedInvoice,
            # Add other PSA models as they are created and used
        }

        # Ensure schemas exist
        self._create_schemas()

    def _create_schemas(self):
        """Create necessary schemas if they don't exist."""
        for schema_name in self.schemas.values():
            try:
                self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
                etl_logger.info(f"Ensured schema exists: {schema_name}")
            except Exception as e:
                etl_logger.error(f"Error creating schema {schema_name}: {e}")

    def process_entity_to_bronze(
        self,
        entity_name: str,
        client: ConnectWiseClient, # Made client mandatory for clarity
        conditions: str | None = None,
        page_size: int = 100,
        max_pages: int | None = None,
        mode: str = "append", # Default to append for bronze
    ) -> dict[str, Any]:
        """Process a single entity to bronze layer using autogenerated models."""
        
        # Entity specific configs (like API path) should ideally be handled by ConnectWiseClient or extract_entity
        # using get_psa_entity_config(entity_name).get("api_path") or similar.
        # For now, assuming extract_entity abstracts this.
        
        model_class = self.model_mapping.get(entity_name)
        if not model_class:
            etl_logger.warning(f"No Pydantic model class found for entity {entity_name} in model_mapping.")
            # Depending on policy, could raise error or proceed without model-specific parts if possible

        etl_logger.info(f"Extracting {entity_name} data for bronze layer...")
        # extract_entity is assumed to be refactored to use etl_logger and handle its own config needs
        data_result = extract_entity(
            client=client,
            entity_name=entity_name,
            page_size=page_size,
            max_pages=max_pages,
            conditions=conditions,
            return_validated=True, # Assuming this means it returns (validated_models, error_jsons)
        )

        # Unpack results, ensure it's what dataframe_from_models expects
        # The original code casts, implying data_result could be complex.
        # Assuming data_result is tuple[list[BaseModel], list[dict[str, Any]]]
        valid_data: list[BaseModel]
        validation_errors: list[dict[str, Any]]
        valid_data, validation_errors = cast(tuple[list[BaseModel], list[dict[str, Any]]], data_result)
        
        total_extracted_records = len(valid_data) + len(validation_errors)
        if total_extracted_records == 0:
            etl_logger.warning(f"No {entity_name} data extracted.")
            return {"entity": entity_name, "extracted": 0, "bronze_rows": 0, "status": "no_data"}

        raw_df: Optional[DataFrame] = None
        if valid_data:
            # dataframe_from_models is assumed refactored or path adjusted
            raw_df = dataframe_from_models(valid_data, entity_name) 
            etl_logger.info(f"Created DataFrame for {len(valid_data)} valid {entity_name} records.")
        
        if validation_errors:
            etl_logger.warning(f"Encountered {len(validation_errors)} validation errors for {entity_name}.")
            # Consider saving errors to a separate quarantine location or logging them in detail
            # For now, if only errors, create a DF from error data if possible (might lack consistent schema)
            # This part needs careful thought on how to handle partially valid batches.
            # Current logic creates DF from errors if no valid_data, might be problematic for schema.
            if raw_df is None:
                try:
                    # Attempt to create a DataFrame from error data, schema might be inconsistent
                    error_df = self.spark.createDataFrame(validation_errors) # type: ignore 
                    raw_df = error_df
                    etl_logger.info(f"Created DataFrame for {len(validation_errors)} error records for {entity_name} (no valid records).")
                except Exception as e:
                    etl_logger.error(f"Could not create DataFrame from validation_errors for {entity_name}: {e}")
                    # If we can't even make a DF from errors, and no valid data, then fail this part.
                    if raw_df is None:
                         return {"entity": entity_name, "extracted": total_extracted_records, "bronze_rows": 0, "status": "error_no_df_created"}
            # If there's already a raw_df from valid_data, decide if/how to append error data (e.g. with nulls for model fields)
            # For simplicity, current code prioritizes valid_data for DF creation.

        if raw_df is None: # Should not happen if total_extracted_records > 0 and error_df creation fallback works.
            etl_logger.error(f"DataFrame creation failed for {entity_name} despite extracted records. Skipping bronze write.")
            return {"entity": entity_name, "extracted": total_extracted_records, "bronze_rows": 0, "status": "error_df_creation_failed"}

        raw_df = raw_df.withColumn("etlTimestamp", lit(datetime.utcnow().isoformat()))
        raw_df = raw_df.withColumn("etlEntityName", lit(entity_name)) # Standardized metadata column name

        bronze_table_path = f"{self.schemas['bronze']}.{entity_name}"
        partition_cols_bronze = get_psa_entity_config(entity_name).get("partition_columns_bronze")

        write_delta_table(
            df=raw_df,
            table_path=bronze_table_path,
            mode=mode,
            partition_by_cols=partition_cols_bronze,
            merge_schema_flag=True # Good for bronze to allow schema evolution from source
        )
        
        # Count can be expensive, consider only if essential for return value or use estimate
        row_count = raw_df.count() 
        etl_logger.info(f"Wrote {row_count} rows to {bronze_table_path} (mode: {mode}).")

        return {
            "entity": entity_name,
            "extracted": total_extracted_records,
            "bronze_rows": row_count,
            "bronze_table": bronze_table_path,
            "model_class": model_class.__name__ if model_class else None,
            "status": "success"
        }

    def process_bronze_to_silver(self, entity_name: str, mode: str = "overwrite") -> dict[str, Any]:
        """Process data from bronze to silver layer with model validation."""
        bronze_table_path = f"{self.schemas['bronze']}.{entity_name}"
        silver_table_path = f"{self.schemas['silver']}.{entity_name}"
        
        entity_config = get_psa_entity_config(entity_name)
        silver_partition_columns = entity_config.get("partition_columns_silver")

        try:
            bronze_df = self.spark.table(bronze_table_path)
        except Exception as e:
            etl_logger.error(f"Error reading bronze table {bronze_table_path}: {e}", exc_info=True)
            return {"entity": entity_name, "status": "error_reading_bronze", "error": str(e)}

        initial_count = bronze_df.count()
        etl_logger.info(f"Found {initial_count} rows in {bronze_table_path} for silver processing.")
        if initial_count == 0:
            etl_logger.info(f"No data in {bronze_table_path} to process for {entity_name}. Skipping silver.")
            return {"entity": entity_name, "status": "skipped_no_bronze_data", "bronze_rows": 0}

        model_class = self.model_mapping.get(entity_name)
        
        # flatten_all_nested_structures is assumed refactored or path adjusted
        flattened_df = flatten_all_nested_structures(bronze_df)
        silver_df = flattened_df # Start with flattened

        if model_class:
            try:
                model_schema = model_class.model_spark_schema()
                common_columns = [f.name for f in model_schema.fields if f.name in flattened_df.columns]
                
                # Select only columns defined in the Pydantic model's Spark schema
                # This ensures alignment with the model's structure.
                validated_df = flattened_df.select(*common_columns) 

                for field in model_schema.fields:
                    if field.name in validated_df.columns:
                        validated_df = validated_df.withColumn(
                            field.name, col(field.name).cast(field.dataType)
                        )
                etl_logger.info(f"Schema validation and casting applied for {entity_name} using {model_class.__name__}.")
                silver_df = validated_df
            except Exception as e:
                etl_logger.warning(f"Could not fully apply Pydantic model schema for {entity_name} using {model_class.__name__}: {e}. Proceeding with best effort.", exc_info=True)
                # silver_df remains as flattened_df or partially processed if error occurs mid-cast
        else:
            etl_logger.warning(f"No Pydantic model found for {entity_name}. Proceeding without schema validation against Pydantic model for Silver.")

        silver_df = self._apply_entity_transformations(silver_df, entity_name)
        silver_df = silver_df.withColumn("SilverProcessedAt", F.current_timestamp()) # Add silver processing timestamp

        write_delta_table(
            df=silver_df,
            table_path=silver_table_path,
            mode=mode,
            partition_by_cols=silver_partition_columns,
            merge_schema_flag=True, # Important for silver layer
            overwrite_schema_flag=(mode=="overwrite")
        )

        final_count = silver_df.count()
        etl_logger.info(f"Wrote {final_count} rows to {silver_table_path} (mode: {mode}).")

        return {
            "entity": entity_name,
            "status": "success",
            "bronze_rows": initial_count,
            "silver_rows": final_count,
            "silver_table": silver_table_path,
            "model_validated": model_class is not None,
        }

    def _apply_entity_transformations(self, df: DataFrame, entity_name: str) -> DataFrame:
        """Apply entity-specific transformations based on model requirements."""
        # This function should ideally use configurations if transformations become complex
        etl_logger.debug(f"Applying entity-specific transformations for {entity_name} if any.")
        # Agreement specific transformations
        if entity_name == "Agreement":
            if "customFields" in df.columns:
                # This is an example; robust custom field handling is complex
                # It might involve parsing a map or array of structs.
                # Assuming customFields is a MapType and we want 'agreementNumber' if it exists.
                # A safer way is to check if the key exists or handle potential errors.
                # For now, keeping original logic for direct comparison.
                try: # Original try-except was too broad
                    df = df.withColumn(
                        "agreementNumber", col("customFields").getItem("agreementNumber")
                    )
                    etl_logger.debug(f"Extracted 'agreementNumber' from customFields for Agreement.")
                except Exception as e: # Catch specific Spark errors if possible
                    etl_logger.warning(f"Could not extract 'agreementNumber' from customFields for Agreement: {e}", exc_info=False)
                    df = df.withColumn("agreementNumber", lit(None).cast("string"))


        # TimeEntry specific transformations
        elif entity_name == "TimeEntry":
            if "invoice_id" in df.columns and "invoiceId" not in df.columns:
                df = df.withColumn("invoiceId", col("invoice_id"))
                etl_logger.debug(f"Standardized 'invoiceId' column for TimeEntry.")
        
        # PostedInvoice specific transformations are left as pass in original code.

        return df

    def process_silver_to_gold(self) -> dict[str, Any]:
        """Process silver layer to create gold layer data marts."""
        # Updated import paths
        from fabric_api.gold.dimensions import run_conformed_dimensions
        from fabric_api.gold.psa_facts import run_psa_financial_gold # Assuming new psa_facts module

        results = {}
        etl_logger.info("Starting Silver to Gold processing for PSA.")

        try:
            etl_logger.info("Creating conformed dimensions (PSA context)...")
            # Assuming run_conformed_dimensions is refactored for new utils
            dim_results = run_conformed_dimensions(
                silver_path=self.schemas["silver"], # Pass base silver schema path
                gold_path=self.schemas["gold"],   # Pass base gold schema path
                spark=self.spark,
                # include_bc_data=False, # This flag might be PSA/BC specific in the shared function
                # For PSA pipeline, we might imply it's PSA-only context or pass a source_system flag
                source_system_type="PSA" # Example of a new parameter if needed
            )
            results["dimensions"] = dim_results
            etl_logger.info("Conformed dimensions processing completed for PSA.")

            etl_logger.info("Creating PSA financial model (facts)...")
            # Assuming run_psa_financial_gold is refactored for new utils
            financial_results = run_psa_financial_gold(
                silver_db_name=self.schemas["silver"], # Pass relevant schema/DB names
                gold_db_name=self.schemas["gold"],
                spark=self.spark
            )
            results["facts"] = financial_results
            etl_logger.info("PSA financial model (facts) processing completed.")

            results["status"] = "success"

        except Exception as e:
            etl_logger.error(f"Error processing PSA Silver to Gold: {e}", exc_info=True)
            results["status"] = "error"
            results["error"] = str(e)

        return results

    def run_full_pipeline(
        self,
        entity_names: list[str] | None = None,
        client: ConnectWiseClient | None = None, # Allow passing an initialized client
        conditions: str | None = None,
        page_size: int = 100,
        max_pages: int | None = None,
        bronze_mode: str = "append",
        silver_mode: str = "overwrite",
        process_gold: bool = True,
    ) -> dict[str, dict[str, Any]]:
        """Run the full PSA medallion pipeline with model validation."""
        # Initialize client if not provided
        active_client = client if client else ConnectWiseClient()
        # ConnectWiseClient itself would use get_global_setting for its own base URL, etc.

        if entity_names is None:
            entity_names = list(self.model_mapping.keys())
            etl_logger.info(f"No entity names provided, processing all mapped entities: {entity_names}")

        results: Dict[str, Any] = {} # Simpler structure for results accumulation

        etl_logger.info("Phase 1: Extracting to Bronze layer")
        bronze_phase_results = {}
        for entity_name in entity_names:
            etl_logger.info(f"Processing {entity_name} to Bronze...")
            bronze_result = self.process_entity_to_bronze(
                entity_name=entity_name,
                client=active_client, # Pass the single client instance
                conditions=conditions,
                page_size=page_size,
                max_pages=max_pages,
                mode=bronze_mode,
            )
            bronze_phase_results[entity_name] = bronze_result
        results["bronze_processing"] = bronze_phase_results

        etl_logger.info("Phase 2: Transforming Bronze to Silver")
        silver_phase_results = {}
        for entity_name in entity_names:
            # Check if bronze processing was successful and produced data
            if bronze_phase_results.get(entity_name, {}).get("status") == "success" and \
               bronze_phase_results.get(entity_name, {}).get("bronze_rows", 0) > 0:
                etl_logger.info(f"Processing {entity_name} from Bronze to Silver...")
                silver_result = self.process_bronze_to_silver(
                    entity_name=entity_name, mode=silver_mode
                )
                silver_phase_results[entity_name] = silver_result
            else:
                etl_logger.info(f"Skipping Silver processing for {entity_name} due to no data or bronze error.")
                silver_phase_results[entity_name] = {
                    "status": "skipped",
                    "reason": bronze_phase_results.get(entity_name, {}).get("status", "unknown bronze status"),
                }
        results["silver_processing"] = silver_phase_results

        if process_gold:
            etl_logger.info("Phase 3: Processing Gold layer")
            gold_results = self.process_silver_to_gold()
            results["gold_processing"] = gold_results
        else:
            etl_logger.info("Skipping Gold layer processing as per configuration.")
            results["gold_processing"] = {"status": "skipped_as_per_config"}

        return results # Return the consolidated results dictionary

    def run_daily_pipeline(
        self,
        entity_names: list[str] | None = None,
        days_back: int = 1,
        client: ConnectWiseClient | None = None, # Allow passing client
        page_size: int = 100,
        max_pages: int | None = None,
        process_gold: bool = True,
    ) -> dict[str, dict[str, Any]]:
        """Run daily incremental pipeline with model validation."""
        # Dates for incremental load condition
        # Consider timezone awareness if source system uses specific timezones for lastUpdated
        end_date = datetime.utcnow() # Exclusive end
        start_date = end_date - timedelta(days=days_back) # Inclusive start
        
        # Format for ConnectWise API if it requires specific format (e.g. ISO 8601 with Z for UTC)
        # Example: lastUpdated>=[2023-01-01T00:00:00Z]
        # This needs to match the exact format expected by the `lastUpdated` field in ConnectWise.
        # Assuming a simple YYYY-MM-DD for now as per original code, but this might need adjustment.
        start_date_str = start_date.strftime("%Y-%m-%d") # Original format
        end_date_str = end_date.strftime("%Y-%m-%d")     # Original format

        etl_logger.info(f"Running daily PSA pipeline for updates between {start_date_str} and {end_date_str}.")
        
        # Construct condition string for ConnectWise API
        # This might need to be specific, e.g. using ISO format with time for some fields
        # For `lastUpdated` fields, a common pattern is ISO 8601.
        # Let's assume the original simple date format was sufficient for `lastUpdated`.
        # A more robust condition might be:
        # start_iso = start_date.isoformat() + "Z"
        # end_iso = end_date.isoformat() + "Z"
        # condition = f"lastUpdated >= [{start_iso}] AND lastUpdated < [{end_iso}]"
        # For now, sticking to the original pattern for direct comparison.
        condition = f"lastUpdated>=[{start_date_str}] AND lastUpdated<[{end_date_str}]"
        etl_logger.info(f"Using condition for daily run: {condition}")

        return self.run_full_pipeline(
            entity_names=entity_names,
            client=client, # Pass client through
            conditions=condition,
            page_size=page_size,
            max_pages=max_pages,
            bronze_mode="append",   # Daily bronze should always append
            silver_mode="overwrite", # Silver typically overwrites relevant partitions or full for snapshot tables
            process_gold=process_gold,
        )

# Example usage (commented out, for illustration)
# """
# from core_etl.spark_utils import get_spark_session # Caller would do this
# from fabric_api.pipeline.psa_pipeline import PSAPipeline # Assuming final path
# 
# spark = get_spark_session()
# pipeline = PSAPipeline(spark)
# 
# # Example: Run for specific entities, daily incremental
# daily_results = pipeline.run_daily_pipeline(
#     entity_names=["Agreement", "TimeEntry"], # Or None for all mapped entities
#     days_back=1,
#     process_gold=True
# )
# print(daily_results)

# # Example: Run full for specific entities, potentially with custom conditions
# full_run_results = pipeline.run_full_pipeline(
#     entity_names=["PostedInvoice"],
#     conditions="invoiceDate > [2023-01-01]", # Example filter
#     bronze_mode="overwrite", # For a full refresh scenario
#     silver_mode="overwrite",
#     process_gold=True
# )
# print(full_run_results)
# """
