"""
Streamlined Business Central medallion pipeline for Microsoft Fabric.

This pipeline simplifies the BC ETL process by:
1. Using autogenerated Pydantic models instead of YAML configs
2. Leveraging Fabric's managed Spark environment
3. Retaining key patterns (surrogate keys, dimensions, date tables)
4. Simplifying fact table creation
"""

from datetime import date, datetime, timedelta
from typing import Any

import pyspark.sql.functions as F
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.types import (
    DateType,
    DecimalType,
    IntegerType,
    StringType,
    StructField,
    StructType,
)
from pyspark.sql.window import Window

from fabric_api.bc_models import models
from fabric_api.core.pipeline_utils import log_dataframe_info, log_error, log_info


class BCMedallionPipeline:
    """Simplified Business Central medallion pipeline."""
    
    def __init__(self, spark: SparkSession):
        """Initialize the pipeline with a Spark session."""
        self.spark = spark
        
        # Map BC table names to Pydantic models (without numeric suffixes)
        self.model_mapping = {
            "AccountingPeriod": models.AccountingPeriod,
            "AccScheduleLine": models.AccScheduleLine,
            "AccScheduleName": models.AccScheduleName,
            "CompanyInformation": models.CompanyInformation,
            "Currency": models.Currency,
            "CustLedgerEntry": models.CustLedgerEntry,
            "Customer": models.Customer,
            "DefaultDimension": models.DefaultDimension,
            "DetailedCustLedgEntry": models.DetailedCustLedgEntry,
            "DetailedVendorLedgEntry": models.DetailedVendorLedgEntry,
            "Dimension": models.Dimension,
            "DimensionSetEntry": models.DimensionSetEntry,
            "DimensionValue": models.DimensionValue,
            "GLAccount": models.GLAccount,
            "GLEntry": models.GLEntry,
            "GeneralLedgerSetup": models.GeneralLedgerSetup,
            "Item": models.Item,
            "Job": models.Job,
            "JobLedgerEntry": models.JobLedgerEntry,
            "Resource": models.Resource,
            "SalesInvoiceHeader": models.SalesInvoiceHeader,
            "SalesInvoiceLine": models.SalesInvoiceLine,
            "Vendor": models.Vendor,
            "VendorLedgerEntry": models.VendorLedgerEntry,
        }
        
        # Define natural keys for each table (used for surrogate key generation)
        self.natural_keys = {
            "AccountingPeriod": ["StartingDate"],
            "AccScheduleLine": ["ScheduleName", "LineNo"],
            "AccScheduleName": ["Name"],
            "CompanyInformation": ["Name"],
            "Currency": ["Code"],
            "CustLedgerEntry": ["EntryNo"],
            "Customer": ["No"],
            "DefaultDimension": ["No", "TableID", "DimensionCode"],
            "DetailedCustLedgEntry": ["EntryNo"],
            "DetailedVendorLedgEntry": ["EntryNo"],
            "Dimension": ["Code"],
            "DimensionSetEntry": ["DimensionSetID", "DimensionCode"],
            "DimensionValue": ["DimensionCode", "Code"],
            "GLAccount": ["No"],
            "GLEntry": ["EntryNo"],
            "GeneralLedgerSetup": ["PrimaryKey"],  # This is usually a single row table
            "Item": ["No"],
            "Job": ["No"],
            "JobLedgerEntry": ["EntryNo"],
            "Resource": ["No"],
            "SalesInvoiceHeader": ["No"],
            "SalesInvoiceLine": ["DocumentNo", "LineNo"],
            "Vendor": ["No"],
            "VendorLedgerEntry": ["EntryNo"],
        }
        
        # Define dimension tables (master data)
        self.dimension_tables = [
            "AccountingPeriod", "AccScheduleLine", "AccScheduleName",
            "GLAccount", "Customer", "Vendor", "Item", "Dimension", 
            "DimensionValue", "Currency", "CompanyInformation", "Job", "Resource",
            "GeneralLedgerSetup", "DefaultDimension"
        ]
        
        # Define fact tables (transactional data)
        self.fact_tables = [
            "GLEntry", "CustLedgerEntry", "VendorLedgerEntry", 
            "JobLedgerEntry", "SalesInvoiceHeader", "SalesInvoiceLine",
            "DetailedCustLedgEntry", "DetailedVendorLedgEntry"
        ]

    def generate_surrogate_key(
        self, 
        df: DataFrame, 
        table_name: str,
        key_name: str | None = None
    ) -> DataFrame:
        """Generate surrogate keys for dimension tables."""
        if key_name is None:
            key_name = f"SK_{table_name}"
            
        natural_keys = self.natural_keys.get(table_name, [])
        if not natural_keys:
            log_error(f"No natural keys defined for table {table_name}")
            return df
            
        # Check if all natural keys exist in the dataframe
        missing_keys = [k for k in natural_keys if k not in df.columns]
        if missing_keys:
            log_error(f"Missing natural keys for {table_name}: {missing_keys}")
            return df
        
        # Create window specification partitioned by company if column exists
        company_col = "$Company" if "$Company" in df.columns else None
        
        # Check which natural keys exist in the dataframe
        existing_keys = [k for k in natural_keys if k in df.columns]
        if not existing_keys:
            log_error(f"None of the natural keys {natural_keys} exist in {table_name}")
            return df
            
        order_by_cols = [F.col(k) for k in existing_keys]
        
        if company_col:
            window = Window.partitionBy(F.col(company_col)).orderBy(*order_by_cols)
        else:
            window = Window.orderBy(*order_by_cols)
            
        # Generate surrogate key using row_number
        result_df = df.withColumn(
            key_name,
            F.row_number().over(window)
        )
        
        log_info(f"Generated surrogate key {key_name} for {table_name}")
        return result_df

    def create_date_dimension(
        self, 
        start_date: date, 
        end_date: date,
        fiscal_year_start_month: int = 1
    ) -> DataFrame:
        """Create a date dimension table."""
        log_info(f"Creating date dimension from {start_date} to {end_date}")
        
        # Generate date range
        dates = []
        current_date = start_date
        while current_date <= end_date:
            dates.append((current_date,))
            current_date += timedelta(days=1)
            
        # Create DataFrame
        date_df = self.spark.createDataFrame(dates, ["Date"])
        
        # Add date attributes
        date_df = date_df.withColumn("DateKey", 
            F.year("Date") * 10000 + F.month("Date") * 100 + F.dayofmonth("Date")
        )
        date_df = date_df.withColumn("Year", F.year("Date"))
        date_df = date_df.withColumn("Month", F.month("Date"))
        date_df = date_df.withColumn("Day", F.dayofmonth("Date"))
        date_df = date_df.withColumn("Quarter", F.quarter("Date"))
        date_df = date_df.withColumn("WeekOfYear", F.weekofyear("Date"))
        date_df = date_df.withColumn("DayOfWeek", F.dayofweek("Date"))
        date_df = date_df.withColumn("DayOfYear", F.dayofyear("Date"))
        date_df = date_df.withColumn("MonthName", F.date_format("Date", "MMMM"))
        date_df = date_df.withColumn("DayName", F.date_format("Date", "EEEE"))
        date_df = date_df.withColumn("IsWeekend", 
            F.when(F.col("DayOfWeek").isin([1, 7]), True).otherwise(False)
        )
        
        # Add fiscal year calculations
        date_df = date_df.withColumn("FiscalYearStartDate",
            F.when(F.col("Month") >= fiscal_year_start_month,
                F.make_date(F.col("Year"), F.lit(fiscal_year_start_month), F.lit(1))
            ).otherwise(
                F.make_date(F.col("Year") - 1, F.lit(fiscal_year_start_month), F.lit(1))
            )
        )
        
        date_df = date_df.withColumn("FiscalYear",
            F.when(F.col("Month") >= fiscal_year_start_month,
                F.col("Year")
            ).otherwise(F.col("Year") - 1)
        )
        
        date_df = date_df.withColumn("FiscalQuarter",
            F.concat(F.lit("FY"), F.col("FiscalYear"), F.lit("-Q"),
                F.when(F.col("Month") < fiscal_year_start_month,
                    F.ceil((F.col("Month") + 12 - fiscal_year_start_month + 1) / 3)
                ).otherwise(
                    F.ceil((F.col("Month") - fiscal_year_start_month + 1) / 3)
                )
            )
        )
        
        return date_df

    def create_dimension_bridge(
        self,
        silver_path: str,
        gold_path: str,
        dimension_types: dict[str, str] | None = None
    ) -> DataFrame:
        """
        Create a dimension bridge table mapping dimension sets to dimension values.
        
        This is critical for BC data models that use dimension framework.
        """
        log_info("Creating dimension bridge table")
        
        # Default dimension types if not provided
        if dimension_types is None:
            dimension_types = {
                "TEAM": "TEAM",
                "PRODUCT": "PRODUCT",
                "DEPARTMENT": "DEPARTMENT",
                "PROJECT": "PROJECT"
            }
        
        # Read dimension framework tables
        dim_set_entries = self.spark.table(f"{silver_path}.DimensionSetEntry")
        dim_values = self.spark.table(f"{silver_path}.DimensionValue")
        
        # Create base dataframe with all unique dimension set IDs
        base_df = dim_set_entries.select(
            F.col("$Company"), 
            F.col("DimensionSetID")
        ).distinct()
        
        result_df = base_df
        
        # Process each dimension type
        for dim_type, dim_code in dimension_types.items():
            log_info(f"Processing dimension type: {dim_type} with code: {dim_code}")
            
            # Filter entries for this dimension type
            type_entries = dim_set_entries.filter(F.col("DimensionCode") == dim_code)
            
            # Skip if no entries for this dimension type
            if type_entries.count() == 0:
                log_info(f"No entries found for dimension type {dim_type}, skipping")
                continue
                
            # Join dimension values to set entries
            type_df = (
                type_entries
                .join(
                    dim_values.filter(F.col("DimensionCode") == dim_code),
                    [
                        type_entries.DimensionValueCode == dim_values.Code,
                        type_entries["$Company"] == dim_values["$Company"]
                    ],
                    "left"
                )
                .select(
                    type_entries["$Company"],
                    type_entries["DimensionSetID"],
                    dim_values["Code"].alias(f"{dim_type}Code"),
                    dim_values["Name"].alias(f"{dim_type}Name")
                )
            )
            
            # Join to the result dataframe
            result_df = result_df.join(
                type_df, 
                ["$Company", "DimensionSetID"], 
                "left"
            )
            
            # Add dimension presence flags
            flag_col = f"Has{dim_type}Dimension"
            code_col = f"{dim_type}Code"
            
            result_df = result_df.withColumn(
                flag_col,
                F.when(F.col(code_col).isNotNull(), True).otherwise(False)
            )
        
        # Generate surrogate key for the bridge
        window = Window.partitionBy("$Company").orderBy("DimensionSetID")
        result_df = result_df.withColumn(
            "DimensionBridgeKey", 
            F.row_number().over(window)
        )
        
        log_info(f"Dimension bridge created with {result_df.count()} rows")
        return result_df

    def bronze_to_silver(
        self, 
        table_name: str,
        bronze_path: str,
        incremental: bool = False,
        watermark_column: str = "SystemModifiedAt"
    ) -> DataFrame | None:
        """Transform bronze table to silver with validation and standardization."""
        log_info(f"Processing {table_name} from Bronze to Silver")
        
        # Strip numeric suffix from table name if present
        import re
        clean_table_name = re.sub(r'\d+$', '', table_name)
        
        # Read bronze table (with original name)
        bronze_table_path = f"{bronze_path}.{table_name}"
        try:
            bronze_df = self.spark.table(bronze_table_path)
        except Exception as e:
            # Handle broken or missing tables gracefully
            error_msg = str(e)
            if "doesn't exist" in error_msg or "not found" in error_msg:
                log_error(f"Table {bronze_table_path} not found or broken - skipping")
            else:
                log_error(f"Failed to read bronze table {bronze_table_path}: {error_msg}")
            return None
            
        # Apply incremental filter if enabled
        if incremental and watermark_column in bronze_df.columns:
            # Get last watermark (would normally come from a metadata table)
            last_watermark = datetime.now() - timedelta(days=7)  # Default 7-day lookback
            bronze_df = bronze_df.filter(F.col(watermark_column) >= last_watermark)
            
        # Validate against Pydantic model if available (use clean name)
        model_class = self.model_mapping.get(clean_table_name)
        if model_class:
            # Get Spark schema from Pydantic model
            spark_schema = model_class.model_spark_schema()
            
            # Select only columns that exist in both bronze and model
            common_columns = list(set(bronze_df.columns) & set(spark_schema.fieldNames()))
            bronze_df = bronze_df.select(*common_columns)
            
            # Cast columns to match model schema types
            for field in spark_schema.fields:
                if field.name in bronze_df.columns:
                    bronze_df = bronze_df.withColumn(
                        field.name, 
                        F.col(field.name).cast(field.dataType)
                    )
        
        log_info(f"Transforming {table_name} (Bronze) to {clean_table_name} (Silver)")
        
        # Add processing metadata
        silver_df = bronze_df.withColumn("SilverProcessedAt", F.current_timestamp())
        
        # Apply data quality checks (use clean name for lookups)
        if clean_table_name in self.natural_keys:
            # Remove duplicates based on natural keys
            natural_keys = self.natural_keys[clean_table_name]
            if "$Company" in silver_df.columns:
                natural_keys = ["$Company"] + natural_keys
            silver_df = silver_df.dropDuplicates(natural_keys)
        
        log_dataframe_info(silver_df, f"Silver {clean_table_name}")
        
        # Return the dataframe along with its clean name for the caller
        silver_df._table_name = clean_table_name
        return silver_df

    def silver_to_gold_dimension(
        self,
        table_name: str,
        silver_path: str
    ) -> DataFrame | None:
        """Transform silver dimension table to gold with surrogate keys."""
        log_info(f"Processing dimension {table_name} from Silver to Gold")
        
        # Read silver table
        silver_table_path = f"{silver_path}.{table_name}"
        try:
            silver_df = self.spark.table(silver_table_path)
        except Exception as e:
            log_error(f"Failed to read silver table {silver_table_path}: {str(e)}")
            return None
            
        # Generate surrogate key
        gold_df = self.generate_surrogate_key(silver_df, table_name)
        
        # Add dimension-specific transformations
        if table_name == "GLAccount":
            # Add account categorizations
            gold_df = gold_df.withColumn("AccountCategory",
                F.when(F.col("AccountCategory").isNull(), "Unknown")
                .otherwise(F.col("AccountCategory"))
            )
            
        elif table_name == "Customer":
            # Add customer segments
            gold_df = gold_df.withColumn("CustomerSegment",
                F.when(F.col("CustomerPostingGroup").isNull(), "Unknown")
                .otherwise(F.col("CustomerPostingGroup"))
            )
            
        # Add gold processing metadata
        gold_df = gold_df.withColumn("GoldProcessedAt", F.current_timestamp())
        
        log_dataframe_info(gold_df, f"Gold Dimension {table_name}")
        return gold_df

    def silver_to_gold_fact(
        self,
        table_name: str,
        silver_path: str,
        gold_path: str,
        min_year: int | None = None
    ) -> DataFrame | None:
        """Transform silver fact table to gold with dimension lookups."""
        log_info(f"Processing fact {table_name} from Silver to Gold")
        
        # Read silver table
        silver_table_path = f"{silver_path}.{table_name}"
        try:
            fact_df = self.spark.table(silver_table_path)
        except Exception as e:
            log_error(f"Failed to read silver table {silver_table_path}: {str(e)}")
            return None
            
        # Filter by minimum year if specified
        if min_year and "PostingDate" in fact_df.columns:
            fact_df = fact_df.filter(F.year("PostingDate") >= min_year)
            
        # Join with date dimension
        if "PostingDate" in fact_df.columns:
            date_dim_path = f"{gold_path}.dim_Date"
            try:
                date_dim = self.spark.table(date_dim_path)
                fact_df = fact_df.join(
                    date_dim,
                    fact_df.PostingDate == date_dim.Date,
                    "left"
                ).select(
                    fact_df["*"],
                    date_dim["DateKey"],
                    date_dim["FiscalYear"],
                    date_dim["FiscalQuarter"],
                    date_dim["Year"].alias("PostingYear")
                )
            except Exception as e:
                log_error(f"Failed to join with date dimension: {str(e)}")
                
        # Join with dimension bridge if DimensionSetID exists
        if "DimensionSetID" in fact_df.columns:
            dimension_bridge_path = f"{gold_path}.dim_DimensionBridge"
            try:
                dimension_bridge = self.spark.table(dimension_bridge_path)
                fact_df = fact_df.join(
                    dimension_bridge,
                    (fact_df["DimensionSetID"] == dimension_bridge["DimensionSetID"]) & 
                    (fact_df["$Company"] == dimension_bridge["$Company"]),
                    "left"
                ).select(
                    fact_df["*"],
                    dimension_bridge["DimensionBridgeKey"],
                    # Include dimension codes and names
                    *[col for col in dimension_bridge.columns 
                      if col.endswith("Code") or col.endswith("Name") or col.startswith("Has")]
                )
            except Exception as e:
                log_error(f"Failed to join with dimension bridge: {str(e)}")
                
        # Join with related dimensions based on table type
        if table_name == "GLEntry":
            # Join with GLAccount dimension
            gl_account_dim_path = f"{gold_path}.dim_GLAccount"
            try:
                gl_account_dim = self.spark.table(gl_account_dim_path)
                fact_df = fact_df.join(
                    gl_account_dim,
                    (fact_df["GLAccountNo"] == gl_account_dim["No"]) & 
                    (fact_df["$Company"] == gl_account_dim["$Company"]),
                    "left"
                ).select(
                    fact_df["*"],
                    gl_account_dim["SK_GLAccount"].alias("GLAccountKey"),
                    gl_account_dim["AccountCategory"]
                )
            except Exception as e:
                log_error(f"Failed to join with GL Account dimension: {str(e)}")
                
        elif table_name == "CustLedgerEntry":
            # Join with Customer dimension
            customer_dim_path = f"{gold_path}.dim_Customer"
            try:
                customer_dim = self.spark.table(customer_dim_path)
                fact_df = fact_df.join(
                    customer_dim,
                    (fact_df["CustomerNo"] == customer_dim["No"]) & 
                    (fact_df["$Company"] == customer_dim["$Company"]),
                    "left"
                ).select(
                    fact_df["*"],
                    customer_dim["SK_Customer"].alias("CustomerKey"),
                    customer_dim["CustomerPostingGroup"]
                )
            except Exception as e:
                log_error(f"Failed to join with Customer dimension: {str(e)}")
                
        elif table_name == "VendorLedgerEntry":
            # Join with Vendor dimension
            vendor_dim_path = f"{gold_path}.dim_Vendor"
            try:
                vendor_dim = self.spark.table(vendor_dim_path)
                fact_df = fact_df.join(
                    vendor_dim,
                    (fact_df["VendorNo"] == vendor_dim["No"]) & 
                    (fact_df["$Company"] == vendor_dim["$Company"]),
                    "left"
                ).select(
                    fact_df["*"],
                    vendor_dim["SK_Vendor"].alias("VendorKey"),
                    vendor_dim["VendorPostingGroup"]
                )
            except Exception as e:
                log_error(f"Failed to join with Vendor dimension: {str(e)}")
                
        # Add calculated measures
        if "Amount" in fact_df.columns:
            fact_df = fact_df.withColumn("AmountSign",
                F.when(F.col("Amount") > 0, 1)
                .when(F.col("Amount") < 0, -1)
                .otherwise(0)
            )
            
        # Add gold processing metadata
        fact_df = fact_df.withColumn("GoldProcessedAt", F.current_timestamp())
        
        log_dataframe_info(fact_df, f"Gold Fact {table_name}")
        return fact_df

    def run_pipeline(
        self,
        bronze_path: str,
        silver_path: str,
        gold_path: str,
        tables: list[str] | None = None,
        incremental: bool = False,
        min_year: int | None = None,
        dimension_types: dict[str, str] | None = None
    ):
        """Run the complete medallion pipeline."""
        log_info("Starting BC Medallion Pipeline")
        
        # If no specific tables provided, get from Bronze layer
        # (This is handled later in Phase 2)
            
        # Phase 1: Create Date Dimension
        log_info("Creating Date Dimension")
        start_date = date(min_year if min_year else 2020, 1, 1)
        end_date = date(datetime.now().year + 5, 12, 31)
        date_dim = self.create_date_dimension(start_date, end_date)
        
        # Write date dimension to gold
        date_dim.write.mode("overwrite").saveAsTable(f"{gold_path}.dim_Date")
        
        # Define PSA tables to exclude (not from BC) - these won't have numeric suffixes
        # We'll also filter by checking if the table has a numeric suffix (BC tables)
        psa_tables = {
            "Agreement", "PostedInvoice", "UnpostedInvoice", 
            "ExpenseEntry", "TimeEntry", "ProductItem"
        }
        
        # Phase 2: Process Bronze to Silver
        log_info("Processing Bronze to Silver")
        silver_tables_created = []
        
        # First, get actual Bronze tables if tables not specified
        if not tables:
            bronze_tables = self.spark.sql(f"SHOW TABLES IN {bronze_path}").collect()
            tables = [t.tableName for t in bronze_tables]
        
        for table_name in tables:
            # Skip broken tables
            if table_name in ["VendorLedgerEntry25"]:  # Known broken table
                log_info(f"Skipping broken table: {table_name}")
                continue
                    
            # Skip PSA tables (no numeric suffix)
            import re
            clean_name = re.sub(r'\d+$', '', table_name)
            if clean_name in psa_tables or table_name in psa_tables:
                log_info(f"Skipping PSA table: {table_name}")
                continue
                
            # BC tables should have numeric suffixes, skip if it doesn't and not in mapping
            has_suffix = re.search(r'\d+$', table_name) is not None
            if not has_suffix and clean_name not in self.model_mapping:
                log_info(f"Skipping non-BC table: {table_name}")
                continue
                
            try:
                silver_df = self.bronze_to_silver(
                    table_name, 
                    bronze_path, 
                    incremental=incremental
                )
                if silver_df:
                    # Get clean table name from the dataframe or strip numeric suffix
                    clean_table_name = getattr(silver_df, '_table_name', clean_name)
                    
                    # Write to silver layer with clean name and merge schema option
                    silver_table_path = f"{silver_path}.{clean_table_name}"
                    write_mode = "append" if incremental else "overwrite"
                    silver_df.write.mode(write_mode).option("mergeSchema", "true").saveAsTable(silver_table_path)
                    silver_tables_created.append(clean_table_name)
            except Exception as e:
                log_error(f"Failed to process {table_name}: {str(e)}")
                # Continue with next table
                continue
                
        # Phase 3: Process Silver to Gold - Dimensions first
        log_info("Processing Silver to Gold - Dimensions")
        for table_name in self.dimension_tables:
            # Only process if we created it in Silver
            if table_name in silver_tables_created:
                try:
                    gold_dim_df = self.silver_to_gold_dimension(table_name, silver_path)
                    if gold_dim_df:
                        # Write to gold layer
                        gold_table_path = f"{gold_path}.dim_{table_name}"
                        gold_dim_df.write.mode("overwrite").saveAsTable(gold_table_path)
                except Exception as e:
                    log_error(f"Failed to process dimension {table_name}: {str(e)}")
                    continue
        
        # Phase 4: Create Dimension Bridge (before facts)
        log_info("Creating Dimension Bridge")
        if all(dim in silver_tables_created for dim in ["DimensionSetEntry", "DimensionValue"]):
            dimension_bridge = self.create_dimension_bridge(
                silver_path, 
                gold_path, 
                dimension_types
            )
            if dimension_bridge:
                # Write dimension bridge to gold
                bridge_table_path = f"{gold_path}.dim_DimensionBridge"
                dimension_bridge.write.mode("overwrite").partitionBy("$Company").saveAsTable(bridge_table_path)
                    
        # Phase 5: Process Silver to Gold - Facts
        log_info("Processing Silver to Gold - Facts")
        for table_name in self.fact_tables:
            # Only process if we created it in Silver
            if table_name in silver_tables_created:
                try:
                    gold_fact_df = self.silver_to_gold_fact(
                        table_name, 
                        silver_path, 
                        gold_path,
                        min_year=min_year
                    )
                    if gold_fact_df:
                        # Write to gold layer with partitioning
                        gold_table_path = f"{gold_path}.fact_{table_name}"
                        partition_cols = []
                        if "PostingYear" in gold_fact_df.columns:
                            partition_cols.append("PostingYear")
                        if "$Company" in gold_fact_df.columns:
                            partition_cols.append("$Company")
                            
                        write_operation = gold_fact_df.write.mode("overwrite")
                        if partition_cols:
                            write_operation = write_operation.partitionBy(*partition_cols)
                        write_operation.saveAsTable(gold_table_path)
                except Exception as e:
                    log_error(f"Failed to process fact {table_name}: {str(e)}")
                    continue
                    
        log_info("BC Medallion Pipeline completed successfully")


# Example usage in a Fabric notebook:
"""
from pyspark.sql import SparkSession
from fabric_api.bc_medallion_pipeline import BCMedallionPipeline

# Spark session is already available in Fabric notebooks
spark = spark

# Initialize pipeline
pipeline = BCMedallionPipeline(spark)

# Run the pipeline
pipeline.run_pipeline(
    bronze_path="bronze",
    silver_path="silver", 
    gold_path="gold",
    incremental=False,
    min_year=2020
)
"""