{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e65fd2-6ff5-4e51-afdb-926b7f6ec71c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install the wheel files\n",
    "%pip install /lakehouse/default/Files/unified_etl_core-1.0.0-py3-none-any.whl\n",
    "%pip install /lakehouse/default/Files/unified_etl_connectwise-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c15283f-cad7-4463-93bf-8f94f8eaed8e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-28T09:14:37.1729714Z",
       "execution_start_time": "2025-05-28T09:14:25.5575003Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f262c90f-b3a6-4244-b8ec-db997688b77d",
       "queued_time": "2025-05-28T09:13:39.5061787Z",
       "session_id": "89f93da4-344b-49ff-8a91-0fe14404ed3d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": "StatementMeta(, 89f93da4-344b-49ff-8a91-0fe14404ed3d, 11, Finished, Available, Finished)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered table mappings:\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Automatically discover table mappings\n",
    "def discover_table_mappings(spark, prefix_pattern=\"bronze_cw_\"):\n",
    "    \"\"\"Discover existing tables and create mappings.\"\"\"\n",
    "    tables = spark.sql(f\"SHOW TABLES LIKE '{prefix_pattern}*'\").collect()\n",
    "\n",
    "    mappings = {\"bronze\": {}, \"silver\": {}, \"gold\": {}}\n",
    "\n",
    "    for row in tables:\n",
    "        table_name = row.tableName\n",
    "        # Extract entity name from table name\n",
    "        if table_name.startswith(prefix_pattern):\n",
    "            entity_name = table_name.replace(prefix_pattern, \"\")\n",
    "            mappings[\"bronze\"][entity_name] = table_name\n",
    "            # Assume silver and gold follow same pattern\n",
    "            mappings[\"silver\"][entity_name] = f\"silver_cw_{entity_name}\"\n",
    "            mappings[\"gold\"][entity_name] = f\"gold_cw_{entity_name}\"\n",
    "\n",
    "    return mappings\n",
    "\n",
    "\n",
    "# Discover existing tables\n",
    "discovered_mappings = discover_table_mappings(spark)\n",
    "print(\"Discovered table mappings:\")\n",
    "for layer, entities in discovered_mappings.items():\n",
    "    if entities:\n",
    "        print(f\"\\n{layer.upper()}:\")\n",
    "        for entity, table in entities.items():\n",
    "            print(f\"  {entity} -> {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47737b6d-223f-4b74-aeb0-b719abdd5178",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"0b6bc113-3ad7-4edf-b3c6-3ebcfef3e0ab\",\"activityId\":\"89f93da4-344b-49ff-8a91-0fe14404ed3d\",\"applicationId\":\"application_1748423414310_0001\",\"jobGroupId\":\"15\",\"advices\":{\"warn\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-28T09:19:45.6715299Z",
       "execution_start_time": "2025-05-28T09:18:26.0868223Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1afd5a85-dbe6-433d-bf24-8d3e3583f180",
       "queued_time": "2025-05-28T09:18:26.0855627Z",
       "session_id": "89f93da4-344b-49ff-8a91-0fe14404ed3d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": "StatementMeta(, 89f93da4-344b-49ff-8a91-0fe14404ed3d, 15, Finished, Available, Finished)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Discovering tables in schema-enabled lakehouse...\n\n1. Listing tables in bronze schema:\n   Found 5 tables in bronze schema\n   - bronze_cw_agreement\n   - bronze_cw_expenseentry\n   - bronze_cw_invoice\n   - bronze_cw_productitem\n   - bronze_cw_timeentry\n\n2. Listing all tables in current database:\n   Found 0 total tables\n\n3. Checking bronze schema tables:\n   ‚úì Found agreement: bronze.bronze_cw_agreement (1,933 rows)\n   ‚úì Found invoice: bronze.bronze_cw_invoice (37,294 rows)\n   ‚úì Found timeentry: bronze.bronze_cw_timeentry (535,963 rows)\n   ‚úì Found expenseentry: bronze.bronze_cw_expenseentry (14,188 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 09:18:47,558 -root - INFO - ‚úÖ Integration 'connectwise' detected and loaded\n2025-05-28 09:18:47,559 -root - INFO - ‚ö†Ô∏è Integration 'businesscentral' not available (package not installed)\n2025-05-28 09:18:47,561 -root - INFO - ‚ö†Ô∏è Integration 'jira' not available (package not installed)\n2025-05-28 09:18:47,562 -root - INFO - Running ETL pipeline for integrations: ['connectwise']\n2025-05-28 09:18:47,562 -root - INFO - Processing layers: ['silver']\n2025-05-28 09:18:47,563 -root - INFO - Processing integration: connectwise\n2025-05-28 09:18:47,563 -root - INFO - Running silver layer for connectwise\n2025-05-28 09:18:48,324 -root - INFO - Processing 1933 rows from bronze.bronze_cw_agreement\n2025-05-28 09:18:48,365 -root - INFO - Skipping validation - data was validated in Bronze layer\n2025-05-28 09:18:48,365 -root - INFO - No column mappings provided, skipping type conversions\n2025-05-28 09:18:51,835 -root - INFO - Flattening 28 nested columns: ['type', 'company', 'contact', 'site', 'subContractCompany', 'subContractContact', 'parentAgreement', 'location', 'department', 'opportunity', 'sla', 'billingCycle', 'billingTerms', 'billToCompany', 'billToContact', 'billToSite', 'taxCode', 'workRole', 'workType', 'projectType', 'invoiceTemplate', 'currency', 'companyLocation', 'shipToCompany', 'shipToContact', 'shipToSite', '_info', 'customFields']\n2025-05-28 09:18:52,937 -root - INFO - Silver transformation successful with 157 columns\n2025-05-28 09:19:01,663 -root - INFO - Processed 1933 records to silver.silver_cw_agreement\n2025-05-28 09:19:02,049 -root - INFO - Processing 535963 rows from bronze.bronze_cw_timeentry\n2025-05-28 09:19:02,075 -root - INFO - Skipping validation - data was validated in Bronze layer\n2025-05-28 09:19:02,076 -root - INFO - No column mappings provided, skipping type conversions\n2025-05-28 09:19:03,046 -root - INFO - Flattening 16 nested columns: ['company', 'member', 'location', 'department', 'workType', 'workRole', 'agreement', 'activity', 'invoice', 'timeSheet', 'ticket', 'project', 'phase', 'taxCode', '_info', 'customFields']\n2025-05-28 09:19:03,749 -root - INFO - Silver transformation successful with 105 columns\n2025-05-28 09:19:20,137 -root - INFO - Processed 535963 records to silver.silver_cw_timeentry\n2025-05-28 09:19:20,557 -root - INFO - Processing 37294 rows from bronze.bronze_cw_invoice\n2025-05-28 09:19:20,587 -root - INFO - Skipping validation - data was validated in Bronze layer\n2025-05-28 09:19:20,587 -root - INFO - No column mappings provided, skipping type conversions\n2025-05-28 09:19:21,347 -root - INFO - Flattening 23 nested columns: ['status', 'company', 'billToCompany', 'shipToCompany', 'billingSite', 'shippingSite', 'billingTerms', 'invoiceTemplate', 'location', 'department', 'territory', 'taxCode', 'currency', 'billingSetupReference', 'ticket', 'project', 'phase', 'salesOrder', 'agreement', 'glBatch', 'unbatchedBatch', '_info', 'customFields']\n2025-05-28 09:19:22,282 -root - INFO - Silver transformation successful with 141 columns\n2025-05-28 09:19:28,230 -root - INFO - Processed 37294 records to silver.silver_cw_invoice\n2025-05-28 09:19:28,630 -root - INFO - Processing 14188 rows from bronze.bronze_cw_expenseentry\n2025-05-28 09:19:28,651 -root - INFO - Skipping validation - data was validated in Bronze layer\n2025-05-28 09:19:28,651 -root - INFO - No column mappings provided, skipping type conversions\n2025-05-28 09:19:29,336 -root - INFO - Flattening 15 nested columns: ['expenseReport', 'company', 'type', 'member', 'paymentMethod', 'classification', 'agreement', 'taxes', 'invoice', 'currency', 'ticket', 'project', 'phase', '_info', 'customFields']\n2025-05-28 09:19:29,788 -root - INFO - Silver transformation successful with 79 columns\n2025-05-28 09:19:33,070 -root - INFO - Processed 14188 records to silver.silver_cw_expenseentry\n2025-05-28 09:19:33,466 -root - INFO - Processing 379748 rows from bronze.bronze_cw_productitem\n2025-05-28 09:19:33,511 -root - INFO - Skipping validation - data was validated in Bronze layer\n2025-05-28 09:19:33,512 -root - INFO - No column mappings provided, skipping type conversions\n2025-05-28 09:19:34,187 -root - INFO - Flattening 23 nested columns: ['catalogItem', 'unitOfMeasure', 'agreement', 'location', 'businessUnit', 'vendor', 'recurring', 'sla', 'entityType', 'ticket', 'project', 'phase', 'salesOrder', 'opportunity', 'invoice', 'warehouseIdObject', 'warehouseBinIdObject', 'taxCode', 'company', 'forecastStatus', 'invoiceGrouping', '_info', 'customFields']\n2025-05-28 09:19:35,172 -root - INFO - Flattening 2 nested columns: ['recurringBillCycle', 'recurringAgreementType']\n2025-05-28 09:19:35,634 -root - INFO - Silver transformation successful with 149 columns\n2025-05-28 09:19:44,519 -root - INFO - Processed 379748 records to silver.silver_cw_productitem\n2025-05-28 09:19:44,829 -root - ERROR - Silver processing failed for productrecurring: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cw_productrecurring` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [bronze_cw_productrecurring], [], false\n\n"
     ]
    }
   ],
   "source": [
    "from unified_etl_core.main import run_etl_pipeline\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(\"üîç Discovering tables in schema-enabled lakehouse...\")\n",
    "\n",
    "# For schema-enabled lakehouses, we need to use the proper namespace\n",
    "# Format: workspace.lakehouse.schema.table\n",
    "\n",
    "# Try listing tables in the bronze schema\n",
    "print(\"\\n1. Listing tables in bronze schema:\")\n",
    "try:\n",
    "    bronze_tables = spark.sql(\"SHOW TABLES IN bronze\").collect()\n",
    "    print(f\"   Found {len(bronze_tables)} tables in bronze schema\")\n",
    "    for row in bronze_tables:\n",
    "        print(f\"   - {row.tableName}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {str(e)[:200]}...\")\n",
    "\n",
    "# Alternative: List all tables and filter\n",
    "print(\"\\n2. Listing all tables in current database:\")\n",
    "all_tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "print(f\"   Found {len(all_tables)} total tables\")\n",
    "\n",
    "# Build table mappings\n",
    "table_mappings = {\"bronze\": {}, \"silver\": {}, \"gold\": {}}\n",
    "\n",
    "# For schema-enabled lakehouse, tables are referenced as schema.table\n",
    "print(\"\\n3. Checking bronze schema tables:\")\n",
    "bronze_entity_names = [\"agreement\", \"invoice\", \"timeentry\", \"expenseentry\", \"productitem\"]\n",
    "\n",
    "for entity in bronze_entity_names:\n",
    "    # Try different table name formats\n",
    "    table_candidates = [\n",
    "        f\"bronze.bronze_cw_{entity}\",  # schema.table format\n",
    "        f\"bronze_cw_{entity}\",  # just table name\n",
    "        f\"`bronze`.`bronze_cw_{entity}`\",  # quoted format\n",
    "    ]\n",
    "\n",
    "    for table_name in table_candidates:\n",
    "        try:\n",
    "            count = spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").collect()[0][0]\n",
    "            print(f\"   ‚úì Found {entity}: {table_name} ({count:,} rows)\")\n",
    "            table_mappings[\"bronze\"][entity] = table_name\n",
    "            table_mappings[\"silver\"][entity] = f\"silver.silver_cw_{entity}\"\n",
    "            table_mappings[\"gold\"][entity] = f\"gold.gold_cw_{entity}\"\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"\\n‚úÖ Discovered {len(table_mappings['bronze'])} bronze tables\")\n",
    "print(f\"Table mappings: {table_mappings}\")\n",
    "\n",
    "if table_mappings[\"bronze\"]:\n",
    "    print(\"\\nüöÄ Starting Silver Layer Processing...\")\n",
    "    run_etl_pipeline(\n",
    "        integrations=[\"connectwise\"], layers=[\"silver\"], config={}, table_mappings=table_mappings\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n‚ùå No bronze tables found. Check schema configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019e60a2-fea4-450d-885c-b8a8e200a1a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-28T09:21:57.2855606Z",
       "execution_start_time": "2025-05-28T09:21:55.4985923Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "12044800-0935-475e-b70e-eeb030b36b08",
       "queued_time": "2025-05-28T09:21:55.4974469Z",
       "session_id": "89f93da4-344b-49ff-8a91-0fe14404ed3d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": "StatementMeta(, 89f93da4-344b-49ff-8a91-0fe14404ed3d, 16, Finished, Available, Finished)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n+---------+---------+-----------+\n\n\nSilver Table Summary:\n==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check silver tables\n",
    "silver_tables = spark.sql(\"SHOW TABLES LIKE 'silver_cw*'\")\n",
    "silver_tables.show()\n",
    "\n",
    "# Check row counts and sample data\n",
    "print(\"\\nSilver Table Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for row in silver_tables.collect():\n",
    "    table_name = row.tableName\n",
    "    df = spark.table(table_name)\n",
    "    count = df.count()\n",
    "    print(f\"\\n{table_name}: {count:,} rows\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"Sample columns: {', '.join(df.columns[:5])}...\")\n",
    "\n",
    "    # Show sample data\n",
    "    print(\"\\nSample data:\")\n",
    "    df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0eaa41-a7d4-4604-a5aa-65f8f27f69fc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-28T09:14:37.4472697Z",
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "cancelled",
       "parent_msg_id": "e1a5dea7-a9e0-4ded-b2e4-1766b6c28b3b",
       "queued_time": "2025-05-28T09:13:39.5119036Z",
       "session_id": "89f93da4-344b-49ff-8a91-0fe14404ed3d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "cancelled",
       "statement_id": -1,
       "statement_ids": null
      },
      "text/plain": "StatementMeta(, 89f93da4-344b-49ff-8a91-0fe14404ed3d, -1, Cancelled, , Cancelled)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nüìä Silver Layer Processing Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all tables and their sizes\n",
    "all_tables = spark.sql(\"SHOW TABLES\")\n",
    "table_sizes = {}\n",
    "\n",
    "for row in all_tables.collect():\n",
    "    table = row.tableName\n",
    "    if table.startswith((\"bronze_\", \"silver_\")):\n",
    "        try:\n",
    "            count = spark.table(table).count()\n",
    "            table_sizes[table] = count\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Group by layer\n",
    "bronze = {k: v for k, v in table_sizes.items() if k.startswith(\"bronze_\")}\n",
    "silver = {k: v for k, v in table_sizes.items() if k.startswith(\"silver_\")}\n",
    "\n",
    "print(f\"Bronze Tables: {len(bronze)} tables, {sum(bronze.values()):,} total rows\")\n",
    "print(f\"Silver Tables: {len(silver)} tables, {sum(silver.values()):,} total rows\")\n",
    "\n",
    "print(\"\\n‚úÖ Silver processing completed successfully!\")\n",
    "print(\"\\nKey improvements in this version:\")\n",
    "print(\"- No collect() or row-by-row processing\")\n",
    "print(\"- Distributed Spark operations throughout\")\n",
    "print(\"- Scalable to millions of rows\")\n",
    "print(\"\\nNext step: Run Gold layer processing in a separate notebook\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "22176b2b-5fe3-46bb-8b0a-193e1356c3cf",
    "default_lakehouse_name": "Lakehouse",
    "default_lakehouse_workspace_id": "a3a23dd7-9f52-4b88-b056-46da3617c0b2",
    "known_lakehouses": [
     {
      "id": "22176b2b-5fe3-46bb-8b0a-193e1356c3cf"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}