{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Silver Layer Processing Only\n",
    "\n",
    "This notebook processes existing bronze tables to silver with schema transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the Wheel Files\n",
    "\n",
    "First, upload the wheel files to your Lakehouse Files directory, then install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the wheel files\n",
    "%pip install /lakehouse/default/Files/dist/unified_etl_core-1.0.0-py3-none-any.whl\n",
    "%pip install /lakehouse/default/Files/dist/unified_etl_connectwise-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Existing Bronze Tables\n",
    "\n",
    "Let's see what bronze tables we have with data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# List bronze tables\nbronze_tables = spark.sql(\"SHOW TABLES LIKE 'bronze_cw_*'\")\nbronze_tables.show()\n\n# Check row counts\nprint(\"\\nBronze Table Row Counts:\")\nprint(\"=\" * 50)\nfor row in bronze_tables.collect():\n    table_name = row.tableName\n    count = spark.table(table_name).count()\n    print(f\"{table_name}: {count:,} rows\")"
  },
  {
   "cell_type": "code",
   "source": "# Alternative: Automatically discover table mappings\ndef discover_table_mappings(spark, prefix_pattern=\"bronze_cw_\"):\n    \"\"\"Discover existing tables and create mappings.\"\"\"\n    tables = spark.sql(f\"SHOW TABLES LIKE '{prefix_pattern}*'\").collect()\n    \n    mappings = {\"bronze\": {}, \"silver\": {}, \"gold\": {}}\n    \n    for row in tables:\n        table_name = row.tableName\n        # Extract entity name from table name\n        if table_name.startswith(prefix_pattern):\n            entity_name = table_name.replace(prefix_pattern, \"\")\n            mappings[\"bronze\"][entity_name] = table_name\n            # Assume silver and gold follow same pattern\n            mappings[\"silver\"][entity_name] = f\"silver_cw_{entity_name}\"\n            mappings[\"gold\"][entity_name] = f\"gold_cw_{entity_name}\"\n    \n    return mappings\n\n# Discover existing tables\ndiscovered_mappings = discover_table_mappings(spark)\nprint(\"Discovered table mappings:\")\nfor layer, entities in discovered_mappings.items():\n    if entities:\n        print(f\"\\n{layer.upper()}:\")\n        for entity, table in entities.items():\n            print(f\"  {entity} -> {table}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Alternative: Automatically discover table mappings with full paths\ndef discover_table_mappings(spark, prefix_pattern=\"bronze_cw_\"):\n    \"\"\"Discover existing tables and create mappings with full Lakehouse paths.\"\"\"\n    tables = spark.sql(f\"SHOW TABLES LIKE '{prefix_pattern}*'\").collect()\n    \n    mappings = {\"bronze\": {}, \"silver\": {}, \"gold\": {}}\n    \n    for row in tables:\n        table_name = row.tableName\n        # Extract entity name from table name\n        if table_name.startswith(prefix_pattern):\n            entity_name = table_name.replace(prefix_pattern, \"\")\n            # Use fully qualified names\n            mappings[\"bronze\"][entity_name] = f\"Lakehouse.bronze.{table_name}\"\n            mappings[\"silver\"][entity_name] = f\"Lakehouse.silver.silver_cw_{entity_name}\"\n            mappings[\"gold\"][entity_name] = f\"Lakehouse.gold.gold_cw_{entity_name}\"\n    \n    return mappings\n\n# Discover existing tables\ndiscovered_mappings = discover_table_mappings(spark)\nprint(\"Discovered table mappings:\")\nfor layer, entities in discovered_mappings.items():\n    if entities:\n        print(f\"\\n{layer.upper()}:\")\n        for entity, table in entities.items():\n            print(f\"  {entity} -> {table}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from unified_etl_core.main import run_etl_pipeline\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n# Discover existing bronze tables in schema-enabled lakehouse\nprint(\"üîç Discovering tables in schema-enabled lakehouse...\")\n\n# For schema-enabled lakehouses, we need to use the proper namespace\n# Format: workspace.lakehouse.schema.table\n\n# Try listing tables in the bronze schema\nprint(\"\\n1. Listing tables in bronze schema:\")\ntry:\n    bronze_tables = spark.sql(\"SHOW TABLES IN bronze\").collect()\n    print(f\"   Found {len(bronze_tables)} tables in bronze schema\")\n    for row in bronze_tables:\n        print(f\"   - {row.tableName}\")\nexcept Exception as e:\n    print(f\"   Error: {str(e)[:200]}...\")\n\n# Alternative: List all tables and filter\nprint(\"\\n2. Listing all tables in current database:\")\nall_tables = spark.sql(\"SHOW TABLES\").collect()\nprint(f\"   Found {len(all_tables)} total tables\")\n\n# Build table mappings dynamically\ntable_mappings = {\"bronze\": {}, \"silver\": {}, \"gold\": {}}\n\n# For schema-enabled lakehouse, tables are referenced as schema.table\nprint(\"\\n3. Checking bronze schema tables:\")\n# Use camelCase entity names that match the actual table names\nbronze_entity_names = [\"agreement\", \"invoice\", \"timeentry\", \"expenseentry\", \"productitem\"]\n\nfor entity in bronze_entity_names:\n    # Try different table name formats\n    table_candidates = [\n        f\"bronze.bronze_cw_{entity}\",  # schema.table format\n        f\"bronze_cw_{entity}\",          # just table name\n        f\"`bronze`.`bronze_cw_{entity}`\"  # quoted format\n    ]\n    \n    for table_name in table_candidates:\n        try:\n            count = spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").collect()[0][0]\n            print(f\"   ‚úì Found {entity}: {table_name} ({count:,} rows)\")\n            table_mappings[\"bronze\"][entity] = table_name\n            table_mappings[\"silver\"][entity] = f\"silver.silver_cw_{entity}\"\n            table_mappings[\"gold\"][entity] = f\"gold.gold_cw_{entity}\"\n            break\n        except:\n            continue\n\nprint(f\"\\n‚úÖ Discovered {len(table_mappings['bronze'])} bronze tables\")\nprint(f\"Table mappings: {table_mappings}\")\n\nif table_mappings[\"bronze\"]:\n    print(\"\\nüöÄ Starting Silver Layer Processing...\")\n    run_etl_pipeline(\n        integrations=[\"connectwise\"],\n        layers=[\"silver\"],\n        config={},\n        table_mappings=table_mappings\n    )\nelse:\n    print(\"\\n‚ùå No bronze tables found. Check schema configuration.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Check silver tables\nsilver_tables = spark.sql(\"SHOW TABLES LIKE 'silver_cw_*'\")\nsilver_tables.show()\n\n# Check row counts and sample data\nprint(\"\\nSilver Table Summary:\")\nprint(\"=\" * 50)\nfor row in silver_tables.collect():\n    table_name = row.tableName\n    df = spark.table(table_name)\n    count = df.count()\n    print(f\"\\n{table_name}: {count:,} rows\")\n    print(f\"Columns: {len(df.columns)}\")\n    print(f\"Sample columns: {', '.join(df.columns[:5])}...\")\n    \n    # Show sample data\n    print(\"\\nSample data:\")\n    df.show(5, truncate=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nüìä Silver Layer Processing Summary\")\nprint(\"=\" * 50)\n\n# Get all tables and their sizes\nall_tables = spark.sql(\"SHOW TABLES\")\ntable_sizes = {}\n\nfor row in all_tables.collect():\n    table = row.tableName\n    if table.startswith(('bronze_cw_', 'silver_cw_')):\n        try:\n            count = spark.table(table).count()\n            table_sizes[table] = count\n        except:\n            pass\n\n# Group by layer\nbronze = {k: v for k, v in table_sizes.items() if k.startswith('bronze_cw_')}\nsilver = {k: v for k, v in table_sizes.items() if k.startswith('silver_cw_')}\n\nprint(f\"Bronze Tables: {len(bronze)} tables, {sum(bronze.values()):,} total rows\")\nprint(f\"Silver Tables: {len(silver)} tables, {sum(silver.values()):,} total rows\")\n\nprint(\"\\n‚úÖ Silver processing completed successfully!\")\nprint(\"\\nKey improvements in this version:\")\nprint(\"- No collect() or row-by-row processing\")\nprint(\"- Distributed Spark operations throughout\")\nprint(\"- Scalable to millions of rows\")\nprint(\"\\nNext step: Run Gold layer processing in a separate notebook\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}