[project]
name = "fabric-api"
version = "0.4.0"
description = "ConnectWise PSA to Microsoft Fabric OneLake integration."
authors = [
    {name = "HÃ¡kon Freyr Gunnarsson", email = "hakonf@wise.is"}
]
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "requests",
    "pandas>=1.5.0",
    "pyarrow>=10.0.0",
    "python-dotenv",
    "pydantic>=2.11.4",
    "pyspark",
    "sparkdantic",
    "ruff>=0.11.9",
    "datamodel-code-generator>=0.30.1",
    "basedpyright>=1.29.1",
]

[project.optional-dependencies]
fabric = ["pyspark"]
azure = ["azure-identity", "azure-keyvault-secrets"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.uv]
managed = true

[tool.ruff]
target-version = "py311"
line-length = 100
exclude = [
    ".venv",
    "venv",
    "build",
    "dist",
    "__pycache__",
]

[tool.ruff.lint]
select = [
    "E",    # pycodestyle errors
    "W",    # pycodestyle warnings
    "F",    # pyflakes
    "I",    # isort
    "B",    # flake8-bugbear
    "C4",   # flake8-comprehensions
    "UP",   # pyupgrade
    "N",    # naming conventions
    "RUF",  # ruff-specific rules
]
ignore = [
    "E501",  # line too long, handled by formatter
]

[tool.ruff.lint.per-file-ignores]
"tests/**/*.py" = ["E402", "I001", "F821", "W293", "B018", "W291", "W292"]  # Ignore issues in test files
"fabric_api/connectwise_models/**/*.py" = ["N815"]  # Ignore variable naming in auto-generated models
"backup/connectwise_models/**/*.py" = ["N815"]  # Ignore variable naming in backup models
"fabric_api/client.py" = ["E402", "RUF001", "W293"]  # Imports after docstring, special characters, blank lines
"fabric_api/**/__init__.py" = ["RUF022", "E402"]  # Allow non-sorted __all__ for more logical grouping
"fabric_api/core/spark_utils.py" = ["W293", "F841"]  # Blank lines in docstrings, unused variable
"fabric_api/extract/_common.py" = ["E402"]  # Imports after docstring
"notebook-test.py" = ["E402", "F821", "W291", "B018"]  # Notebook format issues
"bronze_silver_example.py" = ["E402", "F821", "W291", "W292"]  # Example notebook issues

[tool.ruff.lint.isort]
case-sensitive = true
combine-as-imports = true

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
line-ending = "auto"

[tool.basedpyright]
pythonVersion = "3.11"
pythonPlatform = "Linux"
typeCheckingMode = "basic"
venvPath = "."
venv = ".venv"
include = ["fabric_api"]
exclude = [
    "**/.venv/**",
    "**/__pycache__/**",
    "**/node_modules/**",
    "**/.git/**",
    "**/dist/**",
    "**/build/**",
]
enableTypeIgnoreComments = true
useLibraryCodeForTypes = true
reportMissingTypeStubs = "none"

[tool.datamodel-codegen]
target-python-version = "3.11"
base-class = "sparkdantic.SparkModel"
input-file-type = "openapi"
output-model-type = "pydantic_v2.BaseModel"
use-standard-collections = true
use-schema-description = true
use-field-description = true
field-include-all-keys = true
field-constraints = true
strict-nullable = true
collapse-root-models = true
validate-configuration = true
use-model-config = true
capitalize-enum-members = true
use-union-operator = true

# Field handling settings
snake-case-field = false
aliased-fields = false
use-annotated = false
strip-default-none = false

# Output formatting settings
output-model-sorted = true
wrap-string-literal = false
reuse-model = true

# Enums handling
reuse-enum = true
enum-field-as-literal = "all"
use-subclass-enum = true

# Regular class generation settings
class-name = "JsonSchema"
