[project]
name = "unified-etl"
version = "0.1.0"
description = "Unified ETL framework for JSON-based sources to Microsoft Fabric OneLake"
authors = [
    {name = "HÃ¡kon Freyr Gunnarsson", email = "hakonf@wise.is"}
]
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "pydantic>=2.11.4",
    "sparkdantic",
    "requests>=2.28.0",
    "pandas>=1.5.0",
    "pyarrow>=10.0.0",
    "pyyaml>=6.0.0",
    "datamodel-code-generator>=0.30.1",
    "python-dotenv>=1.0.0",
    "basedpyright>=1.29.1",
    "ruff>=0.11.9",
]

[project.optional-dependencies]
spark = ["pyspark>=3.3.0", "delta-spark>=2.2.0"]
observability = ["logfire>=0.6.0"]
azure = ["azure-identity", "azure-keyvault-secrets"]
dev = [
    "pytest>=7.0.0",
    "ruff>=0.11.9",
    "basedpyright>=1.29.1",
    "types-pyyaml",
    "types-requests",
]
local = [
    "pyspark>=3.3.0",
 "delta-spark>=2.2.0",
 "logfire>=0.6.0",
]
minimal = []

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.uv]
managed = true

[tool.ruff]
target-version = "py311"
line-length = 100
exclude = [
    ".venv",
    "venv",
    "build",
    "dist",
    "__pycache__",
    "unified_etl/models/",  # Exclude generated models
]

[tool.ruff.lint]
select = [
    "E",    # pycodestyle errors
    "W",    # pycodestyle warnings
    "F",    # pyflakes
    "I",    # isort
    "B",    # flake8-bugbear
    "C4",   # flake8-comprehensions
    "UP",   # pyupgrade
    "N",    # naming conventions
    "RUF",  # ruff-specific rules
    "PT",   # pytest
    "SIM",  # flake8-simplify
]
ignore = [
    "E501",  # line too long, handled by formatter
]

[tool.ruff.lint.per-file-ignores]
"tests/**/*.py" = ["E402", "I001", "F821", "W293", "B018", "W291", "W292"]
"unified_etl/models/**/*.py" = ["N815", "N802", "N806", "N813"]  # Generated models may have naming issues
"unified_etl/generators/**/*.py" = ["E402"]
"unified_etl/**/*.py" = ["N802", "N806", "N813"]  # Allow camelCase function and variable names

[tool.ruff.lint.isort]
case-sensitive = true
combine-as-imports = true

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
line-ending = "auto"

[tool.basedpyright]
pythonVersion = "3.11"
pythonPlatform = "Linux"
typeCheckingMode = "basic"
venvPath = "."
venv = ".venv"
include = ["unified_etl"]
exclude = [
    "**/.venv/**",
    "**/__pycache__/**",
    "**/node_modules/**",
    "**/.git/**",
    "**/dist/**",
    "**/build/**",
    "unified_etl/models/**",  # Exclude generated models from type checking
]
enableTypeIgnoreComments = true
useLibraryCodeForTypes = true
reportMissingTypeStubs = "none"
reportUnusedImport = "warning"
reportPrivateUsage = false
reportOptionalMemberAccess = false
reportPrivateImportUsage = false
reportUnsupportedDunderAll = false
reportConstantRedefinition = false
reportUnknownLambdaType = false
reportPossiblyUnboundVariable = false
reportCallIssue = false
reportUnknownMemberType = "none"
reportUnknownArgumentType = "none"
reportUnknownVariableType = "none"
reportAttributeAccessIssue = "none"
reportOptionalCall = "warning"

[tool.datamodel-codegen]
target-python-version = "3.11"
base-class = "sparkdantic.SparkModel"
input-file-type = "jsonschema"
output-model-type = "pydantic_v2.BaseModel"
use-standard-collections = true
use-schema-description = true
use-field-description = true
field-include-all-keys = true
field-constraints = true
strict-nullable = true
collapse-root-models = true
validate-configuration = true
use-model-config = true
capitalize-enum-members = true
use-union-operator = true

# Field handling settings - preserve camelCase
snake-case-field = false  # Preserve camelCase field names
aliased-fields = true
use-annotated = false
strip-default-none = false

# Output formatting settings
output-model-sorted = true
wrap-string-literal = false
reuse-model = true

# Enums handling
reuse-enum = true
enum-field-as-literal = "all"
use-subclass-enum = true

# Regular class generation settings
class-name = "JsonSchema"

[dependency-groups]
dev = [
    "pytest>=8.3.5",
]
