{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConnectWise PSA to Microsoft Fabric Integration\n",
    "\n",
    "This notebook demonstrates how to use the fabric_api package to extract data from ConnectWise PSA and load it into Microsoft Fabric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# First cell - Install the package\n",
    "%pip install /lakehouse/default/Files/dist/fabric_api-0.2.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure environment and logging\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Set credentials (replace with your actual credentials)\n",
    "os.environ[\"CW_AUTH_USERNAME\"] = \"thekking+yemGyHDPdJ1hpuqx\"  # Replace with your username\n",
    "os.environ[\"CW_AUTH_PASSWORD\"] = \"yMqpe26Jcu55FbQk\"  # Replace with your password\n",
    "os.environ[\"CW_CLIENTID\"] = \"c7ea92d2-eaf5-4bfb-a09c-58d7f9dd7b81\"  # Replace with your client ID\n",
    "\n",
    "# Optional: Set other environment variables\n",
    "os.environ[\"FABRIC_STORAGE_ACCOUNT\"] = \"onelake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize PySpark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get active Spark session in Fabric\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Display Spark version for verification\n",
    "print(f\"Using Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the package - test if imports work correctly\n",
    "from fabric_api.client import ConnectWiseClient\n",
    "from fabric_api.connectwise_models import (\n",
    "    Agreement,\n",
    "    TimeEntry,\n",
    "    ExpenseEntry,\n",
    "    Invoice,\n",
    "    UnpostedInvoice,\n",
    "    ProductItem\n",
    ")\n",
    "\n",
    "# Test client connection\n",
    "client = ConnectWiseClient()\n",
    "\n",
    "# Try a simple API call to check connectivity\n",
    "response = client.get(\"/system/info\")\n",
    "print(f\"API connection status: {response.status_code}\")\n",
    "print(\"API response:\")\n",
    "import json\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 1: Run daily ETL with default settings\n",
    "from fabric_api.pipeline import run_daily_etl\n",
    "\n",
    "# Define lakehouse path\n",
    "lakehouse_root = \"/lakehouse/default/Tables/connectwise\"\n",
    "\n",
    "# Run ETL for last 30 days (default)\n",
    "print(\"Running daily ETL process with default settings (last 30 days)...\")\n",
    "result = run_daily_etl(\n",
    "    lakehouse_root=lakehouse_root,  # Where to store the data\n",
    "    max_pages=1,  # Limit to 1 page for testing\n",
    "    mode=\"overwrite\"  # Use 'overwrite' for testing, 'append' for production\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDaily ETL Results:\")\n",
    "for entity_name, (record_count, error_count) in result.items():\n",
    "    print(f\"{entity_name}: {record_count} records, {error_count} errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 2: Process specific entities\n",
    "from fabric_api.pipeline import process_entity\n",
    "\n",
    "# Define parameters\n",
    "entity_name = \"TimeEntry\"  # Choose from: Agreement, TimeEntry, ExpenseEntry, Invoice, UnpostedInvoice, ProductItem\n",
    "start_date = (datetime.now() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "table_path = f\"{lakehouse_root}/{entity_name.lower()}\"\n",
    "\n",
    "print(f\"Processing {entity_name} from {start_date} to {end_date}...\")\n",
    "\n",
    "# Process the entity\n",
    "result_df, errors = process_entity(\n",
    "    entity_name=entity_name,\n",
    "    table_path=table_path,\n",
    "    spark=spark,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    page_size=100,\n",
    "    max_pages=1,  # Limit for testing\n",
    "    write_mode=\"overwrite\"  # Use 'overwrite' for testing, 'append' for production\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(f\"Processed {result_df.count()} {entity_name} records with {len(errors)} errors\")\n",
    "if result_df.count() > 0:\n",
    "    print(\"\\nSample data:\")\n",
    "    result_df.limit(5).show()\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nSample errors (showing first 3):\")\n",
    "    for error in errors[:3]:\n",
    "        print(f\"- {error.error_type}: {error.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 3: Low-level API using extract functions\n",
    "from fabric_api.extract.time import fetch_time_entries_raw\n",
    "from fabric_api.connectwise_models import TimeEntry\n",
    "\n",
    "# Fetch raw time entries (using client we already created)\n",
    "print(\"Fetching raw time entries...\")\n",
    "raw_entries = fetch_time_entries_raw(\n",
    "    client=client,\n",
    "    page_size=10,\n",
    "    max_pages=1,  # Limit for testing\n",
    "    conditions=f\"dateStart >= [{start_date}] and dateStart <= [{end_date}]\"\n",
    ")\n",
    "\n",
    "print(f\"Fetched {len(raw_entries)} raw time entries\")\n",
    "\n",
    "# Validate using models\n",
    "valid_entries = []\n",
    "validation_errors = []\n",
    "\n",
    "for entry in raw_entries:\n",
    "    try:\n",
    "        # Validate using Pydantic model\n",
    "        validated = TimeEntry.model_validate(entry)\n",
    "        valid_entries.append(validated)\n",
    "    except Exception as e:\n",
    "        validation_errors.append({\"id\": entry.get(\"id\"), \"error\": str(e)})\n",
    "        \n",
    "print(f\"Validated {len(valid_entries)} entries with {len(validation_errors)} errors\")\n",
    "\n",
    "# Display first entry if available\n",
    "if valid_entries:\n",
    "    print(\"\\nSample validated entry:\")\n",
    "    print(valid_entries[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify data was written to Delta tables\n",
    "from fabric_api.storage.delta import read_delta_table, table_exists\n",
    "\n",
    "# Check the tables that were created\n",
    "entity_types = [\"agreement\", \"timeentry\", \"expenseentry\", \"invoice\", \"unpostedinvoice\", \"productitem\"]\n",
    "\n",
    "print(\"Checking Delta tables:\")\n",
    "for entity in entity_types:\n",
    "    path = f\"{lakehouse_root}/{entity}\"\n",
    "    exists = table_exists(spark, path)\n",
    "    print(f\"- {entity}: {'✅ Exists' if exists else '❌ Not found'}\") \n",
    "    \n",
    "    if exists:\n",
    "        # Read the table\n",
    "        df = read_delta_table(spark, path)\n",
    "        count = df.count()\n",
    "        print(f\"  Records: {count}\")\n",
    "        \n",
    "        # Show schema\n",
    "        print(\"  Schema:\")\n",
    "        df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}