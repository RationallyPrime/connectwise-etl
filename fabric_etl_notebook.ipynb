{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConnectWise to Microsoft Fabric ETL Notebook\n",
    "\n",
    "This notebook demonstrates how to use the ConnectWise PSA to Microsoft Fabric OneLake integration pipeline. The code uses the modernized ETL architecture that provides direct Delta writes to OneLake with proper table naming and partitioning strategies.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the package and any dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (adjust path as needed)\n",
    "%pip install /lakehouse/Files/dist/fabric_api-0.1.0-py3-none-any.whl\n",
    "\n",
    "# Optional: Install any additional dependencies\n",
    "%pip install delta-spark sparkdantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables\n",
    "\n",
    "Configure the environment variables for API access and logging:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n# Set up environment variables directly in the notebook for testing\n# In production, these should come from Fabric Key Vault\nos.environ[\"CW_COMPANY\"] = \"your_company_name\"  # Replace with actual value\nos.environ[\"CW_PUBLIC_KEY\"] = \"your_public_key\"  # Replace with actual value\nos.environ[\"CW_PRIVATE_KEY\"] = \"your_private_key\"  # Replace with actual value\nos.environ[\"CW_CLIENTID\"] = \"your_client_id\"  # Replace with actual value\n\n# Set Fabric storage environment variables - these help with path resolution\nos.environ[\"FABRIC_STORAGE_ACCOUNT\"] = \"your_fabric_storage\"  # Replace with your storage account name\nos.environ[\"FABRIC_TENANT_ID\"] = \"your_tenant_id\"  # Optional, can be obtained from workspace settings\n\n# Verify all required variables are available\nrequired_vars = [\n    \"CW_COMPANY\", \n    \"CW_PUBLIC_KEY\",\n    \"CW_PRIVATE_KEY\",\n    \"CW_CLIENTID\"\n]\n\n# Verify all required variables are available\nmissing_vars = [var for var in required_vars if not os.getenv(var)]\nif missing_vars:\n    raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}. \"\n                     f\"Please add these secrets to your workspace Key Vault or set them directly in the notebook.\")\n\nprint(\"Environment variables configured successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ETL Execution\n",
    "\n",
    "Run a full ETL process to extract all entities and load them to OneLake:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom fabric_api.bronze_loader import process_all_entities, process_entities\nfrom fabric_api.client import ConnectWiseClient\n\n# Get or create SparkSession\nspark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n\n# Set the Bronze layer path in OneLake\n# This will be automatically formatted for Fabric using ensure_fabric_path()\nbronze_path = \"/lakehouse/default/Tables/psa_bronze\"\n\n# Process all configured entities\nresults = process_all_entities(\n    spark=spark,\n    bronze_path=bronze_path,\n    page_size=100,\n    max_pages=10,  # Limit for testing, set to None for all pages in production\n    write_mode=\"append\"\n)\n\n# Display the results\nprint(\"\\nETL Results:\")\nfor entity_name, (df, errors) in results.items():\n    print(f\"  {entity_name}: {df.count()} records, {len(errors)} validation errors\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental ETL with Date Filtering\n",
    "\n",
    "Run an incremental ETL process to extract data for a specific date range:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from datetime import datetime, timedelta\nimport fabric_api.api_utils as api_utils\n\n# Set date range for incremental load\nend_date = datetime.now().strftime(\"%Y-%m-%d\")\nstart_date = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n\n# Build date conditions for each entity type\nconditions = {\n    \"TimeEntry\": api_utils.build_condition_string(date_gte=start_date, date_lte=end_date),\n    \"ExpenseEntry\": api_utils.build_condition_string(date_gte=start_date, date_lte=end_date),\n    \"PostedInvoice\": api_utils.build_condition_string(date_gte=start_date, date_lte=end_date),\n    \"UnpostedInvoice\": api_utils.build_condition_string(date_gte=start_date, date_lte=end_date)\n}\n\n# Process specific entities with date filtering\nincremental_results = process_entities(\n    spark=spark,\n    entity_names=[\"TimeEntry\", \"ExpenseEntry\", \"PostedInvoice\", \"UnpostedInvoice\"],\n    bronze_path=bronze_path,\n    page_size=100,\n    max_pages=10,  # Limit for testing\n    conditions=conditions,  # Apply our date filters\n    write_mode=\"append\"    # Use append mode for incremental loading\n)\n\n# Display the results\nprint(\"\\nIncremental ETL Results:\")\nfor entity_name, (df, errors) in incremental_results.items():\n    print(f\"  {entity_name}: {df.count()} records, {len(errors)} validation errors\")\n    \n    # Show validation error summary if errors exist\n    if errors:\n        from fabric_api import log_utils as log\n        error_summary = log.summarize_validation_errors(errors)\n        print(f\"  Error types: {error_summary.get('fields', [])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Processing Specific Entities\n",
    "\n",
    "If you need more control over the ETL process, you can use the lower-level APIs:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Process specific entities with custom configuration\nfrom fabric_api.extract import fetch_agreements_raw, fetch_active_agreements\nfrom fabric_api.extract._common import validate_batch\nfrom fabric_api.connectwise_models import Agreement\nfrom fabric_api.client import ConnectWiseClient\n\n# Create a ConnectWise client \nclient = ConnectWiseClient()\n\n# Example of manually fetching and processing data\nprint(\"\\nManual processing example - Active Agreements:\")\n\n# 1. Fetch raw data directly\nraw_agreements = fetch_active_agreements(client, max_pages=5)\nprint(f\"  Fetched {len(raw_agreements)} active agreements\")\n\n# 2. Validate against schema\nvalid_agreements, errors = validate_batch(raw_agreements, Agreement)\nprint(f\"  Validation: {len(valid_agreements)} valid, {len(errors)} invalid\")\n\n# 3. Convert to DataFrame (optional)\nif valid_agreements:\n    from fabric_api.bronze_loader import create_dataframe\n    df = create_dataframe(spark, valid_agreements, Agreement)\n    \n    print(\"\\nSample data:\")\n    df.select(\"id\", \"name\", \"type\").show(5, truncate=False)\n    \n    # 4. Write to custom location if needed\n    custom_path = \"/lakehouse/default/Tables/psa_bronze/cw_active_agreements\"\n    \n    from fabric_api.bronze_loader import write_to_delta, add_fabric_metadata, register_table_metadata\n    \n    # Add metadata\n    df = add_fabric_metadata(df, \"ActiveAgreement\", \"ConnectWise active agreements only\")\n    \n    # Write to Delta\n    write_to_delta(\n        df=df,\n        table_path=custom_path,\n        partition_cols=[\"type\"],\n        mode=\"overwrite\"\n    )\n    \n    # Register table\n    register_table_metadata(\n        spark=spark, \n        table_path=custom_path,\n        entity_name=\"ActiveAgreement\",\n        description=\"ConnectWise active agreements only\"\n    )\n    \n    print(f\"\\nWrote {df.count()} active agreements to {custom_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Loaded Data\n",
    "\n",
    "Once the data is loaded, you can query it using Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Query the loaded data using Spark SQL\nfrom fabric_api.bronze_loader import ENTITY_CONFIG\nimport os\n\n# Get the table names from our configuration\nagreement_table = os.path.basename(ENTITY_CONFIG[\"Agreement\"][\"output_table\"])\nposted_invoice_table = os.path.basename(ENTITY_CONFIG[\"PostedInvoice\"][\"output_table\"])\ntime_table = os.path.basename(ENTITY_CONFIG[\"TimeEntry\"][\"output_table\"])\n\nprint(f\"Querying tables: {agreement_table}, {posted_invoice_table}, {time_table}\")\n\n# Query agreements\nagreement_df = spark.sql(f\"\"\"\nSELECT id, name, type, agreementType, billingCycle, startDate, endDate\nFROM {agreement_table}\nWHERE startDate IS NOT NULL\nORDER BY startDate DESC\nLIMIT 10\n\"\"\")\n\n# Display the results\nprint(\"\\nSample Agreements:\")\nagreement_df.show(truncate=False)\n\n# Example of using the etl_timestamp metadata column\nmetadata_df = spark.sql(f\"\"\"\nSELECT \n    COUNT(*) as count,\n    entity_name,\n    DATE(etl_timestamp) as load_date\nFROM {agreement_table}\nGROUP BY entity_name, DATE(etl_timestamp)\nORDER BY load_date DESC\n\"\"\")\n\nprint(\"\\nData Load Summary:\")\nmetadata_df.show()\n\n# Example complex query joining multiple tables\nprint(\"\\nAttempting to run a join query - this will only work if all tables exist:\")\ntry:\n    joined_df = spark.sql(f\"\"\"\n    SELECT \n        i.id as invoice_id,\n        i.identifier as invoice_number,\n        i.total as invoice_total,\n        t.id as time_entry_id,\n        t.hours as hours_worked,\n        t.timeStart as work_date,\n        t.billableOption as billing_option\n    FROM {posted_invoice_table} i\n    JOIN {time_table} t ON t.invoice_id = i.id\n    WHERE i.total > 0\n    ORDER BY i.date DESC\n    LIMIT 10\n    \"\"\")\n    \n    print(\"Sample Joined Data:\")\n    joined_df.show(truncate=False)\nexcept Exception as e:\n    print(f\"Join query failed (this is normal if you haven't loaded all tables): {str(e)}\")\n\n# Example of querying the validation errors table\ntry:\n    validation_errors_df = spark.sql(\"\"\"\n    SELECT \n        entity,\n        COUNT(*) as error_count,\n        COLLECT_SET(error_type) as error_types\n    FROM cw_validation_errors\n    GROUP BY entity\n    \"\"\")\n    \n    print(\"\\nValidation Error Summary:\")\n    validation_errors_df.show(truncate=False)\nexcept Exception as e:\n    print(f\"Validation errors query failed (this is normal if no errors exist): {str(e)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Fabric-Native Features\n\nThe modernized pipeline includes several Fabric-specific optimizations:\n\n1. **Standardized Naming Convention**: All tables use a consistent `cw_` prefix (e.g., `cw_agreement`, `cw_time_entry`, etc.)\n\n2. **Entity-Specific Partitioning**:\n   - Time-related tables (`cw_time_entry`, `cw_expense_entry`, `cw_posted_invoice`) - Partitioned by date\n   - Agreement tables (`cw_agreement`) - Partitioned by type\n   - Error tables (`cw_validation_errors`) - Partitioned by entity\n\n3. **Rich Metadata**: All tables include standardized metadata columns:\n   - `etl_entity_name`: Source system name (\"ConnectWise\")\n   - `etl_entity_type`: Entity type (e.g., \"Agreement\", \"TimeEntry\")\n   - `etl_timestamp`: When the data was loaded\n   - `etl_version`: Pipeline version\n\n4. **Delta Optimizations**:\n   - `delta.autoOptimize.optimizeWrite` - Optimizes write performance\n   - `delta.autoOptimize.autoCompact` - Auto-compacts small files\n   - `mergeSchema` - Supports schema evolution\n\n5. **Automatic Table Registration**: Tables are automatically registered in the Fabric catalog for easy discovery\n\n6. **ABFSS Path Construction**: Direct writes to OneLake using proper ABFSS URLs when in Fabric\n\n7. **Comprehensive Validation**: Invalid records are tracked in a dedicated `cw_validation_errors` table\n\nThese features ensure that data is immediately available for analysis in Fabric without any post-processing steps.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}