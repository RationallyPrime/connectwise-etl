{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConnectWise to Microsoft Fabric ETL Notebook\n",
    "\n",
    "This notebook demonstrates how to use the ConnectWise PSA to Microsoft Fabric OneLake integration pipeline. The code uses the modernized ETL architecture that provides direct Delta writes to OneLake with proper table naming and partitioning strategies.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the package and any dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (adjust path as needed)\n",
    "%pip install /lakehouse/Files/dist/fabric_api-0.1.0-py3-none-any.whl\n",
    "\n",
    "# Optional: Install any additional dependencies\n",
    "%pip install delta-spark sparkdantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables\n",
    "\n",
    "Configure the environment variables for API access and logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Set up environment variables from Fabric Key Vault\n",
    "# These will be available automatically if you've configured secrets in your workspace Key Vault\n",
    "required_vars = [\n",
    "    \"CW_COMPANY\", \n",
    "    \"CW_PUBLIC_KEY\",\n",
    "    \"CW_PRIVATE_KEY\",\n",
    "    \"CW_CLIENTID\"\n",
    "]\n",
    "\n",
    "# Verify all required variables are available\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}. \"\n",
    "                     f\"Please add these secrets to your workspace Key Vault.\")\n",
    "\n",
    "print(\"Environment variables configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ETL Execution\n",
    "\n",
    "Run a full ETL process to extract all entities and load them to OneLake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric_api.orchestration import run_onelake_etl\n",
    "\n",
    "# Run a full ETL process (no date filtering)\n",
    "table_paths = run_onelake_etl(\n",
    "    mode=\"append\",      # Use append or overwrite\n",
    "    max_pages=100       # Limit the number of pages for testing\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nETL Results:\")\n",
    "for entity_name, path in table_paths.items():\n",
    "    print(f\"  {entity_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental ETL with Date Filtering\n",
    "\n",
    "Run an incremental ETL process to extract data for a specific date range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an incremental ETL process with date filtering\n",
    "incremental_results = run_onelake_etl(\n",
    "    start_date=\"2025-04-01\",  # Start date in YYYY-MM-DD format\n",
    "    end_date=\"2025-04-30\",    # End date in YYYY-MM-DD format\n",
    "    mode=\"append\",            # Always use append for incremental updates\n",
    "    max_pages=100             # Limit the number of pages for testing\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nIncremental ETL Results:\")\n",
    "for entity_name, path in incremental_results.items():\n",
    "    print(f\"  {entity_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Processing Specific Entities\n",
    "\n",
    "If you need more control over the ETL process, you can use the lower-level APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric_api.orchestration import ETLOrchestrator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the active Spark session in Fabric\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create an ETL orchestrator with OneLake optimizations\n",
    "orchestrator = ETLOrchestrator(\n",
    "    spark=spark,\n",
    "    write_mode=\"append\",\n",
    "    use_onelake=True\n",
    ")\n",
    "\n",
    "# Process specific entities\n",
    "results = orchestrator.process_entities(\n",
    "    entity_names=[\"Agreement\", \"TimeEntry\"],  # Only process these entities\n",
    "    max_pages=50                              # Limit the number of pages\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nSpecific Entity ETL Results:\")\n",
    "for entity_name, (_, row_count, error_count) in results.items():\n",
    "    print(f\"  {entity_name}: {row_count} rows written, {error_count} validation errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Loaded Data\n",
    "\n",
    "Once the data is loaded, you can query it using Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data using Spark SQL\n",
    "from fabric_api.onelake_utils import get_table_name\n",
    "\n",
    "# Get fully qualified table names\n",
    "agreement_table = get_table_name(\"Agreement\")\n",
    "invoice_table = get_table_name(\"PostedInvoice\")\n",
    "time_table = get_table_name(\"TimeEntry\")\n",
    "\n",
    "# Query agreements\n",
    "agreement_df = spark.sql(f\"\"\"\n",
    "SELECT id, name, type, agreementType, startDate, endDate\n",
    "FROM {agreement_table}\n",
    "ORDER BY startDate DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Sample Agreements:\")\n",
    "agreement_df.show(truncate=False)\n",
    "\n",
    "# Join invoices and time entries\n",
    "joined_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    i.invoice_number,\n",
    "    i.date as invoice_date,\n",
    "    i.total as invoice_total,\n",
    "    t.time_entry_id,\n",
    "    t.hours_worked,\n",
    "    t.work_date,\n",
    "    t.work_role_id,\n",
    "    t.work_type\n",
    "FROM {invoice_table} i\n",
    "JOIN {time_table} t ON i.invoice_number = t.invoice_number\n",
    "ORDER BY i.date DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Sample Joined Data:\")\n",
    "joined_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that the data is loaded into OneLake Delta tables, you can:\n",
    "\n",
    "1. Create Power BI reports directly from the tables\n",
    "2. Set up scheduled refreshes using Fabric Data Pipelines\n",
    "3. Create derived tables with additional business logic\n",
    "4. Export the data to other systems as needed\n",
    "\n",
    "The modular design of the ETL process allows for easy extensibility and maintenance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}