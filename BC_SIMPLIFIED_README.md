# Business Central ETL - Simplified Architecture

## Overview

This simplified Business Central ETL pipeline removes the complexity of YAML configuration files and leverages Microsoft Fabric's native Spark capabilities. The pipeline uses autogenerated Pydantic models from CDM schemas and provides a streamlined approach to medallion architecture.

## Key Components

### 1. BC Extractor (`fabric_api/bc_extractor.py`)
- Extracts data from Business Central API
- Validates using autogenerated Pydantic models
- Loads directly to Bronze layer in Delta format
- No YAML configuration needed

### 2. BC Medallion Pipeline (`fabric_api/bc_medallion_pipeline.py`)
- Processes data through Bronze → Silver → Gold layers
- Generates surrogate keys for dimensions
- Creates dimension bridge for BC's dimensional framework
- Handles fact tables with proper dimension joins
- Includes date dimension generation

### 3. Autogenerated Models (`fabric_api/bc_models/models.py`)
- Pydantic models generated from CDM JSON schemas
- Provides schema validation and Spark compatibility
- Single source of truth for table structures

## Installation

```bash
# Install in Microsoft Fabric notebook
%pip install /lakehouse/default/Files/dist/fabric_api-0.4.0-py3-none-any.whl
```

## Usage Example

```python
from pyspark.sql import SparkSession
from fabric_api.bc_extractor import BCExtractor
from fabric_api.bc_medallion_pipeline import BCMedallionPipeline

# Initialize components
spark = spark  # Already available in Fabric notebooks

# Extract data from Business Central
extractor = BCExtractor(
    spark=spark,
    company_id="YOUR_COMPANY_ID",
    environment="PRODUCTION"
)

# Extract specific entities
extractor.extract_all_entities(
    bronze_path="bronze",
    start_date="2024-01-01",
    end_date="2024-12-31",
    specific_entities=[
        "generalLedgerAccounts",
        "generalLedgerEntries",
        "customers",
        "vendors",
        "dimensions",
        "dimensionValues",
        "dimensionSetEntries"
    ]
)

# Run medallion pipeline
pipeline = BCMedallionPipeline(spark)

pipeline.run_pipeline(
    bronze_path="bronze",
    silver_path="silver",
    gold_path="gold",
    tables=[
        "GLAccount", "GLEntry", "Customer", "Vendor",
        "Dimension", "DimensionValue", "DimensionSetEntry"
    ],
    incremental=False,
    min_year=2020,
    dimension_types={
        "TEAM": "TEAM",
        "PRODUCT": "PRODUCT",
        "DEPARTMENT": "DEPT",
        "PROJECT": "PROJ"
    }
)
```

## Key Features

### 1. Dimension Bridge
The pipeline automatically creates a dimension bridge table that maps dimension sets to dimension values. This is critical for BC data models that use the dimensional framework.

```python
# Dimension bridge is created automatically when these tables are processed:
# - DimensionSetEntry
# - DimensionValue

# The bridge includes:
# - DimensionBridgeKey (surrogate key)
# - DimensionSetID
# - Dimension codes and names for each type
# - Flags indicating which dimensions are present
```

### 2. Surrogate Key Generation
All dimension tables get surrogate keys automatically:
- Uses ROW_NUMBER() window function
- Partitioned by company for multi-company setups
- Consistent naming pattern: SK_{TableName}

### 3. Date Dimension
Comprehensive date dimension with:
- Calendar hierarchy (Year, Quarter, Month, Week, Day)
- Fiscal year calculations
- Date keys for easy joins

### 4. Fact Table Processing
Fact tables are enriched with:
- Date dimension joins (DateKey, FiscalYear, etc.)
- Dimension bridge joins (for analytical dimensions)
- Related dimension joins (Customer, Vendor, GL Account, etc.)
- Calculated measures (AmountSign, etc.)

## Benefits Over YAML-Based Approach

1. **Simplicity**: No complex YAML files to maintain
2. **Type Safety**: Pydantic models provide compile-time validation
3. **Performance**: Leverages Fabric's optimized Spark environment
4. **Maintainability**: Code-based approach is easier to debug
5. **Flexibility**: Easy to extend or customize

## Architecture Flow

```
1. Extract (BCExtractor)
   BC API → Validate → Bronze Layer
   
2. Bronze to Silver (BCMedallionPipeline)
   Bronze → Standardize → Deduplicate → Silver Layer
   
3. Silver to Gold - Dimensions
   Silver → Add Surrogate Keys → Gold Dimensions
   
4. Silver to Gold - Dimension Bridge
   DimensionSetEntry + DimensionValue → Dimension Bridge
   
5. Silver to Gold - Facts  
   Silver → Join Dimensions → Join Bridge → Calculate Measures → Gold Facts
```

## Incremental Processing

The pipeline supports incremental processing:
- Watermark-based approach for changed data
- Metadata tracking for pipeline runs
- Configurable lookback periods

```python
# Enable incremental processing
pipeline.run_pipeline(
    bronze_path="bronze",
    silver_path="silver",
    gold_path="gold",
    incremental=True
)
```

## Customization

### Adding New Dimension Types
```python
# Specify custom dimension types
dimension_types = {
    "TEAM": "TEAM",
    "PRODUCT": "PRODUCT", 
    "LOCATION": "LOC",
    "CUSTOMDIM": "CUSTOM"
}

pipeline.run_pipeline(
    ...,
    dimension_types=dimension_types
)
```

### Filtering Specific Tables
```python
# Process only specific tables
tables_to_process = ["GLAccount", "GLEntry", "Customer"]

pipeline.run_pipeline(
    ...,
    tables=tables_to_process
)
```

## Error Handling

The pipeline includes comprehensive error handling:
- Graceful failure for missing tables
- Detailed logging for debugging
- Continues processing even if individual tables fail

## Next Steps

1. **Scheduling**: Set up Fabric pipeline for regular execution
2. **Monitoring**: Add data quality checks and alerts
3. **Optimization**: Tune Spark settings for your data volume
4. **Extensions**: Add custom business logic as needed

## Migration from YAML-Based System

If migrating from the old YAML-based system:

1. Generate Pydantic models from your CDM schemas
2. Map YAML configurations to pipeline parameters
3. Replace YAML-based transformations with code
4. Test thoroughly with sample data

## Troubleshooting

Common issues and solutions:

1. **Missing Tables**: Ensure Bronze layer has been populated
2. **Schema Mismatches**: Regenerate models from latest CDM
3. **Performance**: Check Spark configuration and partitioning
4. **API Limits**: Implement rate limiting in extractor

## Support

For issues or questions:
- Check the logs for detailed error messages
- Review the source code for implementation details
- Ensure all dependencies are properly installed