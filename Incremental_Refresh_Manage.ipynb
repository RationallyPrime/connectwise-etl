{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e067515-2ea8-4dbe-9517-b7f2aaa5d6f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install /lakehouse/default/Files/unified_etl_core-1.0.0-py3-none-any.whl\n",
    "%pip install /lakehouse/default/Files/unified_etl_connectwise-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56de359f-0666-4ab5-aa9c-9a828ffbf327",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-22T22:39:34.4995966Z",
       "execution_start_time": "2025-06-22T22:39:34.2247565Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6743f91f-4d72-47b5-a4f1-de12a3e7e814",
       "queued_time": "2025-06-22T22:38:44.5477955Z",
       "session_id": "feb0af9a-44a1-4d6b-a202-0ddf4d764558",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": "StatementMeta(, feb0af9a-44a1-4d6b-a202-0ddf4d764558, 10, Finished, Available, Finished)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- CELL 2: Environment setup ----\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set credentials from Key Vault (in production) or environment\n",
    "os.environ[\"CW_AUTH_USERNAME\"] = \"thekking+yemGyHDPdJ1hpuqx\"\n",
    "os.environ[\"CW_AUTH_PASSWORD\"] = \"yMqpe26Jcu55FbQk\"\n",
    "os.environ[\"CW_CLIENTID\"] = \"c7ea92d2-eaf5-4bfb-a09c-58d7f9dd7b81\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b52a26-92fa-4700-9a08-9818f647323f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incremental data refresh for ConnectWise ETL - Fabric Notebook Version\n",
    "\n",
    "This code refreshes only recent data instead of re-fetching 5 years worth of records.\n",
    "Designed to run as a single cell in a Microsoft Fabric notebook.\n",
    "\n",
    "Prerequisites:\n",
    "- Run pip install commands in a separate cell first\n",
    "- Ensure Key Vault secrets are configured (CW_AUTH_USERNAME, CW_AUTH_PASSWORD, CW_CLIENTID)\n",
    "\"\"\"\n",
    "\n",
    "# Standard imports\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# ETL framework imports\n",
    "from unified_etl_connectwise import ConnectWiseClient\n",
    "from unified_etl_connectwise.api_utils import build_condition_string\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def refresh_recent_data(days_back: int = 30) -> dict[str, DataFrame]:\n",
    "    \"\"\"\n",
    "    Refresh only recent data from ConnectWise.\n",
    "\n",
    "    Strategy:\n",
    "    - TimeEntry/ExpenseEntry: Only recent entries (by dateEntered)\n",
    "    - Agreement/PostedInvoice: Only recently updated (by lastUpdated)\n",
    "    - UnpostedInvoice: ALL records (they're work in progress)\n",
    "\n",
    "    Args:\n",
    "        days_back: Number of days to look back (default 30)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping entity names to DataFrames\n",
    "    \"\"\"\n",
    "    # Initialize client\n",
    "    client = ConnectWiseClient()\n",
    "\n",
    "    # Calculate date threshold\n",
    "    since_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Refreshing data since: {since_date}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Use the existing endpoint mapping from client._get_entity_name\n",
    "    # Note: ConnectWise API endpoints:\n",
    "    # - /finance/invoices returns UnpostedInvoice entities\n",
    "    # - /finance/invoices/posted returns PostedInvoice entities\n",
    "    endpoints = {\n",
    "        \"TimeEntry\": \"/time/entries\",\n",
    "        \"Agreement\": \"/finance/agreements\",\n",
    "        \"UnpostedInvoice\": \"/finance/invoices\",  # Returns UnpostedInvoice entities\n",
    "        \"PostedInvoice\": \"/finance/invoices/posted\",  # Posted invoices\n",
    "        \"ExpenseEntry\": \"/expense/entries\",\n",
    "    }\n",
    "\n",
    "    for entity_name, endpoint in endpoints.items():\n",
    "        print(f\"\\nRefreshing {entity_name}...\")\n",
    "\n",
    "        # Build appropriate conditions based on entity type\n",
    "        if entity_name in [\"TimeEntry\", \"ExpenseEntry\"]:\n",
    "            conditions = build_condition_string(date_entered_gte=since_date)\n",
    "            order_by = \"dateEntered desc\"\n",
    "        else:  # Agreement, PostedInvoice, UnpostedInvoice\n",
    "            conditions = f\"(lastUpdated>=[{since_date}])\"\n",
    "            order_by = \"lastUpdated desc\"\n",
    "\n",
    "        try:\n",
    "            df = client.extract(\n",
    "                endpoint=endpoint, conditions=conditions, order_by=order_by, page_size=1000\n",
    "            )\n",
    "\n",
    "            # Add ETL metadata columns per CLAUDE.md philosophy\n",
    "            # Note: Legacy tables have old column names, but we should use proper naming\n",
    "            from pyspark.sql import functions as F\n",
    "\n",
    "            df = df.withColumn(\"_etl_timestamp\", F.current_timestamp())\n",
    "            df = df.withColumn(\"_etl_source\", F.lit(\"connectwise\"))\n",
    "            df = df.withColumn(\"_etl_batch_id\", F.lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "\n",
    "            # Also add legacy columns for backward compatibility with existing schema\n",
    "            df = df.withColumn(\"etl_timestamp\", F.col(\"_etl_timestamp\").cast(\"string\"))\n",
    "            df = df.withColumn(\"etl_entity\", F.lit(entity_name))\n",
    "            df = df.withColumn(\"etlTimestamp\", F.col(\"_etl_timestamp\").cast(\"string\"))\n",
    "            df = df.withColumn(\"etlEntity\", F.lit(entity_name))\n",
    "\n",
    "            results[entity_name] = df\n",
    "            print(f\"  Found {df.count()} {entity_name} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR extracting {entity_name}: {e}\")\n",
    "            results[entity_name] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def refresh_specific_date_range(start_date: str, end_date: str) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Refresh data for a specific date range.\n",
    "\n",
    "    Args:\n",
    "        start_date: Start date in YYYY-MM-DD format\n",
    "        end_date: End date in YYYY-MM-DD format\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping entity names to record counts\n",
    "    \"\"\"\n",
    "    client = ConnectWiseClient()\n",
    "    print(f\"Refreshing data from {start_date} to {end_date}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Time entries in date range\n",
    "    conditions = f\"dateEntered>=[{start_date}] AND dateEntered<=[{end_date}]\"\n",
    "    time_entries = client.extract(\n",
    "        endpoint=\"/time/entries\", conditions=conditions, order_by=\"dateEntered desc\", page_size=1000\n",
    "    )\n",
    "    results[\"time_entries\"] = time_entries.count()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def check_latest_records() -> None:\n",
    "    \"\"\"\n",
    "    Check the most recent records to see what dates we have.\n",
    "    \"\"\"\n",
    "    client = ConnectWiseClient()\n",
    "\n",
    "    # Get just the 10 most recent time entries\n",
    "    recent_time = client.paginate(\n",
    "        endpoint=\"/time/entries\",\n",
    "        entity_name=\"time_entries\",\n",
    "        fields=\"id,dateEntered,notes\",\n",
    "        order_by=\"dateEntered desc\",\n",
    "        max_pages=1,\n",
    "        page_size=10,\n",
    "    )\n",
    "\n",
    "    print(\"\\nMost recent time entries:\")\n",
    "    for entry in recent_time[:5]:\n",
    "        print(f\"  {entry['dateEntered']}: {entry.get('notes', '')[:50]}...\")\n",
    "\n",
    "    # Get most recent invoices\n",
    "    recent_invoices = client.paginate(\n",
    "        endpoint=\"/finance/invoices\",\n",
    "        entity_name=\"invoices\",\n",
    "        fields=\"id,invoiceNumber,dateCreated,lastUpdated\",\n",
    "        order_by=\"lastUpdated desc\",\n",
    "        max_pages=1,\n",
    "        page_size=10,\n",
    "    )\n",
    "\n",
    "    print(\"\\nMost recent invoices:\")\n",
    "    for inv in recent_invoices[:5]:\n",
    "        print(f\"  Invoice keys: {list(inv.keys())[:5]}...\")  # Debug line to see actual field names\n",
    "        print(\n",
    "            f\"  {inv.get('invoiceNumber', inv.get('id', 'N/A'))}: Created {inv.get('dateCreated', inv.get('date', 'N/A'))}, Updated {inv.get('lastUpdated', 'N/A')}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# FABRIC NOTEBOOK EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "# BEFORE RUNNING THIS CELL, YOU NEED TO KNOW YOUR TABLE PATHS!\n",
    "#\n",
    "# BRONZE → SILVER → GOLD TABLE MAPPING:\n",
    "#\n",
    "# Bronze Tables (from API):\n",
    "#   Agreement        → Silver: Agreement        → Gold: fact_agreement_period\n",
    "#   TimeEntry        → Silver: TimeEntry        → Gold: fact_time_entry\n",
    "#   ExpenseEntry     → Silver: ExpenseEntry     → Gold: fact_expense_entry\n",
    "#   ProductItem      → Silver: ProductItem      → (used in fact_invoice_line)\n",
    "#   PostedInvoice    → Silver: PostedInvoice    → Gold: fact_invoice_line (requires API permission)\n",
    "#   UnpostedInvoice  → Silver: UnpostedInvoice  → (may also feed fact_invoice_line)\n",
    "#\n",
    "# Gold combines multiple Silver tables:\n",
    "#   fact_time_entry: TimeEntry + Agreement + Member\n",
    "#   fact_invoice_line: PostedInvoice/UnpostedInvoice + TimeEntry + ProductItem + Agreement\n",
    "#\n",
    "# Based on your SHOW TABLES output, your tables are case-sensitive!\n",
    "\n",
    "# Configuration - UPDATE THESE BASED ON YOUR ENVIRONMENT\n",
    "LAKEHOUSE_ROOT = \"/lakehouse/default/Tables/\"  # Update if different\n",
    "DAYS_TO_REFRESH = 35  # How many days back to refresh\n",
    "\n",
    "# Option 1: Check what's the latest data we have\n",
    "print(\"=== Checking Latest Records ===\")\n",
    "check_latest_records()\n",
    "\n",
    "# Option 2: Refresh recent data (recommended for incremental updates)\n",
    "print(f\"\\n=== Refreshing Last {DAYS_TO_REFRESH} Days ===\")\n",
    "results = refresh_recent_data(DAYS_TO_REFRESH)\n",
    "print(\"\\nRefresh Summary:\")\n",
    "for entity, df in results.items():\n",
    "    if df is not None:\n",
    "        print(f\"  {entity}: {df.count()} records\")\n",
    "    else:\n",
    "        print(f\"  {entity}: ERROR\")\n",
    "\n",
    "# Option 3: Refresh specific date range (uncomment to use)\n",
    "# start_date = \"2024-11-01\"\n",
    "# end_date = \"2024-12-31\"\n",
    "# results = refresh_specific_date_range(start_date, end_date)\n",
    "\n",
    "# Option 4: Write refreshed data to Bronze tables\n",
    "# The client.extract() already returns validated Spark DataFrames with proper schema\n",
    "\n",
    "# Initialize client to get spark session\n",
    "client = ConnectWiseClient()\n",
    "spark = client.spark\n",
    "\n",
    "# Debug: Check what tables exist\n",
    "print(\"\\n=== Checking Bronze Tables ===\")\n",
    "try:\n",
    "    # List all tables that start with bronze_cw_\n",
    "    all_tables = spark.sql(\"SHOW TABLES\")\n",
    "    bronze_tables = all_tables.filter(\"tableName LIKE 'bronze_cw_%'\")\n",
    "    bronze_tables.show(100, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error listing bronze tables: {e}\")\n",
    "\n",
    "# Write the refreshed data to Bronze tables\n",
    "print(\"\\n=== Writing to Bronze Tables ===\")\n",
    "for entity_name, df in results.items():\n",
    "    if df is not None and df.count() > 0:\n",
    "        try:\n",
    "            # Use the correct table naming convention: bronze_cw_<entity> (lowercase)\n",
    "            bronze_table = f\"bronze_cw_{entity_name.lower()}\"\n",
    "\n",
    "            # The DataFrame from client.extract() is already validated\n",
    "            # Use append mode with mergeSchema to handle evolution\n",
    "            df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(bronze_table)\n",
    "\n",
    "            print(f\"  Appended {df.count()} records to {bronze_table}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR writing {entity_name}: {e}\")\n",
    "\n",
    "            # For schema merge errors, try to understand the conflict\n",
    "            if \"Failed to merge fields\" in str(e) or \"DELTA_FAILED_TO_MERGE_FIELDS\" in str(e):\n",
    "                print(\"  Schema conflict detected. Checking schema differences...\")\n",
    "                try:\n",
    "                    # Get existing table schema\n",
    "                    existing_df = spark.table(bronze_table)\n",
    "                    print(f\"\\n  Existing {entity_name} schema:\")\n",
    "                    existing_df.printSchema()\n",
    "                    print(f\"\\n  New {entity_name} schema:\")\n",
    "                    df.printSchema()\n",
    "                except Exception as schema_e:\n",
    "                    print(f\"  Could not compare schemas: {schema_e}\")\n",
    "\n",
    "            # Try to understand other errors\n",
    "            elif \"doesn't exist\" in str(e) or \"Table or view not found\" in str(e):\n",
    "                print(f\"  Table {bronze_table} doesn't exist. Creating it...\")\n",
    "                try:\n",
    "                    df.write.mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "                    print(f\"  Created {bronze_table} with {df.count()} records\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"  ERROR creating table: {e2}\")\n",
    "\n",
    "print(\"\\n=== Incremental Refresh Complete ===\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Verify the data looks correct\")\n",
    "print(\"2. Merge (don't overwrite!) the fresh data to Bronze tables\")\n",
    "print(\"3. Run Silver transformations for the refreshed entities\")\n",
    "print(\"4. Run Gold transformations (fact tables) that depend on updated Silver tables\")\n",
    "\n",
    "# CASCADE UPDATE EXAMPLE:\n",
    "# After merging to Bronze, you need to update Silver and Gold layers\n",
    "#\n",
    "# # 1. Update Silver layer for refreshed entities\n",
    "# from unified_etl_core.silver import process_silver_layer\n",
    "# from unified_etl_connectwise.config import ENTITY_CONFIGS\n",
    "#\n",
    "# # Process only the entities we refreshed\n",
    "# refreshed_entities = [name for name, df in results.items() if df is not None]\n",
    "# for entity in refreshed_entities:\n",
    "#     if entity in ENTITY_CONFIGS:\n",
    "#         config = ENTITY_CONFIGS[entity]\n",
    "#         process_silver_layer(\n",
    "#             bronze_table=config[\"bronze_table\"],\n",
    "#             silver_table=config[\"silver_table\"],\n",
    "#             # ... other config params\n",
    "#         )\n",
    "#\n",
    "# # 2. Update Gold layer (fact tables that depend on Silver)\n",
    "# from unified_etl_connectwise.transforms import (\n",
    "#     create_time_entry_fact,\n",
    "#     create_invoice_line_fact,\n",
    "#     create_expense_entry_fact,\n",
    "#     create_agreement_period_fact\n",
    "# )\n",
    "#\n",
    "# # Update fact tables based on what was refreshed\n",
    "# if \"time_entry\" in refreshed_entities or \"agreement\" in refreshed_entities:\n",
    "#     # fact_time_entry depends on silver_timeentry + silver_agreement\n",
    "#     create_time_entry_fact(spark, \"silver_\", \"gold_\")\n",
    "#\n",
    "# if \"invoice\" in refreshed_entities or \"time_entry\" in refreshed_entities:\n",
    "#     # fact_invoice_line depends on multiple silver tables\n",
    "#     create_invoice_line_fact(spark, \"silver_\", \"gold_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5b1f-c9fc-4acc-bbab-bf60d5ddc2de",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# ==============================================================================\n",
    "# CASCADE UPDATE CELL - Run this after successful Bronze refresh\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "This cell updates Silver and Gold layers after Bronze incremental refresh.\n",
    "Prerequisites:\n",
    "- Run the incremental refresh cell above first\n",
    "- Ensure 'refreshed_entities' variable contains the entities to update\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== Updating Silver Layer ===\")\n",
    "from unified_etl_connectwise import models\n",
    "from unified_etl_connectwise.config import SILVER_CONFIG\n",
    "from unified_etl_core.silver import apply_silver_transformations\n",
    "\n",
    "# Get the model classes - models is a dict with lowercase keys\n",
    "model_mapping = {\n",
    "    \"Agreement\": models.get(\"agreement\"),\n",
    "    \"TimeEntry\": models.get(\"timeentry\"),\n",
    "    \"ExpenseEntry\": models.get(\"expenseentry\"),\n",
    "    \"UnpostedInvoice\": models.get(\"invoice\"),  # Uses Invoice model\n",
    "    \"PostedInvoice\": models.get(\"invoice\"),  # Also uses Invoice model\n",
    "    \"ProductItem\": models.get(\"productitem\"),\n",
    "}\n",
    "\n",
    "# Process only entities that had new data\n",
    "refreshed_entities = [name for name, df in results.items() if df is not None and df.count() > 0]\n",
    "print(f\"Entities to update in Silver: {refreshed_entities}\")\n",
    "\n",
    "for entity_name in refreshed_entities:\n",
    "    # Check if entity has Silver config\n",
    "    if entity_name in SILVER_CONFIG[\"entities\"]:\n",
    "        entity_config = SILVER_CONFIG[\"entities\"][entity_name]\n",
    "        bronze_table = f\"bronze_cw_{entity_name.lower()}\"\n",
    "        silver_table = entity_config[\"silver_table\"]  # Use config table name\n",
    "\n",
    "        print(f\"\\nProcessing {entity_name}: {bronze_table} -> {silver_table}\")\n",
    "\n",
    "        try:\n",
    "            # Read Bronze data\n",
    "            bronze_df = spark.table(bronze_table)\n",
    "            print(f\"  Bronze records: {bronze_df.count()}\")\n",
    "\n",
    "            # Get model class\n",
    "            model_class = model_mapping.get(entity_name)\n",
    "            if not model_class:\n",
    "                print(f\"  WARNING: No model class found for {entity_name}\")\n",
    "                continue\n",
    "\n",
    "            # Apply Silver transformations - use entity_config from SILVER_CONFIG\n",
    "            silver_df = apply_silver_transformations(\n",
    "                df=bronze_df,\n",
    "                entity_config=entity_config,  # Pass the full config\n",
    "                model_class=model_class,\n",
    "            )\n",
    "\n",
    "            # Write to Silver table\n",
    "            silver_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "                silver_table\n",
    "            )\n",
    "            print(f\"  Updated {silver_table} with {silver_df.count()} records\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {entity_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n{entity_name} not found in SILVER_CONFIG, skipping Silver update\")\n",
    "\n",
    "# CASCADE UPDATE - GOLD LAYER\n",
    "print(\"\\n\\n=== Updating Gold Layer ===\")\n",
    "from unified_etl_connectwise.transforms import (\n",
    "    create_agreement_period_fact,\n",
    "    create_expense_entry_fact,\n",
    "    create_invoice_line_fact,\n",
    "    create_time_entry_fact,\n",
    ")\n",
    "\n",
    "# Check which Gold fact tables need updating based on refreshed entities\n",
    "print(\"\\nDetermining which fact tables to update...\")\n",
    "\n",
    "if \"TimeEntry\" in refreshed_entities or \"Agreement\" in refreshed_entities:\n",
    "    print(\"\\nCreating fact_time_entry (depends on TimeEntry + Agreement)...\")\n",
    "    try:\n",
    "        # Load required DataFrames - using correct table names\n",
    "        time_entry_silver = spark.table(\"silver_cw_timeentry\")\n",
    "        agreement_silver = spark.table(\"silver_cw_agreement\")\n",
    "\n",
    "        # Check if member table exists (optional for cost data)\n",
    "        member_silver = None\n",
    "        if spark.catalog.tableExists(\"silver_cw_member\"):\n",
    "            member_silver = spark.table(\"silver_cw_member\")\n",
    "            print(\"  Found member table for cost enrichment\")\n",
    "\n",
    "        fact_df = create_time_entry_fact(\n",
    "            spark=spark,\n",
    "            time_entry_df=time_entry_silver,\n",
    "            agreement_df=agreement_silver,\n",
    "            member_df=member_silver,\n",
    "        )\n",
    "\n",
    "        # Use overwrite mode since this is a new fact table\n",
    "        fact_df.write.mode(\"overwrite\").saveAsTable(\"gold_fact_time_entry\")\n",
    "        print(f\"  Created fact_time_entry with {fact_df.count()} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR creating fact_time_entry: {e}\")\n",
    "\n",
    "if \"Agreement\" in refreshed_entities:\n",
    "    print(\"\\nCreating fact_agreement_period...\")\n",
    "    try:\n",
    "        agreement_silver = spark.table(\"silver_cw_agreement\")\n",
    "        fact_df = create_agreement_period_fact(\n",
    "            spark=spark, agreement_df=agreement_silver, config={}\n",
    "        )\n",
    "        fact_df.write.mode(\"overwrite\").saveAsTable(\"gold_fact_agreement_period\")\n",
    "        print(f\"  Created fact_agreement_period with {fact_df.count()} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR creating fact_agreement_period: {e}\")\n",
    "\n",
    "if (\n",
    "    \"UnpostedInvoice\" in refreshed_entities\n",
    "    or \"PostedInvoice\" in refreshed_entities\n",
    "    or \"TimeEntry\" in refreshed_entities\n",
    "):\n",
    "    print(\n",
    "        \"\\nCreating fact_invoice_line (depends on Invoice + TimeEntry + ProductItem + Agreement)...\"\n",
    "    )\n",
    "    try:\n",
    "        # Load all required tables\n",
    "        invoice_silver = spark.table(\"silver_cw_invoice\")\n",
    "        time_entry_silver = (\n",
    "            spark.table(\"silver_cw_timeentry\")\n",
    "            if spark.catalog.tableExists(\"silver_cw_timeentry\")\n",
    "            else None\n",
    "        )\n",
    "        product_silver = (\n",
    "            spark.table(\"silver_cw_productitem\")\n",
    "            if spark.catalog.tableExists(\"silver_cw_productitem\")\n",
    "            else None\n",
    "        )\n",
    "        agreement_silver = (\n",
    "            spark.table(\"silver_cw_agreement\")\n",
    "            if spark.catalog.tableExists(\"silver_cw_agreement\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        fact_df = create_invoice_line_fact(\n",
    "            spark=spark,\n",
    "            invoice_df=invoice_silver,\n",
    "            time_entry_df=time_entry_silver,\n",
    "            product_df=product_silver,\n",
    "            agreement_df=agreement_silver,\n",
    "        )\n",
    "        fact_df.write.mode(\"overwrite\").saveAsTable(\"gold_fact_invoice_line\")\n",
    "        print(f\"  Created fact_invoice_line with {fact_df.count()} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR creating fact_invoice_line: {e}\")\n",
    "\n",
    "if \"ExpenseEntry\" in refreshed_entities:\n",
    "    print(\"\\nCreating fact_expense_entry...\")\n",
    "    try:\n",
    "        expense_silver = spark.table(\"silver_cw_expenseentry\")\n",
    "        agreement_silver = (\n",
    "            spark.table(\"silver_cw_agreement\")\n",
    "            if spark.catalog.tableExists(\"silver_cw_agreement\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        fact_df = create_expense_entry_fact(\n",
    "            spark=spark, expense_df=expense_silver, agreement_df=agreement_silver\n",
    "        )\n",
    "        fact_df.write.mode(\"overwrite\").saveAsTable(\"gold_fact_expense_entry\")\n",
    "        print(f\"  Created fact_expense_entry with {fact_df.count()} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR creating fact_expense_entry: {e}\")\n",
    "\n",
    "print(\"\\n=== Cascade Update Complete ===\")\n",
    "print(\"Bronze -> Silver -> Gold pipeline updated with fresh data!\")\n",
    "\n",
    "# %%\n",
    "# ==============================================================================\n",
    "# DIMENSION GENERATION CELL - Run after Gold layer update\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "Generate dimensions from Silver tables for PowerBI consumption.\n",
    "This creates lightweight dimension tables from enum-like columns.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== Generating Dimensions ===\")\n",
    "from unified_etl_core.date_utils import generate_date_dimension\n",
    "from unified_etl_core.dimensions import create_dimension_from_column\n",
    "\n",
    "# Generate date dimension if it doesn't exist\n",
    "if not spark.catalog.tableExists(\"gold_dim_date\"):\n",
    "    print(\"\\nCreating date dimension...\")\n",
    "    try:\n",
    "        date_dim = generate_date_dimension(\n",
    "            spark=spark,\n",
    "            start_date=\"2020-01-01\",\n",
    "            end_date=\"2030-12-31\",\n",
    "            fiscal_year_start_month=7,  # July fiscal year\n",
    "        )\n",
    "        date_dim.write.mode(\"overwrite\").saveAsTable(\"gold_dim_date\")\n",
    "        print(f\"  Created dim_date with {date_dim.count()} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR creating dim_date: {e}\")\n",
    "\n",
    "# Define dimensions to generate from Silver tables\n",
    "dimension_configs = [\n",
    "    # From TimeEntry\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_timeentry\",\n",
    "        \"column\": \"billableOption\",\n",
    "        \"dimension_name\": \"dim_billable_option\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_timeentry\",\n",
    "        \"column\": \"status\",\n",
    "        \"dimension_name\": \"dim_time_status\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_timeentry\",\n",
    "        \"column\": \"chargeToType\",\n",
    "        \"dimension_name\": \"dim_charge_type\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "    # From Agreement\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_agreement\",\n",
    "        \"column\": \"agreementStatus\",\n",
    "        \"dimension_name\": \"dim_agreement_status\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_agreement\",\n",
    "        \"column\": \"billCycleIdentifier\",\n",
    "        \"dimension_name\": \"dim_bill_cycle\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_agreement\",\n",
    "        \"column\": \"periodType\",\n",
    "        \"dimension_name\": \"dim_period_type\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "    # From Invoice\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_invoice\",\n",
    "        \"column\": \"statusName\",\n",
    "        \"dimension_name\": \"dim_invoice_status\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "    # From ExpenseEntry\n",
    "    {\n",
    "        \"source_table\": \"silver.silver_cw_expenseentry\",\n",
    "        \"column\": \"typeName\",\n",
    "        \"dimension_name\": \"dim_expense_type\",\n",
    "        \"source\": \"connectwise\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Generate dimensions\n",
    "for config in dimension_configs:\n",
    "    if spark.catalog.tableExists(config[\"source_table\"]):\n",
    "        print(\n",
    "            f\"\\nGenerating {config['dimension_name']} from {config['source_table']}.{config['column']}...\"\n",
    "        )\n",
    "        try:\n",
    "            dim_df = create_dimension_from_column(\n",
    "                spark=spark,\n",
    "                source_table=config[\"source_table\"],\n",
    "                column_name=config[\"column\"],\n",
    "                dimension_name=config[\"dimension_name\"],\n",
    "                include_counts=True,\n",
    "            )\n",
    "\n",
    "            # Write dimension table\n",
    "            table_name = f\"gold_{config['dimension_name']}\"\n",
    "            dim_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "            print(f\"  Created {table_name} with {dim_df.count()} values\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR creating {config['dimension_name']}: {e}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nSkipping {config['dimension_name']} - source table {config['source_table']} not found\"\n",
    "        )\n",
    "\n",
    "print(\"\\n=== Dimension Generation Complete ===\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8dfad0-fae6-4c0f-91e5-46d2bc5874af",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-22T22:49:46.0304611Z",
       "execution_start_time": "2025-06-22T22:49:44.4846969Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "ca92f66a-f167-4299-8b96-a664af7d172f",
       "queued_time": "2025-06-22T22:49:44.483203Z",
       "session_id": "feb0af9a-44a1-4d6b-a202-0ddf4d764558",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": "StatementMeta(, feb0af9a-44a1-4d6b-a202-0ddf4d764558, 14, Finished, Available, Finished)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== Column Value Analysis ===\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COLUMN VALUE ANALYSIS CELL - Identify high-value vs noise columns\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "Analyze columns to identify which have signal vs noise for PowerBI views.\n",
    "This helps create focused views that exclude columns with no analytical value.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== Column Value Analysis ===\")\n",
    "\n",
    "from pyspark.sql import functions as F  # noqa: E402\n",
    "\n",
    "\n",
    "def analyze_column_value(df, table_name, sample_size=10000):\n",
    "    \"\"\"Analyze columns to provide detailed statistics without arbitrary scoring.\"\"\"\n",
    "\n",
    "    # Sample the data if it's large\n",
    "    total_count = df.count()\n",
    "    if total_count > sample_size:\n",
    "        df_sample = df.sample(fraction=sample_size / total_count)\n",
    "    else:\n",
    "        df_sample = df\n",
    "\n",
    "    print(f\"\\nAnalyzing {table_name} ({total_count} rows, sampled {df_sample.count()})...\")\n",
    "\n",
    "    analysis_results = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Skip system columns\n",
    "        if col.startswith(\"_etl_\") or col.startswith(\"etl\"):\n",
    "            continue\n",
    "\n",
    "        # Get basic stats\n",
    "        distinct_count = df_sample.select(col).distinct().count()\n",
    "        null_count = df_sample.filter(F.col(col).isNull()).count()\n",
    "        null_percentage = (null_count / df_sample.count()) * 100\n",
    "        non_null_count = df_sample.count() - null_count\n",
    "\n",
    "        # Determine column type\n",
    "        col_type = str(df.schema[col].dataType)\n",
    "\n",
    "        # Calculate cardinality ratio (distinct values / non-null rows)\n",
    "        cardinality_ratio = distinct_count / non_null_count if non_null_count > 0 else 0\n",
    "\n",
    "        # Get sample values for low-cardinality columns\n",
    "        sample_values = []\n",
    "        if distinct_count <= 10 and distinct_count > 0:\n",
    "            values_df = (\n",
    "                df_sample.select(col).distinct().filter(F.col(col).isNotNull()).limit(10).collect()\n",
    "            )\n",
    "            sample_values = [str(row[0]) for row in values_df]\n",
    "\n",
    "        # For numeric columns, get min/max/mean\n",
    "        numeric_stats = {}\n",
    "        if (\n",
    "            \"Int\" in col_type\n",
    "            or \"Double\" in col_type\n",
    "            or \"Decimal\" in col_type\n",
    "            or \"Float\" in col_type\n",
    "        ):\n",
    "            stats_df = df_sample.select(\n",
    "                F.min(col).alias(\"min\"),\n",
    "                F.max(col).alias(\"max\"),\n",
    "                F.mean(col).alias(\"mean\"),\n",
    "                F.stddev(col).alias(\"stddev\"),\n",
    "            ).collect()[0]\n",
    "            numeric_stats = {\n",
    "                \"min\": stats_df[\"min\"],\n",
    "                \"max\": stats_df[\"max\"],\n",
    "                \"mean\": round(stats_df[\"mean\"], 2) if stats_df[\"mean\"] else None,\n",
    "                \"stddev\": round(stats_df[\"stddev\"], 2) if stats_df[\"stddev\"] else None,\n",
    "            }\n",
    "\n",
    "        # Column category based on characteristics\n",
    "        category = \"unknown\"\n",
    "        if col.lower().endswith((\"id\", \"key\", \"_id\", \"_key\")):\n",
    "            category = \"identifier\"\n",
    "        elif any(\n",
    "            kw in col.lower() for kw in [\"amount\", \"cost\", \"price\", \"revenue\", \"total\", \"sum\"]\n",
    "        ):\n",
    "            category = \"financial\"\n",
    "        elif any(kw in col.lower() for kw in [\"hours\", \"days\", \"minutes\", \"duration\"]):\n",
    "            category = \"time_measure\"\n",
    "        elif any(kw in col.lower() for kw in [\"date\", \"time\", \"created\", \"updated\", \"modified\"]):\n",
    "            category = \"temporal\"\n",
    "        elif any(kw in col.lower() for kw in [\"name\", \"description\", \"notes\", \"comment\"]):\n",
    "            category = \"text\"\n",
    "        elif any(kw in col.lower() for kw in [\"status\", \"type\", \"category\", \"class\"]):\n",
    "            category = \"categorical\"\n",
    "        elif \"Boolean\" in col_type:\n",
    "            category = \"boolean\"\n",
    "        elif distinct_count == 1:\n",
    "            category = \"constant\"\n",
    "        elif cardinality_ratio > 0.95:\n",
    "            category = \"high_cardinality\"\n",
    "        elif cardinality_ratio < 0.01:\n",
    "            category = \"low_cardinality\"\n",
    "\n",
    "        analysis_results.append(\n",
    "            {\n",
    "                \"column\": col,\n",
    "                \"type\": col_type,\n",
    "                \"category\": category,\n",
    "                \"distinct_values\": distinct_count,\n",
    "                \"null_count\": null_count,\n",
    "                \"null_percentage\": round(null_percentage, 1),\n",
    "                \"cardinality_ratio\": round(cardinality_ratio, 3),\n",
    "                \"sample_values\": sample_values[:5] if sample_values else [],\n",
    "                \"numeric_stats\": numeric_stats,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort by category, then by null percentage\n",
    "    analysis_results.sort(key=lambda x: (x[\"category\"], x[\"null_percentage\"]))\n",
    "\n",
    "    # Print analysis by category\n",
    "    categories = {}\n",
    "    for result in analysis_results:\n",
    "        cat = result[\"category\"]\n",
    "        if cat not in categories:\n",
    "            categories[cat] = []\n",
    "        categories[cat].append(result)\n",
    "\n",
    "    print(\"\\n  Column Analysis by Category:\")\n",
    "    print(f\"  {'Category':<20} {'Count':<8} {'Avg Nulls %':<12}\")\n",
    "    print(f\"  {'-' * 40}\")\n",
    "\n",
    "    for cat, cols in categories.items():\n",
    "        avg_nulls = sum(c[\"null_percentage\"] for c in cols) / len(cols)\n",
    "        print(f\"  {cat:<20} {len(cols):<8} {avg_nulls:<12.1f}\")\n",
    "\n",
    "    # Show detailed analysis for each category\n",
    "    for cat, cols in categories.items():\n",
    "        print(f\"\\n  === {cat.upper()} COLUMNS ===\")\n",
    "        for col in cols[:5]:  # Show first 5 of each category\n",
    "            print(f\"\\n  {col['column']} ({col['type']})\")\n",
    "            print(\n",
    "                f\"    Nulls: {col['null_percentage']}% | Distinct: {col['distinct_values']} | Cardinality: {col['cardinality_ratio']}\"\n",
    "            )\n",
    "            if col[\"sample_values\"]:\n",
    "                print(f\"    Sample values: {col['sample_values']}\")\n",
    "            if col[\"numeric_stats\"]:\n",
    "                stats = col[\"numeric_stats\"]\n",
    "                print(\n",
    "                    f\"    Range: [{stats['min']} - {stats['max']}] | Mean: {stats['mean']} | StdDev: {stats['stddev']}\"\n",
    "                )\n",
    "\n",
    "        if len(cols) > 5:\n",
    "            print(f\"    ... and {len(cols) - 5} more {cat} columns\")\n",
    "\n",
    "    # Identify potentially problematic columns\n",
    "    print(\"\\n  === POTENTIAL ISSUES ===\")\n",
    "\n",
    "    constant_cols = [r for r in analysis_results if r[\"category\"] == \"constant\"]\n",
    "    if constant_cols:\n",
    "        print(\"\\n  Constant columns (single value):\")\n",
    "        for col in constant_cols:\n",
    "            print(\n",
    "                f\"    {col['column']}: '{col['sample_values'][0] if col['sample_values'] else 'NULL'}'\"\n",
    "            )\n",
    "\n",
    "    high_null_cols = [r for r in analysis_results if r[\"null_percentage\"] > 95]\n",
    "    if high_null_cols:\n",
    "        print(\"\\n  Nearly empty columns (>95% null):\")\n",
    "        for col in high_null_cols:\n",
    "            print(f\"    {col['column']}: {col['null_percentage']}% null\")\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "# Analyze key tables\n",
    "tables_to_analyze = [\n",
    "    \"silver_cw_timeentry\",\n",
    "    \"silver_cw_agreement\",\n",
    "    \"silver_cw_invoice\",\n",
    "    \"gold_fact_time_entry\",\n",
    "]\n",
    "\n",
    "all_analysis = {}\n",
    "for table in tables_to_analyze:\n",
    "    if spark.catalog.tableExists(table):\n",
    "        df = spark.table(table)\n",
    "        analysis = analyze_column_value(df, table)\n",
    "        all_analysis[table] = analysis\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083ca87-0003-4a2c-8972-59b1df362194",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-22T22:40:54.7193606Z",
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "cancelled",
       "parent_msg_id": "1f70f2a7-3c9f-49ca-9f0e-7b00411b891a",
       "queued_time": "2025-06-22T22:38:44.966174Z",
       "session_id": "feb0af9a-44a1-4d6b-a202-0ddf4d764558",
       "session_start_time": null,
       "spark_pool": null,
       "state": "cancelled",
       "statement_id": -1,
       "statement_ids": null
      },
      "text/plain": "StatementMeta(, feb0af9a-44a1-4d6b-a202-0ddf4d764558, -1, Cancelled, , Cancelled)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CREATE POWERBI VIEWS CELL - Create optimized views for PowerBI\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "Create SQL views optimized for PowerBI consumption.\n",
    "These views exclude low-value columns identified in the analysis above.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== Creating PowerBI Optimized Views ===\")\n",
    "\n",
    "\n",
    "# Function to create view based on column analysis\n",
    "def create_powerbi_view(table_name, analysis_results, view_suffix=\"_pbi\"):\n",
    "    \"\"\"Create a view excluding problematic columns based on analysis.\"\"\"\n",
    "\n",
    "    # Exclude columns based on objective criteria\n",
    "    exclude_categories = [\"constant\", \"unknown\"]  # Always exclude these\n",
    "    exclude_columns = set()\n",
    "\n",
    "    for result in analysis_results:\n",
    "        # Exclude constant columns (single value)\n",
    "        if result[\"category\"] == \"constant\" or (result[\"null_percentage\"] > 95 and result[\"category\"] not in [\n",
    "            \"identifier\",\n",
    "            \"financial\",\n",
    "        ]) or (result[\"category\"] == \"unknown\" and result[\"null_percentage\"] > 80):\n",
    "            exclude_columns.add(result[\"column\"])\n",
    "\n",
    "    # Get columns to keep\n",
    "    keep_columns = [r[\"column\"] for r in analysis_results if r[\"column\"] not in exclude_columns]\n",
    "\n",
    "    # Always include certain critical columns even if they have issues\n",
    "    critical_patterns = [\"id\", \"key\", \"sk\", \"date\", \"amount\", \"revenue\"]\n",
    "    for result in analysis_results:\n",
    "        col = result[\"column\"]\n",
    "        if any(pattern in col.lower() for pattern in critical_patterns) and col not in keep_columns:\n",
    "            keep_columns.append(col)\n",
    "\n",
    "    # Build SELECT statement\n",
    "    select_cols = \", \".join(keep_columns)\n",
    "    view_name = f\"{table_name}{view_suffix}\"\n",
    "\n",
    "    create_view_sql = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {view_name} AS\n",
    "    SELECT {select_cols}\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        spark.sql(create_view_sql)\n",
    "        print(\n",
    "            f\"  Created view {view_name} with {len(keep_columns)} columns (excluded {len(exclude_columns)} columns)\"\n",
    "        )\n",
    "        if exclude_columns:\n",
    "            print(f\"    Excluded: {', '.join(sorted(exclude_columns))}\")\n",
    "        return view_name\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR creating view {view_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create views for analyzed tables\n",
    "for table_name, analysis in all_analysis.items():\n",
    "    create_powerbi_view(table_name, analysis)\n",
    "\n",
    "# Create specialized fact views with business logic\n",
    "print(\"\\n\\nCreating specialized business views...\")\n",
    "\n",
    "# Time Entry Analysis View\n",
    "time_entry_view_sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW v_time_entry_analysis AS\n",
    "SELECT\n",
    "    TimeEntrySK,\n",
    "    WorkDateSK,\n",
    "    memberName,\n",
    "    agreementName,\n",
    "    agreementType,\n",
    "    utilizationType,\n",
    "    actualHours,\n",
    "    potentialRevenue,\n",
    "    actualCost,\n",
    "    margin,\n",
    "    marginPercentage,\n",
    "    isInternalWork,\n",
    "    isTimapottur,\n",
    "    effectiveBillingStatus,\n",
    "    daysSinceWork,\n",
    "    daysUninvoiced\n",
    "FROM gold_fact_time_entry\n",
    "WHERE actualHours > 0  -- Filter out zero-hour entries\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(time_entry_view_sql)\n",
    "    print(\"  Created v_time_entry_analysis\")\n",
    "except Exception as e:\n",
    "    print(f\"  ERROR creating v_time_entry_analysis: {e}\")\n",
    "\n",
    "print(\"\\n=== PowerBI View Creation Complete ===\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Connect PowerBI to the Lakehouse\")\n",
    "print(\"2. Use the _pbi views for optimized performance\")\n",
    "print(\"3. Use v_time_entry_analysis for time tracking analytics\")\n",
    "print(\"4. Join views with gold_dim_* tables for descriptive labels\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "22176b2b-5fe3-46bb-8b0a-193e1356c3cf",
    "default_lakehouse_name": "Lakehouse",
    "default_lakehouse_workspace_id": "a3a23dd7-9f52-4b88-b056-46da3617c0b2",
    "known_lakehouses": [
     {
      "id": "22176b2b-5fe3-46bb-8b0a-193e1356c3cf"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}