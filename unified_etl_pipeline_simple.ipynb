{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified ETL Pipeline - Simple Version\n",
    "\n",
    "This notebook uses the unified ETL framework as designed, without reimplementing existing functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the wheel files\n",
    "%pip install /lakehouse/default/Files/unified_etl_core-1.0.0-py3-none-any.whl\n",
    "%pip install /lakehouse/default/Files/unified_etl_connectwise-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set ConnectWise credentials\n",
    "os.environ[\"CW_AUTH_USERNAME\"] = \"thekking+yemGyHDPdJ1hpuqx\"\n",
    "os.environ[\"CW_AUTH_PASSWORD\"] = \"yMqpe26Jcu55FbQk\"\n",
    "os.environ[\"CW_CLIENTID\"] = \"c7ea92d2-eaf5-4bfb-a09c-58d7f9dd7b81\"\n",
    "\n",
    "print(\"‚úÖ Credentials configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_core.main import run_etl_pipeline\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Run full pipeline\n",
    "print(\"üöÄ Starting Full ETL Pipeline...\")\n",
    "\n",
    "run_etl_pipeline(integrations=[\"connectwise\"], layers=[\"bronze\", \"silver\", \"gold\"], config={})\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alternative: Run Specific Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only Bronze layer\n",
    "run_etl_pipeline(integrations=[\"connectwise\"], layers=[\"bronze\"], config={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only Silver layer (assuming Bronze exists)\n",
    "run_etl_pipeline(integrations=[\"connectwise\"], layers=[\"silver\"], config={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only Gold layer (assuming Silver exists)\n",
    "run_etl_pipeline(integrations=[\"connectwise\"], layers=[\"gold\"], config={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# Check table counts\n",
    "for schema in [\"bronze\", \"silver\", \"gold\"]:\n",
    "    print(f\"\\n{schema.upper()} Tables:\")\n",
    "    try:\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {schema}\").collect()\n",
    "        for row in tables:\n",
    "            table_name = row.tableName\n",
    "            count = spark.sql(f\"SELECT COUNT(*) FROM {schema}.{table_name}\").collect()[0][0]\n",
    "            print(f\"  {table_name}: {count:,} rows\")\n",
    "    except:\n",
    "        print(f\"  Schema not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unified_etl_core.date_utils import generate_date_dimension\n",
    "from unified_etl_core.dimensions import create_dimension_from_column\n",
    "\n",
    "# Generate date dimension\n",
    "if not spark.catalog.tableExists(\"gold.gold_dim_date\"):\n",
    "    date_dim = generate_date_dimension(\n",
    "        spark=spark,\n",
    "        start_date=\"2020-01-01\",\n",
    "        end_date=\"2030-12-31\",\n",
    "        fiscal_year_start_month=7,\n",
    "    )\n",
    "    date_dim.write.mode(\"overwrite\").saveAsTable(\"gold.gold_dim_date\")\n",
    "    print(f\"‚úÖ Created date dimension\")\n",
    "\n",
    "# Generate other dimensions\n",
    "dimension_configs = [\n",
    "    (\"silver_cw_timeentry\", \"billableOption\", \"dim_billable_option\"),\n",
    "    (\"silver_cw_timeentry\", \"status\", \"dim_time_status\"),\n",
    "    (\"silver_cw_agreement\", \"agreementStatus\", \"dim_agreement_status\"),\n",
    "    (\"silver_cw_invoice\", \"statusName\", \"dim_invoice_status\"),\n",
    "]\n",
    "\n",
    "for source_table, column, dim_name in dimension_configs:\n",
    "    try:\n",
    "        dim_df = create_dimension_from_column(\n",
    "            spark=spark, source_table=source_table, column_name=column, dimension_name=dim_name\n",
    "        )\n",
    "        dim_df.write.mode(\"overwrite\").saveAsTable(f\"gold.{dim_name}\")\n",
    "        print(f\"‚úÖ Created {dim_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create {dim_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check uninvoiced revenue\n",
    "if spark.catalog.tableExists(\"gold.gold_fact_invoice_line\"):\n",
    "    uninvoiced = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as uninvoiced_lines,\n",
    "            SUM(lineAmount) as uninvoiced_amount\n",
    "        FROM gold.gold_fact_invoice_line\n",
    "        WHERE invoiceId IS NULL\n",
    "    \"\"\").collect()[0]\n",
    "\n",
    "    print(f\"\\nüí∞ Uninvoiced Work:\")\n",
    "    print(f\"  Lines: {uninvoiced['uninvoiced_lines']:,}\")\n",
    "    if uninvoiced[\"uninvoiced_amount\"]:\n",
    "        print(f\"  Amount: ${uninvoiced['uninvoiced_amount']:,.2f}\")\n",
    "\n",
    "# Check time entry breakdown\n",
    "if spark.catalog.tableExists(\"gold.gold_fact_time_entry\"):\n",
    "    breakdown = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            utilizationType,\n",
    "            COUNT(*) as entries,\n",
    "            SUM(actualHours) as total_hours\n",
    "        FROM gold.gold_fact_time_entry\n",
    "        GROUP BY utilizationType\n",
    "        ORDER BY total_hours DESC\n",
    "    \"\"\").collect()\n",
    "\n",
    "    print(\"\\n‚è∞ Time Entry Breakdown:\")\n",
    "    for row in breakdown:\n",
    "        print(\n",
    "            f\"  {row['utilizationType']}: {row['total_hours']:,.1f} hours ({row['entries']:,} entries)\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}